{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix A: The Little Book of Tensors\n",
        "Welcome to Appendix A. This notebook contains the listings for Appendix A, which introduces the essential properties of tensors and the operations you can perform on them."
      ],
      "metadata": {
        "id": "A2LZbxF7bRXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing A-1 Properties of Tensors\n",
        "Tensors expose many properties that describe their structure, layout, data type, device placement, and relationship to automatic differentiation.\n",
        "\n",
        "Here is a simple way to print many of them:"
      ],
      "metadata": {
        "id": "KOij1A1Jc2Ha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmNOAtuHS7au"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "\n",
        "print(\"Shape:\", tensor.shape)\n",
        "print(\"Rank (Number of Dimensions):\", tensor.dim())\n",
        "print(\"Number of Elements:\", tensor.numel())\n",
        "print(\"Data Type:\", tensor.dtype)\n",
        "print(\"Device:\", tensor.device)\n",
        "print(\"Strides:\", tensor.stride())\n",
        "print(\"Requires Gradient:\", tensor.requires_grad)\n",
        "print(\"Gradient Function:\", tensor.grad_fn)\n",
        "print(\"Is Contiguous:\", tensor.is_contiguous())\n",
        "print(\"Element Size (bytes):\", tensor.element_size())\n",
        "print(\"Storage Offset:\", tensor.storage_offset())\n",
        "print(\"Data Pointer:\", tensor.data_ptr())\n",
        "print(\"Layout:\", tensor.layout)\n",
        "print(\"Is Sparse:\", tensor.is_sparse)\n",
        "print(\"Is Quantized:\", tensor.is_quantized)\n",
        "print(\"Is CUDA:\", tensor.is_cuda)\n",
        "print(\"Is Pinned:\", tensor.is_pinned())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing A-2 Preparing Token Sequences with Indexing, Slicing, and Masking\n",
        "This example prepares a batch of tokenized sentences for a language model that uses padding."
      ],
      "metadata": {
        "id": "jXkBzHyqdZjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Real-world example: preparing a batch of tokenized sentences\n",
        "# for a language model that uses padding.\n",
        "\n",
        "# tokens.shape = (batch_size, seq_len)\n",
        "# 0 is the padding token.\n",
        "tokens = torch.tensor([\n",
        "    [101, 2009, 2003, 1037, 2154,   0,   0,   0],  # \"It is a nice [PAD] [PAD] [PAD]\"\n",
        "    [101, 1045, 2293, 3679, 3185, 102,   0,   0],  # \"I love reading books [SEP] [PAD] [PAD]\"\n",
        "])\n",
        "print(\"tokens:\\n\", tokens)\n",
        "\n",
        "PAD_ID = 0\n",
        "\n",
        "# Indexing: pick specific samples\n",
        "# Example: select the second sentence from the batch\n",
        "second_sentence = tokens[1]\n",
        "print(\"\\nSecond sentence (indexing):\\n\", second_sentence)\n",
        "\n",
        "# Slicing: pick ranges (time steps)\n",
        "# Example: model only uses the first 5 tokens of each sentence\n",
        "first_five_tokens = tokens[:, :5]\n",
        "print(\"\\nFirst 5 tokens of each sentence (slicing):\\n\", first_five_tokens)\n",
        "\n",
        "# Masking: ignore padding tokens\n",
        "# Create a mask where True means \"real token\" and False means \"padding\"\n",
        "non_pad_mask = tokens != PAD_ID\n",
        "print(\"\\nNon-padding mask (masking):\\n\", non_pad_mask)\n",
        "\n",
        "# Use the mask to get all real tokens in a flat view\n",
        "real_tokens = tokens[non_pad_mask]\n",
        "print(\"\\nAll real tokens (flattened, padding removed):\\n\", real_tokens)\n",
        "\n",
        "# In practice, the non_pad_mask is also used to:\n",
        "# - compute loss only on real tokens\n",
        "# - build attention masks for Transformer models\n"
      ],
      "metadata": {
        "id": "goduwoPdd952"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing A-3 Concatenation and Splitting Features in Real Model Workflow\n",
        "Concatenation and splitting appear throughout deep learning workflows.\n",
        "\n",
        "Concatenation is commonly used when combining feature representations from different sources. Examples include merging image features with tabular metadata, stitching together embeddings from multiple encoders, or joining sequence segments into a single input for a model.\n",
        "\n",
        "Splitting is used for the opposite purpose. It divides data into manageable parts. Common uses include creating micro-batches when memory is limited, dividing long sequences into shorter windows, and splitting inputs for parallel or distributed processing.\n",
        "\n",
        "The following example shows both operations in a realistic setting.\n",
        "\n"
      ],
      "metadata": {
        "id": "32bbwWz0eEHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Image features from a CNN\n",
        "# Shape: (batch, image_feature_dim)\n",
        "image_features = torch.randn(12, 128)\n",
        "\n",
        "# Metadata features\n",
        "# Shape: (batch, metadata_feature_dim)\n",
        "metadata = torch.randn(12, 3)\n",
        "\n",
        "# Concatenate features along the feature dimension\n",
        "combined = torch.cat((image_features, metadata), dim=1)\n",
        "print(\"Combined feature shape:\", combined.shape)\n",
        "# (12, 131)\n",
        "\n",
        "# Split into micro-batches of size 4\n",
        "micro_batches = torch.split(combined, 4)\n",
        "\n",
        "print(\"Number of micro-batches:\", len(micro_batches))\n",
        "for i, mb in enumerate(micro_batches):\n",
        "    print(f\"Micro-batch {i} shape:\", mb.shape)\n"
      ],
      "metadata": {
        "id": "u0WCMZcqefjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing A-4 Matrix Operations in a Simple Forward Pass\n",
        "Matrix operations appear throughout a model’s forward pass. Inputs are reshaped, dimen-sions are reordered, and values are combined through matrix multiplication to produce in-termediate results and final outputs. While each operation is simple on its own, their repeat-ed composition is what enables models to learn useful structure from data.\n",
        "\n",
        "The following example shows how these operations appear inside a simple model:"
      ],
      "metadata": {
        "id": "A_mOSpDjevYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# A tiny example: classify a 4×4 grayscale image\n",
        "image = torch.randn(1, 1, 4, 4)   # (batch, channels, height, width)\n",
        "\n",
        "flatten = nn.Flatten()            # reshapes (1, 1, 4, 4) → (1, 16)\n",
        "classifier = nn.Linear(16, 3)     # matrix multiplication + bias\n",
        "\n",
        "x = flatten(image)\n",
        "logits = classifier(x)\n",
        "print(\"Logits:\", logits)"
      ],
      "metadata": {
        "id": "2aRpamu2fNg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing A-5 Fourier Transforms and Gradient Computation in Practice\n",
        "Fourier transforms appear in audio processing, imaging, and scientific analysis. Audio models often convert raw waveforms into frequency representations before feeding them to a neural network. Similar techniques are used in medical imaging and vibration analysis to detect repeating patterns or anomalies.\n",
        "\n",
        "Gradient computations drive the training process for neural networks. Every training step relies on gradients to update model parameters, and automatic differentiation makes it prac-tical to define complex models without manually deriving equations.\n",
        "\n",
        "The following example shows two realistic uses of these operations: computing a frequen-cy representation of a short signal, and computing gradients for a simple linear regression model with explicit values.\n"
      ],
      "metadata": {
        "id": "BALMNB1bgc4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Example 1: Frequency analysis of a short signal\n",
        "signal = torch.tensor([0.0, 1.0, 0.0, -1.0])\n",
        "spectrum = torch.fft.fft(signal)\n",
        "print(\"Frequency spectrum:\", spectrum)\n",
        "\n",
        "# Example 2: Gradients for a tiny regression model\n",
        "weights = torch.tensor([[2.0]], requires_grad=True)\n",
        "inputs  = torch.tensor([[1.0], [2.0]])\n",
        "targets = torch.tensor([[2.0], [4.0]])\n",
        "\n",
        "preds = inputs @ weights          # predictions: [[2.0], [4.0]]\n",
        "loss = F.mse_loss(preds, targets) # zero loss in this case\n",
        "\n",
        "loss.backward()\n",
        "print(\"Gradient on weights:\", weights.grad)"
      ],
      "metadata": {
        "id": "PHqALdPRgthM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing A-6 Using Sparse, Quantized, and Named Tensors in a Recommendation Workflow\n",
        "Advanced tensor properties matter in systems that operate at scale or under resource con-straints. Sparse layouts reduce memory and computation when data contains many zeros. Quantized tensors allow models to run efficiently on hardware with limited memory or bandwidth. The .nonzero() method provides direct access to meaningful values without scanning dense structures. Named tensors improve correctness and maintainability in com-plex, multi-dimensional pipelines.\n",
        "\n",
        "This code demonstrates how these properties appear in a recommendation workflow. Sparse tensors represent user–item interactions efficiently. Dense model outputs derived from these interactions can be quantized for deployment. Named dimensions clarify tensor structure throughout the pipeline.\n"
      ],
      "metadata": {
        "id": "LLnaS-QyhJ2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 1. Sparse user–item interaction matrix\n",
        "indices = torch.tensor([[0, 1, 2],\n",
        "                        [1, 3, 4]])\n",
        "values = torch.tensor([5.0, 3.0, 4.0])\n",
        "\n",
        "interactions = torch.sparse_coo_tensor(indices, values, size=(3, 5))\n",
        "print(\"Is sparse:\", interactions.is_sparse)\n",
        "\n",
        "# 2. Identify active interactions\n",
        "active_positions = interactions.coalesce().indices().t()\n",
        "print(\"Active user–item pairs:\\n\", active_positions)\n",
        "\n",
        "# 3. Dense model output derived from interactions\n",
        "dense_output = torch.randn(3, 5)\n",
        "\n",
        "# Quantize dense output for deployment\n",
        "qoutput = torch.quantize_per_tensor(dense_output, scale=0.1,\n",
        "                                    zero_point=0, dtype=torch.qint8)\n",
        "print(\"Is quantized:\", qoutput.is_quantized)\n",
        "\n",
        "# 4. Use named tensors for clarity\n",
        "named_output = dense_output.refine_names('user', 'item')\n",
        "print(\"Named tensor dims:\", named_output.names)\n"
      ],
      "metadata": {
        "id": "FDEc58xZhd2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2GxA7--1oe9B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}