{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix A: The Little Book of Tensors\n",
        "Welcome to Appendix A. This notebook contains the listings for Appendix A, which introduces the essential properties of tensors and the operations you can perform on them."
      ],
      "metadata": {
        "id": "A2LZbxF7bRXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing A-1 Properties of Tensors"
      ],
      "metadata": {
        "id": "KOij1A1Jc2Ha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmNOAtuHS7au"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "\n",
        "print(\"Shape:\", tensor.shape)\n",
        "print(\"Rank (Number of Dimensions):\", tensor.dim())\n",
        "print(\"Number of Elements:\", tensor.numel())\n",
        "print(\"Data Type:\", tensor.dtype)\n",
        "print(\"Device:\", tensor.device)\n",
        "print(\"Strides:\", tensor.stride())\n",
        "print(\"Requires Gradient:\", tensor.requires_grad)\n",
        "print(\"Gradient Function:\", tensor.grad_fn)\n",
        "print(\"Is Contiguous:\", tensor.is_contiguous())\n",
        "print(\"Element Size (bytes):\", tensor.element_size())\n",
        "print(\"Storage Offset:\", tensor.storage_offset())\n",
        "print(\"Data Pointer:\", tensor.data_ptr())\n",
        "print(\"Layout:\", tensor.layout)\n",
        "print(\"Is Sparse:\", tensor.is_sparse)\n",
        "print(\"Is Quantized:\", tensor.is_quantized)\n",
        "print(\"Is CUDA:\", tensor.is_cuda)\n",
        "print(\"Is Pinned:\", tensor.is_pinned())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing A-2 Preparing Token Sequences with Indexing, Slicing, and Masking\n",
        "This example prepares a batch of tokenized sentences for a language model that uses padding."
      ],
      "metadata": {
        "id": "jXkBzHyqdZjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Real-world example: preparing a batch of tokenized sentences\n",
        "# for a language model that uses padding.\n",
        "\n",
        "# tokens.shape = (batch_size, seq_len)\n",
        "# 0 is the padding token.\n",
        "tokens = torch.tensor([\n",
        "    [101, 2009, 2003, 1037, 2154,   0,   0,   0],  # \"It is a nice [PAD] [PAD] [PAD]\"\n",
        "    [101, 1045, 2293, 3679, 3185, 102,   0,   0],  # \"I love reading books [SEP] [PAD] [PAD]\"\n",
        "])\n",
        "print(\"tokens:\\n\", tokens)\n",
        "\n",
        "PAD_ID = 0\n",
        "\n",
        "# -----------------------------------------\n",
        "# 1. Indexing: pick specific samples\n",
        "# -----------------------------------------\n",
        "# Example: select the second sentence from the batch\n",
        "second_sentence = tokens[1]\n",
        "print(\"\\nSecond sentence (indexing):\\n\", second_sentence)\n",
        "\n",
        "# -----------------------------------------\n",
        "# 2. Slicing: pick ranges (time steps)\n",
        "# -----------------------------------------\n",
        "# Example: model only uses the first 5 tokens of each sentence\n",
        "first_five_tokens = tokens[:, :5]\n",
        "print(\"\\nFirst 5 tokens of each sentence (slicing):\\n\", first_five_tokens)\n",
        "\n",
        "# -----------------------------------------\n",
        "# 3. Masking: ignore padding tokens\n",
        "# -----------------------------------------\n",
        "# Create a mask where True means \"real token\" and False means \"padding\"\n",
        "non_pad_mask = tokens != PAD_ID\n",
        "print(\"\\nNon-padding mask (masking):\\n\", non_pad_mask)\n",
        "\n",
        "# Use the mask to get all real tokens in a flat view\n",
        "real_tokens = tokens[non_pad_mask]\n",
        "print(\"\\nAll real tokens (flattened, padding removed):\\n\", real_tokens)\n",
        "\n",
        "# In practice, the non_pad_mask is also used to:\n",
        "# - compute loss only on real tokens\n",
        "# - build attention masks for Transformer models\n"
      ],
      "metadata": {
        "id": "goduwoPdd952"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing A-3 Combining and Splitting Features in Real Model Workflow\n",
        "In many applications, a model must use both image data and additional metadata about the same sample. For example, a medical imaging model might take both a chest X-ray and patient attributes such as age and smoking status. We can concatenate the two feature sets to form a unified input. Later, if the batch is too large for GPU memory, we can split the combined tensor into micro-batches. This listing shows an example."
      ],
      "metadata": {
        "id": "32bbwWz0eEHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Imagine we extracted image features using a CNN\n",
        "# Shape: (batch, image_feature_dim)\n",
        "image_features = torch.randn(12, 128)     # 12 samples, 128-dimensional features\n",
        "\n",
        "# Metadata features (for example, age, weight, and risk score)\n",
        "# Shape: (batch, metadata_feature_dim)\n",
        "metadata = torch.randn(12, 3)             # 3 additional features per sample\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# STEP 1: Concatenate features from two sources\n",
        "# --------------------------------------------------------\n",
        "\n",
        "# Combine image and metadata features along the last dimension\n",
        "combined = torch.cat((image_features, metadata), dim=1)\n",
        "print(\"Combined feature shape:\", combined.shape)\n",
        "# Shape: (12, 131)\n",
        "\n",
        "# The model now sees one unified representation per patient:\n",
        "# 128 image features + 3 metadata features\n",
        "\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# STEP 2: Split into micro-batches for memory-friendly training\n",
        "# --------------------------------------------------------\n",
        "\n",
        "# Suppose our GPU can only handle 4 samples at a time\n",
        "micro_batches = torch.split(combined, 4)   # splits into 3 chunks of size 4, 4, 4\n",
        "\n",
        "print(\"\\nNumber of micro-batches:\", len(micro_batches))\n",
        "for i, mb in enumerate(micro_batches):\n",
        "    print(f\"Micro-batch {i} shape:\", mb.shape)\n"
      ],
      "metadata": {
        "id": "u0WCMZcqefjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing A-4 Matrix Operations in a Simple Forward Pass\n",
        "This example reshapes a small image into a vector, then a linear layer performs a matrix multiplication to produce logits. This is exactly the pattern used throughout real models: data is reorganized, multiplied by learned weights, and passed forward repeatedly."
      ],
      "metadata": {
        "id": "A_mOSpDjevYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# A tiny example: classify a 4×4 grayscale \"image\"\n",
        "image = torch.randn(1, 1, 4, 4)   # (batch, channels, height, width)\n",
        "\n",
        "# Flatten layer + linear classifier\n",
        "flatten = nn.Flatten()            # reshapes (1, 1, 4, 4) → (1, 16)\n",
        "classifier = nn.Linear(16, 3)     # matrix multiply: (1×16) @ (16×3)\n",
        "\n",
        "# Forward pass\n",
        "x = flatten(image)                # reshape\n",
        "logits = classifier(x)            # matrix multiplication\n",
        "print(\"Logits:\", logits)"
      ],
      "metadata": {
        "id": "2aRpamu2fNg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing A-5 Fourier Transforms and Gradient Computation in Practice\n",
        "This example reflects a common pattern in real systems: specialized transforms prepare data for a model, and automatic differentiation supplies the gradients that allow that model to learn."
      ],
      "metadata": {
        "id": "BALMNB1bgc4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Example 1: Frequency analysis for an audio snippet\n",
        "# Simulated 1-second mono audio at 16 kHz\n",
        "waveform = torch.randn(16_000)              # time-domain signal\n",
        "spectrum = torch.fft.rfft(waveform)         # frequency-domain representation\n",
        "print(\"Spectrum shape:\", spectrum.shape)\n",
        "\n",
        "# Example 2: Gradients for a tiny regression head\n",
        "weights = torch.randn(10, 1, requires_grad=True)\n",
        "\n",
        "inputs = torch.randn(4, 10)                 # 4 samples, 10 features\n",
        "targets = torch.randn(4, 1)                 # regression targets\n",
        "\n",
        "preds = inputs @ weights                    # linear model: matrix multiplication\n",
        "loss = F.mse_loss(preds, targets)           # mean squared error loss\n",
        "loss.backward()                             # compute gradients\n",
        "\n",
        "print(\"Gradient on weights:\\n\", weights.grad)"
      ],
      "metadata": {
        "id": "PHqALdPRgthM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing A-6 Using Sparse, Quantized, and Named Tensors in a Recommendation Workflow\n",
        "This listing builds a small but realistic example of a recommendation pipeline. It constructs a sparse user–item interaction matrix, retrieves the active user–item pairs, simulates dense model outputs and converts them to a quantized format suitable for deployment, and applies names to tensor dimensions for clarity. Together, these steps illustrate how advanced tensor properties support large-scale, resource-conscious systems that must remain readable and maintainable."
      ],
      "metadata": {
        "id": "LLnaS-QyhJ2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Sparse user–item interaction matrix (user-item ratings)\n",
        "# ---------------------------------------------------------\n",
        "# Users: 3, Items: 5\n",
        "indices = torch.tensor([[0, 1, 2],     # user indices\n",
        "                        [1, 3, 4]])    # item indices\n",
        "values = torch.tensor([5.0, 3.0, 4.0]) # non-zero ratings\n",
        "\n",
        "interactions = torch.sparse_coo_tensor(indices, values, size=(3, 5))\n",
        "print(\"Is sparse:\", interactions.is_sparse)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Identify meaningful interactions using .nonzero()\n",
        "# ---------------------------------------------------------\n",
        "active_positions = interactions.coalesce().indices().t()\n",
        "print(\"Active user–item pairs:\\n\", active_positions)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Simulate a dense model output and quantize it for deployment\n",
        "# ---------------------------------------------------------\n",
        "dense_output = torch.randn(3, 5)               # predictions or embeddings\n",
        "qoutput = torch.quantize_per_tensor(dense_output, scale=0.1,\n",
        "                                    zero_point=0, dtype=torch.qint8)\n",
        "print(\"Is quantized:\", qoutput.is_quantized)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. Use named tensors for dimension clarity\n",
        "# ---------------------------------------------------------\n",
        "named_output = dense_output.refine_names('user', 'item')\n",
        "print(\"Named tensor dims:\", named_output.names)\n"
      ],
      "metadata": {
        "id": "FDEc58xZhd2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2GxA7--1oe9B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}