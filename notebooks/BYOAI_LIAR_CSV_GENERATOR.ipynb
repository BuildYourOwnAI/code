{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building the BYOAI_LIAR Dataset\n",
        "*A Practical Example of Building Your Own AI, with a little help from AI*\n",
        "\n",
        "## Why we built a LIAR-style dataset for the book\n",
        "For the chapter on scaling, we wanted a model that shows more than speed or accuracy. We wanted something that also explores how to scale trust. The classic LIAR dataset is a good inspiration. It labels short statements on a six-level truth scale: TRUE, mostly-true, half-true, barely-true, FALSE, and pants-fire. That framing fit our goals and also supports teaching. One of the authors teaches trustworthy AI and this gives a steady source of chapter-anchored multiple choice questions.\n",
        "\n",
        "> The LIAR dataset is a large collection of 12,836 short, human-labeled statements about their truthfulness, collected from the fact-checking website PolitiFact.com. It was created by William Yang Wang and released in a 2017 paper titled \"Liar, Liar Pants on Fire”: A New Benchmark Dataset for Fake News Detection, making it a valuable resource for fake news detection research.\n",
        "\n",
        "\n",
        "Our version, which we call **byoai_liar**, uses passages from the book’s chapters to generate short labeled statements. A future classifier can then learn to predict the same six labels. The result doubles as a training set and as an educational asset tied to the book’s topics.\n",
        "\n",
        "## Planning the build with a code assistant\n",
        "We decided to treat the whole exercise as a worked example of using a coding copilot. Most of the work was done with ChatGPT and Gemini. The plan had two parts:\n",
        "\n",
        "1. **Chunk the chapters** into small windows of text that carry enough context to inspire grounded statements.  \n",
        "2. **Generate labeled statements** from those windows, along with a short context phrase and a short reason for the label.\n",
        "\n",
        "We aimed for about 2,500 to 5,000 rows so a small model like T5-small would have something meaningful to learn from. With more than 1,800 chunks available from the drafts, that target looked realistic.\n",
        "\n",
        "## Data preparation\n",
        "We took a fresh snapshot of the chapter drafts and exported each chapter as a UTF-16 plain text file. That ensured simple, repeatable inputs. We then defined the six labels as constants, based on the LIAR model card, and kept them front and center in every prompt and function.\n",
        "\n",
        "Next, we added **subject tags**. A chapter title alone does not always convey the core topics. We asked the code assistant to propose three concise tags per chapter. These tags appear in the chunk file and later guide generation. For example, the deepfake chapter carries tags like media-forensics, voice-cloning, deepfake. The agentic AI chapter carries agentic-ai, planning, tools. These tags flow through the pipeline and help the generator ground contexts without long prompts.\n",
        "\n",
        ">**Tip:** Before writing any code, you can use a chat-based content analysis to define data structures like we did with: `SUBJECT_TAGS`. Here's what we did.\n",
        "Create a new ChatGPT project, upload your chapter `.txt` files into its file folder, and prompt:  \n",
        "*\"Scan each file and extract three concise tags that best represent its subject or theme. Then output a Python constant named SUBJECT_TAGS mapping chapter numbers to those tags.\"*  \n",
        "Review the results, refine the prompt if any tags feel off, and rerun until each chapter is well represented. This step anchors your later code generation in real content, ensuring your constants match the book’s themes rather than generic guesses.\n",
        "\n",
        "## Listing 1: the Chunker\n",
        "The chunker walks each chapter, breaks it into overlapping windows, and emits a `chunks.csv` with:\n",
        "\n",
        "- `chapter`, `chapter_title`, `chunk_id`, `chunk_text`, `subject_tags`\n",
        "\n",
        "The window size took some tuning. We tested different widths and strides, looking for enough context to inspire a claim, but short enough to keep the model focused. We added simple summary prints to show counts, token ranges, and per-chapter coverage. Those prints paid off. Each round of changes was based on facts rather than gut feel.\n",
        "\n",
        "A key decision was to produce enough chunks so the generator could sample several labels per chunk later. That matters because a single passage can inspire both supported claims and claims that go a bit too far. We want the generator to produce that contrast so the classifier has something to learn.\n",
        "\n",
        ">**Tip:** Let your AI code assistant help you find a practical window size before finalizing the chunker.  \n",
        "Upload one or two representative chapter text files and ask:  \n",
        "*\"Write a short script to split this text into overlapping windows and report average word counts per window for different stride and size values.\"*  \n",
        "Run the suggested code, share the output back with the assistant, and ask:  \n",
        "*\"Based on these stats, what window and stride would balance context and focus for training data generation?\"*  \n",
        "This experiment-driven loop helps you converge on numbers that fit your chapters, rather than guessing.\n",
        "\n",
        "## Listing 2: the single-pass Generator\n",
        "Early versions tried a generator plus a separate QA reviewer. That added complexity. The final approach is simpler and more reliable for this use case:\n",
        "\n",
        "**One prompt. One call per row. No second pass.**\n",
        "\n",
        "The generator takes `chunks.csv` and a few simple constants:\n",
        "\n",
        "- `MAX_CHUNKS` controls how many chunks to process in this run.\n",
        "- `N_PER_CHUNK_TARGET` controls how many rows to attempt per chunk.\n",
        "- `LABELS` and `LABEL_WEIGHTS` control the truth distribution.\n",
        "\n",
        "For each chunk, we sample a label according to the weights and create a single prompt that includes the target label, the chapter info, the subject tags, and the chunk text. The model returns exactly three fields:\n",
        "\n",
        "- `statement` in 20 words or fewer  \n",
        "- `context` in 12 words or fewer  \n",
        "- `label_reason` in 8 to 28 words\n",
        "\n",
        "The rules in the prompt keep things consistent: the label drives the claim, the context anchors to a specific term from the passage or tags, and the reason explains the label in natural language. We ask for varied phrasing and concrete terms such as tools, models, datasets, or metrics. We also ask for natural justification wording so we do not get repetitive openings like “The passage states…”\n",
        "\n",
        ">**Tip:** Use your AI code assistant as a **prompt design partner**, not just a coder.  \n",
        "Upload your `chunks.csv` file into the project, then ask:  \n",
        "*\"Given this dataset, help me design a single, efficient prompt that generates a labeled statement, short context, and natural justification for each chunk and target label.\"*  \n",
        "Experiment by pasting a few sample chunks and iterating on the wording of the generation rules together.  \n",
        "When the model starts returning well-balanced examples, copy that exact text into your `GEN_PROMPT` constant.  \n",
        "This interactive process helps you tune the generator’s behavior before you automate it in code.\n",
        "\n",
        "## Light validation instead of heavy post-processing\n",
        "After each generation, we run quick checks:\n",
        "\n",
        "- Word count bounds  \n",
        "- No meta words like “chapter” or “this statement”  \n",
        "- A short-hash dedupe on the statement\n",
        "\n",
        "If a row fails a check, we retry once. If it fails again, we skip it. These small rules keep the output clean without slowing the pipeline.\n",
        "\n",
        "## Output schema and reproducibility\n",
        "The generator writes `byoai_liar.csv` with:\n",
        "\n",
        "- `id, chunk_id, label, statement, context, label_reason, subject_tags, chapter, chapter_title`\n",
        "\n",
        "This makes it easy to trace any statement back to its origin. The fields also give you the ingredients a future classifier or evaluator needs, including a short rationale that can support training or auditing.\n",
        "\n",
        "## Scaling up without surprises\n",
        "To reach 4,000 to 5,000 rows, pick a combination of `MAX_CHUNKS` and `N_PER_CHUNK_TARGET` that hits your target. For example, 600 chunks with 8 rows per chunk yields about 4,800 attempts. After dedupe and bounds checks, the final count lands near the goal. For variety, set temperature around 0.65 to 0.7 and top-p around 0.9. If you want even coverage across chapters, switch the chunk selection from random to a balanced sampler.\n",
        "\n",
        "## A small audit tool helps you stay honest\n",
        "We added a tiny “coherence and label audit” tool that samples random rows from the final CSV and asks a model to rate:\n",
        "\n",
        "- chapter fit  \n",
        "- label fit  \n",
        "- context quality  \n",
        "- reason quality\n",
        "\n",
        "It prints a few lines per row and then averages. If context quality dips, nudge the prompt to ask for a named anchor term from the passage or tags. If reasons look mechanical, remind the model to explain the relationship naturally without report-style phrases. This loop is fast and keeps the dataset from drifting.\n",
        "\n",
        "## What an AI builder learns from this pattern\n",
        "**Design the target first.** The six labels and the output schema framed every decision that followed.  \n",
        "**Break the work into two clear listings.** One prepares focused context. One generates structured rows.  \n",
        "**Keep prompts compact and precise.** One well-designed prompt beats a long chain of fragile steps.  \n",
        "**Use simple checks.** Bounds and dedupe give most of the benefits of a second pass, with a fraction of the effort.  \n",
        "**Iterate on small samples.** Run ten chunks, study the rows, adjust one rule, and run again.  \n",
        "**Instrument the code.** Print basic stats after each run. Simple numbers are better than guesswork.  \n",
        "**Add a tiny auditor.** A lightweight, randomized audit helps you catch drift early and gives you confidence at scale.\n",
        "\n",
        "## Where you can take it next\n",
        "Once the dataset looks good, you can train a small text classifier such as T5-small or a compact encoder-based model. You can also slice the CSV per chapter or per tag to build practice sets for a class. Since each row has a short reason, you can experiment with explanation-aware training, evaluation rubrics, or teaching assistants that quiz readers on chapter topics.\n",
        "\n",
        "That is the core pattern. Start with plain text chapters. Chunk them. Drive a single-pass generator with clear rules. Keep the checks light, the prompts short, and the iterations quick. You will end up with a dataset that is useful for training and also clear enough to support teaching and review.\n"
      ],
      "metadata": {
        "id": "lacQJAvW9OJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 1: BYOAI Chapter Chunker → `byoai_book_chunks.csv`\n",
        "\n",
        "This script scans all chapter `.txt` files, slices them into overlapping\n",
        "sentence windows, and writes the results to `byoai_book_chunks.csv`. Each record\n",
        "contains the chapter number, title, chunk ID, text window, and up to three\n",
        "subject tags representing the chapter’s main themes.\n",
        "\n",
        "Text is read as **UTF-16** (falling back to **UTF-8**), cleaned for encoding\n",
        "artifacts, split into sentences, and filtered to omit short fragments.\n",
        "Helper functions include:\n",
        "`read_text_any()` for robust reading,\n",
        "`sentence_split()` for segmentation,\n",
        "and `make_windows()` for constructing rolling windows using\n",
        "`WINDOW_SIZE` and `WINDOW_STRIDE`.\n",
        "\n",
        "**Output schema:**\n",
        "*chapter | chapter_title | chunk_id | chunk_text | subject_tags*\n",
        "\n",
        "**Example:**\n",
        "\n",
        "*5 | Deep Learning | 42 | “PyTorch introduced eager execution…” | “deep-learning,frameworks,tensors”*\n",
        "\n",
        "The resulting `byoai_book_chunks.csv` provides consistent, context-rich samples for generation, labeling, and analysis while preserving chapter context and\n",
        "subject tags.\n"
      ],
      "metadata": {
        "id": "fVN0mLepIsHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BYOAI Chapter Chunker v3 → chunks.csv -----------------------------------\n",
        "# Scans all *.txt chapters (UTF-16 preferred; falls back to UTF-8),\n",
        "# slices rolling sentence windows, and writes a lean CSV for generators.\n",
        "#\n",
        "# Output schema (per row):\n",
        "#   chapter (int) | chapter_title (str) | chunk_id (int) | chunk_text (str)\n",
        "#   subject_tags (str; comma-separated, ≤3 per chapter)\n",
        "\n",
        "import re\n",
        "import csv\n",
        "import glob\n",
        "import pathlib\n",
        "import collections\n",
        "\n",
        "# ===================== CONFIG =====================\n",
        "CHAPTER_GLOB   = \"*.txt\"     # pattern for chapter text files\n",
        "OUTPUT_CSV     = \"byoai_book_chunks.csv\"\n",
        "WINDOW_SIZE    = 4           # sentences per window\n",
        "WINDOW_STRIDE  = 3           # step size between windows\n",
        "MIN_CHARS      = 260         # drop very short/meta fragments\n",
        "# ==================================================\n",
        "\n",
        "# Canonical chapter titles (0: Foreword, 1: Introduction, \"Chapter N\" → N+1)\n",
        "CHAPTER_TITLES = {\n",
        "     0: \"Foreword – Robo Interviews Clément Delangue\",\n",
        "     1: \"Introduction – The Gold Rush Paradox\",\n",
        "     2: \"AI Survival Kit\",\n",
        "     3: \"Prepping Data for AI\",\n",
        "     4: \"Classical Machine Learning\",\n",
        "     5: \"Deep Learning\",\n",
        "     6: \"Neuron Building Blocks\",\n",
        "     7: \"Generative AI\",\n",
        "     8: \"Breaking-Securing AI\",\n",
        "     9: \"Deepfake Defense\",\n",
        "    10: \"AI At Scale\",\n",
        "    11: \"AI Ethics and Governance\",\n",
        "    12: \"Agentic AI\",\n",
        "    13: \"Commit to Contribute\",\n",
        "}\n",
        "\n",
        "# === SUBJECT TAGS (3 per chapter max) ===\n",
        "# Used for guiding statement placement and downstream grouping.\n",
        "SUBJECT_TAGS = {\n",
        "     0: [\"open-source\", \"community\", \"ai\"],                 # Foreword\n",
        "     1: [\"ai\", \"open-source\", \"builder\"],                   # Introduction\n",
        "     2: [\"ai\", \"tool-chain\", \"notebooks\"],                  # Ch1 – Survival Kit\n",
        "     3: [\"data-prep\", \"feature-engineering\", \"rag\"],        # Ch2 – Prepping Data\n",
        "     4: [\"machine-learning\", \"classification\", \"evaluation\"],# Ch3 – Classical ML\n",
        "     5: [\"deep-learning\", \"frameworks\", \"tensors\"],         # Ch4 – Deep Learning\n",
        "     6: [\"neural-networks\", \"cnn\", \"transformers\"],         # Ch5 – Neuron Blocks\n",
        "     7: [\"generative-ai\", \"diffusion\", \"gans\"],             # Ch6 – Gen AI\n",
        "     8: [\"security\", \"red-team\", \"guardrails\"],             # Ch7 – Break/Secure\n",
        "     9: [\"media-forensics\", \"voice-cloning\", \"deepfake\"],   # Ch8 – Deepfake Def.\n",
        "    10: [\"mlops\", \"scaling\", \"deployment\"],                 # Ch9 – At Scale\n",
        "    11: [\"ethics\", \"governance\", \"privacy\"],                # Ch10 – Ethics/Gov\n",
        "    12: [\"agentic-ai\", \"planning\", \"tools\"],                # Ch11 – Agentic AI\n",
        "    13: [\"open-source\", \"community\", \"contribution\"],       # Ch12 – Commit\n",
        "}\n",
        "\n",
        "def fix_mojibake(t: str) -> str:\n",
        "    \"\"\"Light cleanup for common encoding artifacts from word processors.\"\"\"\n",
        "    repl = {\n",
        "        \"‚Äôs\": \"’s\", \"‚Äô\": \"’\", \"‚Äú\": \"“\", \"‚Äù\": \"”\",\n",
        "        \"‚Äì\": \"–\",  \"‚Äî\": \"—\", \"‚Ä¶\": \"…\", \"Â\": \"\"\n",
        "    }\n",
        "    for k, v in repl.items():\n",
        "        t = t.replace(k, v)\n",
        "    return t\n",
        "\n",
        "def read_text_any(path: str) -> str:\n",
        "    \"\"\"Read file as UTF-16 first, then UTF-8 as a fallback; normalize newlines.\"\"\"\n",
        "    try:\n",
        "        txt = open(path, \"r\", encoding=\"utf-16\").read()\n",
        "    except Exception:\n",
        "        txt = open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\").read()\n",
        "    return fix_mojibake(txt.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\"))\n",
        "\n",
        "def sentence_split(text: str):\n",
        "    \"\"\"Split text into sentences using simple punctuation-based rule.\"\"\"\n",
        "    parts = re.split(r\"(?<=[.!?])\\s+\", text)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "def make_windows(text: str, n: int, stride: int):\n",
        "    \"\"\"Create rolling windows of n sentences with a given stride.\"\"\"\n",
        "    sents = sentence_split(text)\n",
        "    return [\" \".join(sents[i:i+n]) for i in range(0, len(sents), stride)]\n",
        "\n",
        "def parse_chapter_from_filename(fname: str):\n",
        "    \"\"\"Infer (chapter_index, canonical_title) from filename text.\"\"\"\n",
        "    low = fname.lower()\n",
        "    if \"foreword\" in low:\n",
        "        return 0, CHAPTER_TITLES[0]\n",
        "    if \"introduction\" in low:\n",
        "        return 1, CHAPTER_TITLES[1]\n",
        "    m = re.search(r\"chapter\\D*?(\\d{1,2})\", low)\n",
        "    if m:\n",
        "        n = int(m.group(1)) + 1\n",
        "        return n, CHAPTER_TITLES.get(n, pathlib.Path(fname).stem)\n",
        "    # Fallback to Introduction if naming is irregular\n",
        "    return 1, CHAPTER_TITLES[1]\n",
        "\n",
        "def tags_for_chapter(chapter: int) -> str:\n",
        "    \"\"\"Return comma-separated subject tags for a chapter (≤3).\"\"\"\n",
        "    tags = SUBJECT_TAGS.get(chapter, [\"ai\"])\n",
        "    return \",\".join(tags[:3]) if tags else \"ai\"\n",
        "\n",
        "def main():\n",
        "    \"\"\"Scan chapter files, window sentences, filter, and write CSV.\"\"\"\n",
        "    files = sorted(glob.glob(CHAPTER_GLOB))\n",
        "    if not files:\n",
        "        raise SystemExit(f\"No files matched {CHAPTER_GLOB}\")\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    # Iterate each text file and produce rolling windows\n",
        "    for path in files:\n",
        "        if not path.lower().endswith(\".txt\"):\n",
        "            continue\n",
        "\n",
        "        # Parse chapter index and canonical title\n",
        "        chapter, title = parse_chapter_from_filename(path)\n",
        "\n",
        "        # Read raw text and skip empties\n",
        "        text = read_text_any(path)\n",
        "        if not text.strip():\n",
        "            continue\n",
        "\n",
        "        # Build sentence windows for this chapter\n",
        "        windows = make_windows(text, WINDOW_SIZE, WINDOW_STRIDE)\n",
        "\n",
        "        # Collect non-trivial windows with metadata and tags\n",
        "        chunk_id = 0\n",
        "        for w in windows:\n",
        "            if len(w) < MIN_CHARS:\n",
        "                continue\n",
        "            rows.append({\n",
        "                \"chapter\": chapter,\n",
        "                \"chapter_title\": title,\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"chunk_text\": w,\n",
        "                \"subject_tags\": tags_for_chapter(chapter),\n",
        "            })\n",
        "            chunk_id += 1\n",
        "\n",
        "    # Write CSV with BOM for spreadsheet-friendly behavior\n",
        "    with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
        "        w = csv.DictWriter(\n",
        "            f,\n",
        "            fieldnames=[\n",
        "                \"chapter\", \"chapter_title\", \"chunk_id\",\n",
        "                \"chunk_text\", \"subject_tags\"\n",
        "            ]\n",
        "        )\n",
        "        w.writeheader()\n",
        "        for r in rows:\n",
        "            w.writerow(r)\n",
        "\n",
        "    # Console summary to sanity-check coverage\n",
        "    by_chapter = collections.Counter(r[\"chapter\"] for r in rows)\n",
        "    print(f\"Wrote {len(rows)} rows to {OUTPUT_CSV}\")\n",
        "    print(\"Windows per chapter:\", dict(sorted(by_chapter.items())))\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lis2S_MxlzZQ",
        "outputId": "c765411d-d5a1-4049-b76f-c897eaae8be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote 1891 rows to byoai_book_chunks.csv\n",
            "Windows per chapter: {0: 54, 1: 73, 2: 136, 3: 190, 4: 182, 5: 209, 6: 92, 7: 171, 8: 157, 9: 140, 10: 138, 11: 99, 12: 151, 13: 99}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 1A: BYOAI Chapter Chunk Quality Check\n",
        "\n",
        "This script validates and summarizes the `byoai_book_chunks.csv` output from Listing 1.\n",
        "It checks structural integrity, verifies column presence, and reports per-chapter\n",
        "and global statistics such as word and sentence counts. It also measures\n",
        "distribution balance, identifies overly short or long chunks, and prints\n",
        "keyword and subject-tag coverage to assess content diversity.\n",
        "\n",
        "Key outputs include per-chapter averages (`mean_words`, `mean_sents`), overall\n",
        "word-length ranges, and tag frequency rankings. Chunks below `SHORT_LIMIT`\n",
        "or above `LONG_LIMIT` are flagged, with a sample preview printed for inspection.\n",
        "Together, these diagnostics ensure the dataset remains consistent, balanced,\n",
        "and thematically representative for downstream text generation and analysis."
      ],
      "metadata": {
        "id": "ll40MnmbI9Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === BYOAI Chapter Chunk Quality Check ========================================\n",
        "# Validates the integrity, balance, and richness of 'chunks.csv' produced by\n",
        "# the BYOAI Chapter Chunker (Listing 1). Reports stats, keyword coverage,\n",
        "# and short/long chunk distributions to guide downstream generation.\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from shutil import get_terminal_size\n",
        "\n",
        "# ===================== CONFIG =====================\n",
        "CHUNKS_CSV  = \"byoai_book_chunks.csv\"\n",
        "REQ_COLS    = {\"chapter\", \"chapter_title\", \"chunk_id\",\n",
        "               \"chunk_text\", \"subject_tags\"}\n",
        "SHORT_LIMIT = 50      # minimum acceptable word count per chunk\n",
        "LONG_LIMIT  = 220     # threshold for overly long windows\n",
        "# ==================================================\n",
        "\n",
        "# --- Load CSV and verify columns ---\n",
        "df = pd.read_csv(CHUNKS_CSV, encoding=\"utf-8-sig\")\n",
        "print(f\"Loaded {len(df):,} chunks from {CHUNKS_CSV}\")\n",
        "\n",
        "missing = REQ_COLS - set(df.columns)\n",
        "if missing:\n",
        "    raise SystemExit(f\"Missing columns: {sorted(missing)}\")\n",
        "if df.empty:\n",
        "    raise SystemExit(\"No rows found in chunks.csv\")\n",
        "\n",
        "# --- Basic metrics per chunk ---\n",
        "df[\"chunk_text\"] = df[\"chunk_text\"].astype(str)\n",
        "df[\"word_len\"]   = df[\"chunk_text\"].apply(lambda t: len(t.split()))\n",
        "df[\"sent_count\"] = df[\"chunk_text\"].apply(\n",
        "    lambda t: len(re.findall(r\"[.!?](?:\\s|$)\", t))\n",
        ")\n",
        "\n",
        "# --- Per-chapter summary ---\n",
        "summary = (\n",
        "    df.groupby([\"chapter\", \"chapter_title\"], as_index=False)\n",
        "      .agg(\n",
        "          chunks=(\"chunk_id\", \"count\"),\n",
        "          mean_words=(\"word_len\", \"mean\"),\n",
        "          min_words=(\"word_len\", \"min\"),\n",
        "          max_words=(\"word_len\", \"max\"),\n",
        "          mean_sents=(\"sent_count\", \"mean\"),\n",
        "      )\n",
        "      .round(2)\n",
        ")\n",
        "summary[\"share_pct\"] = (summary[\"chunks\"] / len(df) * 100).round(1)\n",
        "\n",
        "print(\"\\n=== Per-Chapter Chunk Stats ===\")\n",
        "term_width = get_terminal_size((120, 20)).columns\n",
        "print(summary.sort_values(\"chapter\")\n",
        "              .to_string(index=False, line_width=term_width))\n",
        "\n",
        "# --- Global summary ---\n",
        "print(\"\\n=== Global Summary ===\")\n",
        "print(f\"Mean words per chunk: {df['word_len'].mean():.1f}\")\n",
        "print(f\"Shortest chunk: {df['word_len'].min()} words\")\n",
        "print(f\"Longest chunk:  {df['word_len'].max()} words\")\n",
        "print(f\"Mean sentences per chunk: {df['sent_count'].mean():.2f}\")\n",
        "\n",
        "# --- Subject tag coverage ---\n",
        "def split_tags(s):\n",
        "    return [t.strip() for t in str(s).split(\",\") if t.strip()]\n",
        "\n",
        "tag_counts = Counter(tag for tags in df[\"subject_tags\"].map(split_tags)\n",
        "                     for tag in tags)\n",
        "\n",
        "print(\"\\n=== Subject Tag Coverage (top 20) ===\")\n",
        "for tag, count in tag_counts.most_common(20):\n",
        "    print(f\"{tag:<20} {count}\")\n",
        "\n",
        "# --- Keyword occurrence overview ---\n",
        "KEY_TERMS = [\"ai\", \"data\", \"model\", \"learning\",\n",
        "             \"open\", \"ethics\", \"voice\", \"agent\"]\n",
        "totals = Counter()\n",
        "for text in df[\"chunk_text\"]:\n",
        "    text_l = text.lower()\n",
        "    for term in KEY_TERMS:\n",
        "        totals[term] += text_l.count(term)\n",
        "\n",
        "print(\"\\n=== Keyword Occurrence Totals ===\")\n",
        "for term, count in sorted(totals.items(), key=lambda kv: -kv[1]):\n",
        "    print(f\"{term:<12} {count}\")\n",
        "\n",
        "# --- Flag short and long chunks ---\n",
        "short_df = df[df[\"word_len\"] < SHORT_LIMIT]\n",
        "long_df  = df[df[\"word_len\"] > LONG_LIMIT]\n",
        "\n",
        "print(f\"\\nChunks under {SHORT_LIMIT} words: {len(short_df)} \"\n",
        "      f\"({len(short_df)/len(df):.1%})\")\n",
        "print(f\"Chunks over {LONG_LIMIT} words: {len(long_df)} \"\n",
        "      f\"({len(long_df)/len(df):.1%})\")\n",
        "\n",
        "# --- Notebook-friendly preview ---\n",
        "def preview(frame, label):\n",
        "    if frame.empty:\n",
        "        return\n",
        "    cols = [\"chapter\",\"chapter_title\",\"chunk_id\",\"word_len\",\"chunk_text\"]\n",
        "    print(f\"\\nExample {label} chunks:\")\n",
        "    try:\n",
        "        from IPython.display import display\n",
        "        display(frame.sample(min(5, len(frame)))[cols])\n",
        "    except Exception:\n",
        "        print(frame.sample(min(3, len(frame)))[cols].to_string(index=False))\n",
        "\n",
        "preview(short_df, \"short\")\n",
        "preview(long_df, \"long\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Xv8T3223nPp6",
        "outputId": "c878f5b0-99a0-42b6-9070-e44563c4ab0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1,891 chunks from byoai_book_chunks.csv\n",
            "\n",
            "=== Per-Chapter Chunk Stats ===\n",
            " chapter                               chapter_title  chunks  mean_words  min_words  max_words  mean_sents  share_pct\n",
            "       0 Foreword – Robo Interviews Clément Delangue      54       84.17         33        150        3.94        2.9\n",
            "       1        Introduction – The Gold Rush Paradox      73       67.21         46        106        4.00        3.9\n",
            "       2                             AI Survival Kit     136       92.84         44        284        4.00        7.2\n",
            "       3                        Prepping Data for AI     190       81.64         39        231        3.99       10.0\n",
            "       4                  Classical Machine Learning     182       84.37         41        246        4.00        9.6\n",
            "       5                               Deep Learning     209       80.19         32        256        3.99       11.1\n",
            "       6                      Neuron Building Blocks      92      109.51         14        360        3.98        4.9\n",
            "       7                               Generative AI     171       94.39         37        354        4.00        9.0\n",
            "       8                        Breaking-Securing AI     157       72.43         31        234        3.99        8.3\n",
            "       9                            Deepfake Defense     140       87.64         16        252        3.98        7.4\n",
            "      10                                 AI At Scale     138       82.26         37        279        4.00        7.3\n",
            "      11                    AI Ethics and Governance      99       68.24         23        142        3.98        5.2\n",
            "      12                                  Agentic AI     151       82.37         35        233        4.00        8.0\n",
            "      13                        Commit to Contribute      99       82.00         30        258        3.99        5.2\n",
            "\n",
            "=== Global Summary ===\n",
            "Mean words per chunk: 83.7\n",
            "Shortest chunk: 14 words\n",
            "Longest chunk:  360 words\n",
            "Mean sentences per chunk: 3.99\n",
            "\n",
            "=== Subject Tag Coverage (top 20) ===\n",
            "ai                   263\n",
            "open-source          226\n",
            "deep-learning        209\n",
            "frameworks           209\n",
            "tensors              209\n",
            "data-prep            190\n",
            "feature-engineering  190\n",
            "rag                  190\n",
            "machine-learning     182\n",
            "classification       182\n",
            "evaluation           182\n",
            "generative-ai        171\n",
            "diffusion            171\n",
            "gans                 171\n",
            "security             157\n",
            "red-team             157\n",
            "guardrails           157\n",
            "community            153\n",
            "agentic-ai           151\n",
            "planning             151\n",
            "\n",
            "=== Keyword Occurrence Totals ===\n",
            "ai           3672\n",
            "model        2205\n",
            "data         1715\n",
            "open         580\n",
            "agent        397\n",
            "learning     388\n",
            "voice        150\n",
            "ethics       47\n",
            "\n",
            "Chunks under 50 words: 145 (7.7%)\n",
            "Chunks over 220 words: 41 (2.2%)\n",
            "\n",
            "Example short chunks:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      chapter         chapter_title  chunk_id  word_len  \\\n",
              "909         5         Deep Learning        52        32   \n",
              "384        12            Agentic AI       149        43   \n",
              "965         5         Deep Learning       108        49   \n",
              "409        13  Commit to Contribute        23        48   \n",
              "1763       10           AI At Scale       137        37   \n",
              "\n",
              "                                             chunk_text  \n",
              "909   model = tf.keras.Sequential([\\n    tf.keras.la...  \n",
              "384   Finally, we come back to transparency. Open-so...  \n",
              "965   This loss value is the key signal we use to im...  \n",
              "409   For Robby, open innovation was a memory. A rem...  \n",
              "1763  “Liar, Liar Pants on Fire”: A New Benchmark Da...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ac7fc4d-2fc3-4565-8076-619ddd4d0f2d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chapter</th>\n",
              "      <th>chapter_title</th>\n",
              "      <th>chunk_id</th>\n",
              "      <th>word_len</th>\n",
              "      <th>chunk_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>909</th>\n",
              "      <td>5</td>\n",
              "      <td>Deep Learning</td>\n",
              "      <td>52</td>\n",
              "      <td>32</td>\n",
              "      <td>model = tf.keras.Sequential([\\n    tf.keras.la...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384</th>\n",
              "      <td>12</td>\n",
              "      <td>Agentic AI</td>\n",
              "      <td>149</td>\n",
              "      <td>43</td>\n",
              "      <td>Finally, we come back to transparency. Open-so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>965</th>\n",
              "      <td>5</td>\n",
              "      <td>Deep Learning</td>\n",
              "      <td>108</td>\n",
              "      <td>49</td>\n",
              "      <td>This loss value is the key signal we use to im...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>409</th>\n",
              "      <td>13</td>\n",
              "      <td>Commit to Contribute</td>\n",
              "      <td>23</td>\n",
              "      <td>48</td>\n",
              "      <td>For Robby, open innovation was a memory. A rem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1763</th>\n",
              "      <td>10</td>\n",
              "      <td>AI At Scale</td>\n",
              "      <td>137</td>\n",
              "      <td>37</td>\n",
              "      <td>“Liar, Liar Pants on Fire”: A New Benchmark Da...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ac7fc4d-2fc3-4565-8076-619ddd4d0f2d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8ac7fc4d-2fc3-4565-8076-619ddd4d0f2d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8ac7fc4d-2fc3-4565-8076-619ddd4d0f2d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6d9d3bf3-17c3-46a1-82b1-cf82a3506c34\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6d9d3bf3-17c3-46a1-82b1-cf82a3506c34')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6d9d3bf3-17c3-46a1-82b1-cf82a3506c34 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"preview(long_df, \\\"long\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"chapter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 5,\n        \"max\": 13,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          12,\n          10,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chapter_title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Agentic AI\",\n          \"AI At Scale\",\n          \"Deep Learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 54,\n        \"min\": 23,\n        \"max\": 149,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          149,\n          137,\n          108\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 32,\n        \"max\": 49,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          43,\n          37,\n          49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Finally, we come back to transparency. Open-source frameworks like LangChain and CrewAI don't just give developers tools; they give them visibility into how agents reason, plan, and act. This matters. If agentic AI is going to be trusted, it needs to be understandable.\",\n          \"\\u201cLiar, Liar Pants on Fire\\u201d: A New Benchmark Dataset for Fake News Detection. Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Association for Computational Linguistics, 2017, pp. 422\\u2013426. https://aclanthology.org/P17-2067.\",\n          \"This loss value is the key signal we use to improve the model. But how do we know which weights to change, and by how much? Gradients\\nTo improve the model, the network needs to understand how much each parameter (weight or bias) contributed to the error. Enter gradients.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example long chunks:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      chapter           chapter_title  chunk_id  word_len  \\\n",
              "1302        7           Generative AI       144       237   \n",
              "1577        9        Deepfake Defense        91       240   \n",
              "586         3    Prepping Data for AI       101       226   \n",
              "1114        6  Neuron Building Blocks        48       256   \n",
              "74          2         AI Survival Kit        74       248   \n",
              "\n",
              "                                             chunk_text  \n",
              "1302  It captures the essence of how modern AI assis...  \n",
              "1577  These final steps bring together the data, the...  \n",
              "586   import pandas as pd\\n\\n# Helper functions are ...  \n",
              "1114  Here is an example:\\n\\n\\nFigure 5-6 Sample out...  \n",
              "74    You’ve Been Selected as the Next Avenger!',\\n ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8350af6-c333-48e4-82bf-e6f283b07cc6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chapter</th>\n",
              "      <th>chapter_title</th>\n",
              "      <th>chunk_id</th>\n",
              "      <th>word_len</th>\n",
              "      <th>chunk_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1302</th>\n",
              "      <td>7</td>\n",
              "      <td>Generative AI</td>\n",
              "      <td>144</td>\n",
              "      <td>237</td>\n",
              "      <td>It captures the essence of how modern AI assis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1577</th>\n",
              "      <td>9</td>\n",
              "      <td>Deepfake Defense</td>\n",
              "      <td>91</td>\n",
              "      <td>240</td>\n",
              "      <td>These final steps bring together the data, the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>586</th>\n",
              "      <td>3</td>\n",
              "      <td>Prepping Data for AI</td>\n",
              "      <td>101</td>\n",
              "      <td>226</td>\n",
              "      <td>import pandas as pd\\n\\n# Helper functions are ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1114</th>\n",
              "      <td>6</td>\n",
              "      <td>Neuron Building Blocks</td>\n",
              "      <td>48</td>\n",
              "      <td>256</td>\n",
              "      <td>Here is an example:\\n\\n\\nFigure 5-6 Sample out...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>2</td>\n",
              "      <td>AI Survival Kit</td>\n",
              "      <td>74</td>\n",
              "      <td>248</td>\n",
              "      <td>You’ve Been Selected as the Next Avenger!',\\n ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8350af6-c333-48e4-82bf-e6f283b07cc6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c8350af6-c333-48e4-82bf-e6f283b07cc6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c8350af6-c333-48e4-82bf-e6f283b07cc6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-84414150-5644-4e4d-b093-7b0aab1c6f7f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-84414150-5644-4e4d-b093-7b0aab1c6f7f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-84414150-5644-4e4d-b093-7b0aab1c6f7f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"preview(long_df, \\\"long\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"chapter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 2,\n        \"max\": 9,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          9,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chapter_title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Deepfake Defense\",\n          \"AI Survival Kit\",\n          \"Prepping Data for AI\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 35,\n        \"min\": 48,\n        \"max\": 144,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          91,\n          74,\n          101\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 226,\n        \"max\": 256,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          240,\n          248,\n          226\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"These final steps bring together the data, the embeddings, and the model into one cohesive process. # Text to synthesize (try a sentence the speaker would plausibly say)\\ntext = (\\\"Hey ladies and gentlemen, thank you for tuning in to the\\\"\\n        \\\"Wild Ducks podcast featuring your host, Jerry Cuomo.\\\")\\n\\n# Tokenize text and move tensors to the same device as the model\\ninputs = processor(text=text, return_tensors= \\\"pt\\\")\\ninputs = {k: v.to(device) for k, v in inputs.items()}\\n\\n# Generate waveform directly; SpeechT5 calls HiFi-GAN under the hood\\nwith torch.no_grad():\\n    speech = model.generate_speech(\\n        inputs[\\\"input_ids\\\"],                 # tokenized text\\n        speaker_embeddings=speaker_embeddings,  # voice identity\\n        vocoder=vocoder                     # waveform generator\\n    )  # returns a 1D waveform tensor\\n\\n# Save WAV and provide quick playback of cloned sample\\nout_wav = \\\"Jerry-Cloned-Sample01.wav\\\"\\nsf.write(out_wav, speech.cpu().numpy(), 16000)  # 16 kHz to match training\\nprint(f\\\"Saved: {out_wav}\\\")\\n\\nListing 8-5 Test voice synthesis using the fine-tuned SpeechT5 model\\nWe begin by loading the fine-tuned SpeechT5 model, the HiFi-GAN vocoder, and the saved speaker embedding. The embedding carries Jerry\\u2019s distinct vocal traits, and when applied to new text, it helps the model generate speech that sounds recognizably like him. For this test, we used a familiar line from one of Jerry\\u2019s Wild Ducks podcast recordings:\\n\\u201cHey ladies and gentlemen, thank you for tuning in to \\nthe Wild Ducks podcast featuring your host, Jerry Cuomo.\\u201d\\nOnce the text is processed, the model generates a spectrogram that captures the frequency and intensity patterns of the synthesized audio.\",\n          \"You\\u2019ve Been Selected as the Next Avenger!',\\n    'Free Batmobile Test Drive!',\\n    'Quarterly sales report is due next Monday',\\n    'Please review the attached budget proposal for next quarter',\\n    'Reminder: Team meeting on Thursday at 9 AM',\\n    'Your invoice for services rendered is attached',\\n    'Client feedback report from last week\\u2019s presentation'\\n]\\n\\n# Labels for spam (1) and non-spam (0)\\nlabels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\\n\\n# Convert emails to a bag-of-words representation using bi-grams\\nvectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english')\\nX = vectorizer.fit_transform(emails)\\n\\n# Create and train the Na\\u00efve Bayes classifier\\nmodel = MultinomialNB()\\nmodel.fit(X, labels)\\n\\n# Test the model with new emails\\nnew_emails = [\\n    'Congratulations, You\\u2019ve Been Chosen as the Next Avenger!',\\n    'Final review of training schedule',\\n    'Free ticket to the Superhero Conference\\u2014Register now!',\\n    'Please review the attached project plan for the upcoming quarter'\\n]\\nX_new = vectorizer.transform(new_emails)\\npredictions = model.predict(X_new)\\n\\n# Display predictions for each email\\nfor email, prediction in zip(new_emails, predictions):\\n    print(f\\\"Email: '{email}' is {'spam' if prediction == 1 else 'not spam'}\\\")\\nListing 1-1 Classifying Emails Using Na\\u00efve Bayes\\nRunning the code, we see how a Na\\u00efve Bayes classifier can quickly sort superhero-themed emails as spam or not. Initially, the model is unaware of what constitutes spam in an email. The key step is vectorizer.fit_transform(emails), which converts text into numbers so the model can use it. With that numeric representation, model.fit(X, labels) trains a MultinomialNB classifier to spot patterns, such as the frequent use of words like free or register.\",\n          \"import pandas as pd\\n\\n# Helper functions are defined (but not shown here) to:\\n# normalize_join_key, dedupe_info, aggregate_powers,\\n# prune_present, compute_opr_sdr, merge_and_save\\n\\n# Load\\ninfo_df = pd.read_csv(SUPERHEROES_INFO_CLEAN_URL)\\npowers_df = pd.read_csv(SUPERHEROES_POWERS_URL)\\n\\n# Prep\\npowers_df = normalize_join_key(powers_df)\\ninfo_df = dedupe_info(info_df)\\npowers_df = aggregate_powers(powers_df)\\n\\n# Power spec\\nOFFENSIVE_POWERS = [\\n    'Super Strength','Energy Blasts','Weapons Master','Marksmanship',\\n    'Telekinesis','Cryokinesis','Fire Control','Power Augmentation',\\n    'Animal Oriented Powers','Super Speed'\\n]\\nDEFENSIVE_POWERS = [\\n    'Durability','Invulnerability','Force Fields','Energy Absorption',\\n    'Regeneration','Immortality','Camouflage','Phasing',\\n    'Enhanced Senses','Teleportation'\\n]\\nDUAL_WEIGHTS = {\\n    'Magic': {'OPR': 0.7, 'SDR': 0.3},\\n    'Super Speed': {'OPR': 0.7, 'SDR': 0.3},\\n}\\n\\n# Build features\\noff_base, def_base, dual_present, pcols = prune_present(\\n    powers_df, OFFENSIVE_POWERS, DEFENSIVE_POWERS, DUAL_WEIGHTS\\n)\\nratings = compute_opr_sdr(powers_df, off_base, def_base, DUAL_WEIGHTS, pcols)\\n\\n# Merge and save\\ninfo_with_ratings = merge_and_save(info_df, ratings, INFO_POWERS_FILE)\\nListing 2-6 Hero Power Metrics Enhancement\\nThis example, like many others in the book, uses a series of helper functions to keep the listings concise and focused on key ideas. For instance, compute_opr_sdr() handles the summing of selected powers into overall OPR and SDR scores, applies small defensive weights to dual-role abilities such as Magic and Super Speed, and adds a coverage flag so missing rows aren\\u2019t mistaken for zeros. The other helpers follow a similar pattern\\u2014each performing a single, well-defined task for clarity and reuse. You can explore them in the full code listing to see how they fit together in practice before the results are merged and saved as superheroes_info_powers.csv.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2: BYOAI LIAR-Style Generator → `byoai_liar.csv`\n",
        "\n",
        "This generator transforms chapter chunks into concise, labeled statements\n",
        "using the OpenAI API. Each row in the output CSV contains a short claim,\n",
        "context phrase, and brief justification paired with a truthfulness label\n",
        "drawn from the classic *LIAR* dataset categories (`TRUE`, `mostly-true`,\n",
        "`half-true`, `barely-true`, `FALSE`, `pants-fire`). Labels are sampled using\n",
        "weighted probabilities to maintain a balanced overall mix.\n",
        "\n",
        "For every passage in `byoai_book_chunks.csv`, the script composes a compact\n",
        "prompt embedding the target label, chapter metadata, and subject tags. The\n",
        "model (`gpt-4o-mini`) returns a JSON response containing three fields:\n",
        "`statement`, `context`, and `label_reason`. Lightweight validators enforce\n",
        "length limits, remove meta references (like “chapter” or “book”), and ensure\n",
        "syntactic completeness. Deduplication guards prevent near-identical\n",
        "statements from repeating.\n",
        "\n",
        "Output fields include:\n",
        "\n",
        "*id, chunk_id, label, statement, context, label_reason,\n",
        "subject_tags, chapter, chapter_title*\n",
        "\n",
        "Together, these labeled micro-claims form a synthetic fact-checking corpus\n",
        "aligned to the book’s themes. The dataset can be used to train or evaluate\n",
        "classification models that reason about factual accuracy, context grounding,\n",
        "and evidence alignment—mirroring LIAR’s structure while drawing from\n",
        "BYOAI’s original chapter content.\n",
        "\n",
        "In short, this script lets AI **generate a dataset about itself** —\n",
        "a living demonstration of the book’s theme: *using AI to build your own AI.*\n",
        "\n",
        "**REQUIRES:**  \n",
        "`byoai_book_chunks.csv` (from Listings 1), an active OpenAI API key stored via  \n",
        "`google.colab.userdata`, and the `openai` + `pandas` libraries."
      ],
      "metadata": {
        "id": "palgEvuZJnpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === BYOAI LIAR-Style Generator (single-pass; OpenAI >= 1.0) ================\n",
        "# Output CSV schema:\n",
        "#   id,chunk_id,label,statement,context,label_reason,subject_tags,chapter,chapter_title\n",
        "#\n",
        "# Purpose:\n",
        "#   Generate concise, labeled statements with brief context and justification\n",
        "#   directly from chunks.csv using a single, tight GEN prompt per row.\n",
        "#\n",
        "# Requirements:\n",
        "#   pip install --quiet openai pandas\n",
        "#   from google.colab import userdata\n",
        "#   userdata.set('OPENAI_API_KEY', '...')\n",
        "# ============================================================================\n",
        "\n",
        "import os, csv, json, re, time, random, math, hashlib\n",
        "from dataclasses import dataclass, asdict\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# ===================== CONFIG =====================\n",
        "CHUNKS_CSV               = \"byoai_book_chunks.csv\"\n",
        "OUTPUT_CSV               = \"byoai_liar.csv\"\n",
        "\n",
        "# Sampling targets\n",
        "MAX_CHUNKS               = 10           # how many chunks to process\n",
        "N_PER_CHUNK_TARGET       = 2            # rows to generate per chunk\n",
        "RETRIES_PER_ROW          = 1            # retry attempts if checks fail\n",
        "\n",
        "# IDs and pacing\n",
        "BASE_ID                  = 100000\n",
        "RANDOM_SEED              = 42\n",
        "PAUSE_SEC                = 0.02         # gentle pacing\n",
        "\n",
        "# Labels & weights (global target mix)\n",
        "LABELS = [\"TRUE\", \"mostly-true\", \"half-true\", \"barely-true\", \"FALSE\", \"pants-fire\"]\n",
        "LABEL_WEIGHTS = [0.20, 0.18, 0.24, 0.18, 0.12, 0.08]\n",
        "\n",
        "# Chapter-tag mapping (fallback if subject_tags column not present)\n",
        "SUBJECT_TAGS = {\n",
        "     0: [\"open-source\", \"community\", \"ai\"],                 # Foreword\n",
        "     1: [\"ai\", \"open-source\", \"builder\"],                   # Introduction\n",
        "     2: [\"ai\", \"tool-chain\", \"notebooks\"],                  # Ch1 – Survival Kit\n",
        "     3: [\"data-prep\", \"feature-engineering\", \"rag\"],        # Ch2 – Prepping Data\n",
        "     4: [\"machine-learning\", \"classification\", \"evaluation\"],# Ch3 – Classical ML\n",
        "     5: [\"deep-learning\", \"frameworks\", \"tensors\"],         # Ch4 – Deep Learning\n",
        "     6: [\"neural-networks\", \"cnn\", \"transformers\"],         # Ch5 – Neuron Blocks\n",
        "     7: [\"generative-ai\", \"diffusion\", \"gans\"],             # Ch6 – Gen AI\n",
        "     8: [\"security\", \"red-team\", \"guardrails\"],             # Ch7 – Break/Secure\n",
        "     9: [\"media-forensics\", \"voice-cloning\", \"deepfake\"],   # Ch8 – Deepfake Def.\n",
        "    10: [\"mlops\", \"scaling\", \"deployment\"],                 # Ch9 – At Scale\n",
        "    11: [\"ethics\", \"governance\", \"privacy\"],                # Ch10 – Ethics/Gov\n",
        "    12: [\"agentic-ai\", \"planning\", \"tools\"],                # Ch11 – Agentic AI\n",
        "    13: [\"open-source\", \"community\", \"contribution\"],       # Ch12 – Commit\n",
        "}\n",
        "\n",
        "# Light validation bounds\n",
        "META_BAN     = re.compile(r\"\\b(chapter|this chapter|book|this book|this statement)\\b\", re.I)\n",
        "STMT_MAX_W   = 20\n",
        "CTX_MAX_W    = 12\n",
        "RSN_MIN_W    = 8\n",
        "RSN_MAX_W    = 28\n",
        "\n",
        "# Dedupe control\n",
        "DEDUP_HASH_LEN = 60       # portion of statement to hash\n",
        "# ==================================================\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# ============ OpenAI client ============\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise SystemExit(\"Missing OPENAI_API_KEY in Colab userdata.\")\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# ============ CSV model ============\n",
        "HEADERS = [\n",
        "    \"id\",\"chunk_id\",\"label\",\"statement\",\"context\",\"label_reason\",\n",
        "    \"subject_tags\",\"chapter\",\"chapter_title\"\n",
        "]\n",
        "\n",
        "@dataclass\n",
        "class Row:\n",
        "    id: int\n",
        "    chunk_id: int\n",
        "    label: str\n",
        "    statement: str\n",
        "    context: str\n",
        "    label_reason: str\n",
        "    subject_tags: str\n",
        "    chapter: int\n",
        "    chapter_title: str\n",
        "\n",
        "def write_csv(rows, path):\n",
        "    with open(path, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow(HEADERS)\n",
        "        for r in rows:\n",
        "            w.writerow([asdict(r)[h] for h in HEADERS])\n",
        "\n",
        "# ============ Utilities ============\n",
        "def wc(s: str) -> int:\n",
        "    return len(str(s).strip().split())\n",
        "\n",
        "def ensure_sentence(s: str) -> str:\n",
        "    s = str(s or \"\").strip()\n",
        "    if not s:\n",
        "        return s\n",
        "    if s[0].isalpha() and not s[0].isupper():\n",
        "        s = s[0].upper() + s[1:]\n",
        "    if s[-1] not in \".!?\":\n",
        "        s += \".\"\n",
        "    return s\n",
        "\n",
        "def ensure_phrase(s: str) -> str:\n",
        "    # keep concise phrase/clause; period not required\n",
        "    return str(s or \"\").strip().rstrip(\".\")\n",
        "\n",
        "def safe_parse_json(txt: str):\n",
        "    txt = str(txt).strip()\n",
        "    i, j = txt.find(\"{\"), txt.rfind(\"}\") + 1\n",
        "    snippet = txt[i:j] if i != -1 and j > 0 else txt\n",
        "    try:\n",
        "        return json.loads(snippet)\n",
        "    except Exception:\n",
        "        pairs = re.findall(r'\"(\\w+)\":\\s*\"([^\"]+)\"', snippet)\n",
        "        return {k: v for k, v in pairs}\n",
        "\n",
        "def short_hash(statement: str) -> str:\n",
        "    base = statement[:DEDUP_HASH_LEN].strip().lower()\n",
        "    return hashlib.md5(base.encode(\"utf-8\")).hexdigest()[:10]\n",
        "\n",
        "def ok_row(stmt: str, ctx: str, rsn: str) -> bool:\n",
        "    if not stmt or not ctx or not rsn:\n",
        "        return False\n",
        "    if META_BAN.search(stmt) or META_BAN.search(ctx) or META_BAN.search(rsn):\n",
        "        return False\n",
        "    if wc(stmt) > STMT_MAX_W:\n",
        "        return False\n",
        "    if wc(ctx) > CTX_MAX_W:\n",
        "        return False\n",
        "    n = wc(rsn)\n",
        "    if n < RSN_MIN_W or n > RSN_MAX_W:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# ============ Prompt ============\n",
        "GEN_PROMPT = r\"\"\"Return ONLY compact JSON with keys:\n",
        "statement, context, label_reason.\n",
        "\n",
        "Target label: {target_label}\n",
        "Chapter: {chapter} – {chapter_title}\n",
        "Subject tags: {subject_tags}\n",
        "\n",
        "Rules:\n",
        "- statement: ONE concise, third-person, declarative sentence (≤20 words) about the passage topic.\n",
        "  It MUST be written to MATCH the Target label.\n",
        "- Label guidance:\n",
        "  TRUE = directly supported by passage;\n",
        "  mostly-true = broadly supported, minor caveat omitted;\n",
        "  half-true = mix of correct and incorrect specifics;\n",
        "  barely-true = largely unsupported, notable error or overreach;\n",
        "  FALSE = contradicts passage facts;\n",
        "  pants-fire = extreme or implausible contradiction.\n",
        "- context: ONE concise noun phrase or brief clause (≤12 words) showing where/how the claim fits.\n",
        "  Include at least one specific term from the passage or tags (e.g., dataset, model, metric, tool, or concept).\n",
        "- label_reason: 8–18 words explaining WHY the label fits, citing evidence, omissions, or contradictions.\n",
        "  For FALSE or pants-fire, name the detail or assumption being contradicted.\n",
        "  Avoid starting with phrases like “The passage…”, “The claim…”, or similar. Instead, describe the relationship or mismatch directly (e.g., “RAG supports complex use cases” or\n",
        "  “Feature design improves accuracy in most cases.”).\n",
        "\n",
        "Style:\n",
        "- Write label_reason in a natural explanatory tone, not as a citation or report.\n",
        "- Use varied openings and sentence structures across rows.\n",
        "- Prefer concrete mechanisms, datasets, or named tools over generalities.\n",
        "- Maintain a factual, neutral tone; no meta language or self-reference.\n",
        "\n",
        "Avoid: “chapter”, “book”, “this statement”, or similar meta words.\n",
        "\n",
        "Passage:\n",
        "<<<\n",
        "{passage}\n",
        ">>>\n",
        "\n",
        "JSON:\n",
        "\"\"\"\n",
        "\n",
        "# ============ Label selection with drift guard ============\n",
        "def pick_label(counts: Counter, total_made: int, total_target: int) -> str:\n",
        "    # sample by weights; if one label is ahead of its expected share by 25%, resample once\n",
        "    lbl = random.choices(LABELS, weights=LABEL_WEIGHTS, k=1)[0]\n",
        "    if total_made == 0:\n",
        "        return lbl\n",
        "    # expected share so far (based on target totals)\n",
        "    expected = {L: LABEL_WEIGHTS[i] * (total_target) for i, L in enumerate(LABELS)}\n",
        "    ahead = counts[lbl] > 1.25 * (expected[lbl] * (total_made / max(1, total_target)))\n",
        "    if ahead:\n",
        "        lbl2 = random.choices(LABELS, weights=LABEL_WEIGHTS, k=1)[0]\n",
        "        return lbl2\n",
        "    return lbl\n",
        "\n",
        "# ============ LLM call ============\n",
        "def llm_generate_row(label: str, passage: str, chapter: int, chapter_title: str, tags_str: str) -> dict:\n",
        "    prompt = GEN_PROMPT.format(\n",
        "        target_label=label,\n",
        "        chapter=chapter,\n",
        "        chapter_title=chapter_title,\n",
        "        subject_tags=tags_str,\n",
        "        passage=passage\n",
        "    )\n",
        "    r = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.75,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "    return safe_parse_json(r.choices[0].message.content)\n",
        "\n",
        "# ============ Main ============\n",
        "def main():\n",
        "    df = pd.read_csv(CHUNKS_CSV, encoding=\"utf-8-sig\")\n",
        "\n",
        "    for col in (\"chapter\", \"chapter_title\", \"chunk_id\", \"chunk_text\"):\n",
        "        if col not in df.columns:\n",
        "            raise SystemExit(f\"chunks.csv missing required column: {col}\")\n",
        "\n",
        "    # choose which chunks to process (simple shuffle for coverage)\n",
        "    idxs = list(df.index)\n",
        "    random.shuffle(idxs)\n",
        "    idxs = idxs[:min(MAX_CHUNKS, len(idxs))]\n",
        "\n",
        "    total_target = len(idxs) * N_PER_CHUNK_TARGET\n",
        "\n",
        "    next_id = BASE_ID\n",
        "    rows = []\n",
        "    seen_stmt = set()\n",
        "    label_counts = Counter()\n",
        "\n",
        "    print(f\"Processing {len(idxs)} chunks; target rows ≈ {total_target}\")\n",
        "\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        row = df.loc[idx]\n",
        "        passage = str(row[\"chunk_text\"]).strip()\n",
        "        if not passage:\n",
        "            continue\n",
        "\n",
        "        chapter = int(row[\"chapter\"])\n",
        "        chapter_title = str(row[\"chapter_title\"])\n",
        "        chunk_id = int(row[\"chunk_id\"])\n",
        "\n",
        "        # prefer subject_tags column; fallback to map\n",
        "        if \"subject_tags\" in df.columns and str(row[\"subject_tags\"]).strip():\n",
        "            tags_str = str(row[\"subject_tags\"])\n",
        "        else:\n",
        "            tags_str = \", \".join(SUBJECT_TAGS.get(chapter, []))\n",
        "\n",
        "        # per-chunk generation\n",
        "        for _ in range(N_PER_CHUNK_TARGET):\n",
        "            attempts = 0\n",
        "            while attempts <= RETRIES_PER_ROW:\n",
        "                attempts += 1\n",
        "\n",
        "                label = pick_label(label_counts, len(rows), total_target)\n",
        "                out = llm_generate_row(label, passage, chapter, chapter_title, tags_str)\n",
        "\n",
        "                stmt = ensure_sentence(out.get(\"statement\", \"\"))\n",
        "                ctx  = ensure_phrase(out.get(\"context\", \"\"))\n",
        "                rsn  = ensure_sentence(out.get(\"label_reason\", \"\"))\n",
        "\n",
        "                if not ok_row(stmt, ctx, rsn):\n",
        "                    if attempts <= RETRIES_PER_ROW:\n",
        "                        continue\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                key = short_hash(stmt)\n",
        "                if key in seen_stmt:\n",
        "                    if attempts <= RETRIES_PER_ROW:\n",
        "                        continue\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                # accept\n",
        "                seen_stmt.add(key)\n",
        "                label_counts[label] += 1\n",
        "                rows.append(\n",
        "                    Row(\n",
        "                        id=next_id,\n",
        "                        chunk_id=chunk_id,\n",
        "                        label=label,\n",
        "                        statement=stmt,\n",
        "                        context=ctx,\n",
        "                        label_reason=rsn,\n",
        "                        subject_tags=tags_str,\n",
        "                        chapter=chapter,\n",
        "                        chapter_title=chapter_title,\n",
        "                    )\n",
        "                )\n",
        "                next_id += 1\n",
        "                time.sleep(PAUSE_SEC)\n",
        "                break  # next target row for this chunk\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print(f\"[progress] chunks {i}/{len(idxs)} | rows {len(rows)}\")\n",
        "\n",
        "    # write output\n",
        "    write_csv(rows, OUTPUT_CSV)\n",
        "\n",
        "    # summary\n",
        "    print(f\"\\nWrote {len(rows)} rows to {OUTPUT_CSV}\")\n",
        "    print(\"Label counts:\", label_counts)\n",
        "    print(\"Chapters:\", Counter(r.chapter for r in rows))\n",
        "    print(\"\\nSample:\")\n",
        "    for s in rows[:10]:\n",
        "        print(f\"{s.id} [{s.label}] {s.statement} :: {s.context} | tags={s.subject_tags} | ch{s.chapter} – {s.chapter_title}\")\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YAKR_PzNuvV",
        "outputId": "1180721a-c2d2-4182-a2fc-fc17ce73aa54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 10 chunks; target rows ≈ 20\n",
            "\n",
            "Wrote 20 rows to byoai_liar.csv\n",
            "Label counts: Counter({'mostly-true': 5, 'half-true': 4, 'TRUE': 4, 'FALSE': 3, 'barely-true': 2, 'pants-fire': 2})\n",
            "Chapters: Counter({2: 6, 3: 4, 7: 4, 12: 2, 5: 2, 8: 2})\n",
            "\n",
            "Sample:\n",
            "100000 [barely-true] RAG cannot generate serious narratives like policy documents. :: dynamic, context-driven narratives | tags=data-prep,feature-engineering,rag | ch3 – Prepping Data for AI\n",
            "100001 [FALSE] RAG cannot generate dynamic narratives for serious projects. :: RAG's capabilities in generating narratives | tags=data-prep,feature-engineering,rag | ch3 – Prepping Data for AI\n",
            "100002 [half-true] Generative AI can create music, code, and videos from text prompts. :: applications of generative AI models | tags=generative-ai,diffusion,gans | ch7 – Generative AI\n",
            "100003 [TRUE] Generative AI models can create music, code, and videos from text prompts. :: generative-ai applications in model architecture | tags=generative-ai,diffusion,gans | ch7 – Generative AI\n",
            "100004 [FALSE] Advanced algorithms do not require well-engineered features to function effectively. :: feature engineering and model performance | tags=data-prep,feature-engineering,rag | ch3 – Prepping Data for AI\n",
            "100005 [mostly-true] Well-engineered features enhance model performance and reliability. :: feature engineering and model performance | tags=data-prep,feature-engineering,rag | ch3 – Prepping Data for AI\n",
            "100006 [barely-true] AI systems may misinterpret genres like superhero films. :: agent behavior and judgment in film classification | tags=ai,tool-chain,notebooks | ch2 – AI Survival Kit\n",
            "100007 [mostly-true] Open source tools enhance the inspection and improvement of AI agent behavior. :: role of open source in AI development | tags=ai,tool-chain,notebooks | ch2 – AI Survival Kit\n",
            "100008 [TRUE] Agentic AI tools enhance reproducibility and trust in critical domains. :: mechanisms to explain outcomes in agentic AI | tags=agentic-ai,planning,tools | ch12 – Agentic AI\n",
            "100009 [half-true] Reproducibility is less critical in creative applications than in structured fields. :: discussion on reproducibility in agentic AI tools | tags=agentic-ai,planning,tools | ch12 – Agentic AI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2A: Merging Extreme Supervision Samples into the BYOAI_LIAR Dataset\n",
        "\n",
        "This listing merges the core **BYOAI_LIAR** dataset with a small, purpose-built extension file named  \n",
        "`byoai_liar_extremes.csv`. The added file introduces *extreme supervision* samples — short,\n",
        "high-confidence statements manually generated from the book’s chapter texts to balance the dataset’s\n",
        "coverage of clear truths and clear falsehoods.\n",
        "\n",
        "During early experiments, most examples clustered near the middle labels (“half-true” or “barely-true”).\n",
        "To help the model learn stronger boundaries between factually correct and incorrect claims, this step\n",
        "adds roughly ten curated statements per chapter (≈140 in total) representing explicit **TRUE**, **FALSE**,  \n",
        "and **pants-fire** cases.\n",
        "\n",
        "The script normalizes casing, aligns column names, removes duplicates, and produces a single clean file\n",
        "`byoai_liar_merged.csv`. This merged dataset ensures more even label distribution and gives the model\n",
        "a better grasp of clear-cut truth extremes before fine-tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zv62NQSI55Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Listing 9-7: Merge Extended Dataset with Main BYOAI_LIAR ==============\n",
        "# Combines byoai_liar.csv and byoai_liar_extremes.csv.\n",
        "# Normalizes label and text casing, aligns columns, and removes duplicates.\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- Input / Output --------------------------------------------------------\n",
        "BASE_FILE = \"byoai_liar.csv\"\n",
        "EXT_FILE  = \"byoai_liar_extremes.csv\"\n",
        "MERGED_FILE = \"byoai_liar_merged.csv\"\n",
        "\n",
        "# --- Load both datasets ----------------------------------------------------\n",
        "df_base = pd.read_csv(BASE_FILE, encoding=\"utf-8-sig\")\n",
        "df_ext  = pd.read_csv(EXT_FILE, encoding=\"utf-8-sig\")\n",
        "\n",
        "# --- Normalize text casing -------------------------------------------------\n",
        "def normalize_text_fields(df):\n",
        "    for col in [\"label\", \"chapter_title\", \"subject_tags\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(str).str.strip().str.lower()\n",
        "    return df\n",
        "\n",
        "df_base = normalize_text_fields(df_base)\n",
        "df_ext  = normalize_text_fields(df_ext)\n",
        "\n",
        "# --- Align columns ---------------------------------------------------------\n",
        "common_cols = [c for c in df_base.columns if c in df_ext.columns]\n",
        "df_ext = df_ext[common_cols]\n",
        "df_base = df_base[common_cols]\n",
        "\n",
        "# --- Combine and remove duplicates -----------------------------------------\n",
        "df_merged = pd.concat([df_base, df_ext], ignore_index=True)\n",
        "\n",
        "# Drop duplicate statements (case-insensitive)\n",
        "df_merged[\"statement_norm\"] = df_merged[\"statement\"].astype(str).str.strip().str.lower()\n",
        "df_merged = df_merged.drop_duplicates(subset=\"statement_norm\").drop(columns=\"statement_norm\")\n",
        "\n",
        "# --- Sort and reset index --------------------------------------------------\n",
        "df_merged = df_merged.sort_values(by=[\"chapter\", \"id\"]).reset_index(drop=True)\n",
        "\n",
        "# --- Save clean merged dataset --------------------------------------------\n",
        "df_merged.to_csv(MERGED_FILE, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "# --- Summary ---------------------------------------------------------------\n",
        "print(f\"Merged dataset saved → {MERGED_FILE}\")\n",
        "print(f\"Base rows: {len(df_base)}, Extended rows: {len(df_ext)}, Final: {len(df_merged)}\")\n",
        "\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(df_merged[\"label\"].value_counts().to_dict())\n",
        "\n",
        "print(\"\\nChapter distribution:\")\n",
        "print(df_merged[\"chapter\"].value_counts().sort_index().to_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osXEQ_oK7YSX",
        "outputId": "ac8c9608-28fc-488b-95a7-a4bb7fd88be4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged dataset saved → byoai_liar_merged.csv\n",
            "Base rows: 5517, Extended rows: 140, Final: 5558\n",
            "\n",
            "Label distribution:\n",
            "{'half-true': 1338, 'true': 1122, 'barely-true': 1050, 'mostly-true': 936, 'false': 659, 'pants-fire': 453}\n",
            "\n",
            "Chapter distribution:\n",
            "{0: 380, 1: 394, 2: 560, 3: 544, 4: 615, 5: 271, 6: 502, 7: 458, 8: 409, 9: 404, 10: 287, 11: 452, 12: 280, 13: 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2B: BYOAI LIAR Coherence & Label Audit\n",
        "\n",
        "This script performs a lightweight quality audit of `byoai_liar.csv` using\n",
        "GPT-based evaluation. It randomly samples dataset rows and asks the model\n",
        "to rate coherence across four dimensions—**chapter_fit**, **label_fit**,\n",
        "**context_quality**, and **reason_quality**—each scored from 0 to 1. The\n",
        "model also assigns an overall rating (`high`, `medium`, or `low`) and, when\n",
        "needed, suggests a brief fix for weak or inconsistent entries.\n",
        "\n",
        "Each row is evaluated through a structured JSON prompt that ensures\n",
        "consistent scoring and concise feedback. Results are printed with detailed\n",
        "row-level metrics and a short summary of average fit scores and overall\n",
        "distribution. This audit helps verify whether generated statements align\n",
        "with their chapters, whether labels are logically justified, and whether\n",
        "context and reasoning remain distinct and informative—providing an early,\n",
        "LLM-assisted sanity check on the coherence and factual structure of the\n",
        "synthetic BYOAI LIAR dataset."
      ],
      "metadata": {
        "id": "WZKCLyhTutmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === BYOAI LIAR Coherence & Label Audit (LLM-assisted, randomized) ============\n",
        "# Randomly samples rows from 'byoai_liar.csv' and asks gpt-4o-mini to rate:\n",
        "# - chapter_fit: does the statement/context align with the chapter’s theme?\n",
        "# - label_fit: is the label logically consistent with the statement and justification?\n",
        "# - reason_quality: does label_reason explain the label clearly and concisely?\n",
        "# - context_quality: does context add distinct, relevant detail (not restate statement)?\n",
        "# Also returns overall rating and a suggested fix when coherence is low.\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# -------------------- CONFIG --------------------\n",
        "CSV_PATH       = \"byoai_liar.csv\"\n",
        "SAMPLE_CHECKS  = 100         # how many random rows to audit\n",
        "MODEL          = \"gpt-4o-mini\"\n",
        "RANDOM_SEED    = 42\n",
        "SLEEP_SEC      = 0.05\n",
        "# ------------------------------------------------\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise SystemExit(\"Missing OPENAI_API_KEY in Colab userdata.\")\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(CSV_PATH, encoding=\"utf-8-sig\")\n",
        "required = [\n",
        "    \"id\",\"label\",\"statement\",\"context\",\"label_reason\",\n",
        "    \"subject_tags\",\"chapter\",\"chapter_title\"\n",
        "]\n",
        "missing = [c for c in required if c not in df.columns]\n",
        "if missing:\n",
        "    raise SystemExit(f\"Missing columns: {missing}\")\n",
        "\n",
        "# Sample subset\n",
        "sample_df = (\n",
        "    df.sample(SAMPLE_CHECKS, random_state=RANDOM_SEED)\n",
        "    if SAMPLE_CHECKS < len(df)\n",
        "    else df.copy()\n",
        ")\n",
        "\n",
        "# Prompt setup\n",
        "SYSTEM = (\n",
        "    \"You are a concise dataset auditor. \"\n",
        "    \"Return ONLY compact JSON. No preface, no commentary.\"\n",
        ")\n",
        "USER_TMPL = \"\"\"Evaluate the coherence and labeling of this BYOAI dataset row.\n",
        "\n",
        "Return JSON with keys EXACTLY:\n",
        "- chapter_fit: float in [0,1]\n",
        "- label_fit: float in [0,1]\n",
        "- context_quality: float in [0,1]\n",
        "- reason_quality: float in [0,1]\n",
        "- overall: one of [\"high\",\"medium\",\"low\"]\n",
        "- comments: short string (≤25 words)\n",
        "- suggest_fix: short rewrite (≤28 words) improving label_reason or context\n",
        "\n",
        "Guidelines:\n",
        "- chapter_fit: statement+context align with the chapter theme ({chapter_title})?\n",
        "- label_fit: does the label logically match the statement and label_reason?\n",
        "- context_quality: does the context add value without repeating the statement?\n",
        "- reason_quality: is label_reason clear, concise, and directly supportive of the label?\n",
        "- overall: rate overall coherence (high=strong fit, medium=minor flaws, low=clear mismatch)\n",
        "\n",
        "Row:\n",
        "chapter_title: \"{chapter_title}\"\n",
        "chapter: {chapter}\n",
        "label: \"{label}\"\n",
        "subject_tags: \"{subject_tags}\"\n",
        "statement: \"{statement}\"\n",
        "context: \"{context}\"\n",
        "label_reason: \"{label_reason}\"\n",
        "\n",
        "Return JSON:\"\"\"\n",
        "\n",
        "# === Utility functions ===\n",
        "def safe_parse_json(txt: str) -> dict:\n",
        "    txt = (txt or \"\").strip()\n",
        "    i, j = txt.find(\"{\"), txt.rfind(\"}\") + 1\n",
        "    snippet = txt[i:j] if i != -1 and j > 0 else txt\n",
        "    try:\n",
        "        return json.loads(snippet)\n",
        "    except Exception:\n",
        "        pairs = re.findall(r'\"(\\w+)\":\\s*\"([^\"]+)\"', snippet)\n",
        "        return {k: v for k, v in pairs}\n",
        "\n",
        "def clamp01(x):\n",
        "    try:\n",
        "        v = float(x)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "    return max(0.0, min(1.0, v))\n",
        "\n",
        "def audit_row(row):\n",
        "    prompt = USER_TMPL.format(\n",
        "        chapter=row[\"chapter\"],\n",
        "        chapter_title=str(row[\"chapter_title\"]),\n",
        "        label=str(row[\"label\"]),\n",
        "        subject_tags=str(row[\"subject_tags\"]),\n",
        "        statement=str(row[\"statement\"]),\n",
        "        context=str(row[\"context\"]),\n",
        "        label_reason=str(row[\"label_reason\"]),\n",
        "    )\n",
        "    r = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[{\"role\":\"system\",\"content\":SYSTEM},\n",
        "                  {\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    out = safe_parse_json(r.choices[0].message.content)\n",
        "    # normalize\n",
        "    cfit = clamp01(out.get(\"chapter_fit\", 0))\n",
        "    lfit = clamp01(out.get(\"label_fit\", 0))\n",
        "    cq   = clamp01(out.get(\"context_quality\", 0))\n",
        "    rq   = clamp01(out.get(\"reason_quality\", 0))\n",
        "    overall = str(out.get(\"overall\",\"\")).lower().strip()\n",
        "    if overall not in {\"high\",\"medium\",\"low\"}:\n",
        "        avg = (cfit + lfit + cq + rq) / 4.0\n",
        "        overall = \"high\" if avg >= 0.75 else \"medium\" if avg >= 0.5 else \"low\"\n",
        "    comments = str(out.get(\"comments\",\"\")).strip()\n",
        "    fix = str(out.get(\"suggest_fix\",\"\")).strip()\n",
        "    return {\n",
        "        \"chapter_fit\": cfit,\n",
        "        \"label_fit\": lfit,\n",
        "        \"context_quality\": cq,\n",
        "        \"reason_quality\": rq,\n",
        "        \"overall\": overall,\n",
        "        \"comments\": comments,\n",
        "        \"suggest_fix\": fix,\n",
        "    }\n",
        "\n",
        "# === Run audit ===\n",
        "results = []\n",
        "print(f\"Auditing {len(sample_df)} random rows from {len(df)} total...\\n\")\n",
        "for i, (_, row) in enumerate(sample_df.iterrows(), 1):\n",
        "    res = audit_row(row)\n",
        "    results.append((row, res))\n",
        "    print(f\"[{i:02d}] id={row['id']} | ch{row['chapter']} \"\n",
        "          f\"({row['chapter_title']}) | label={row['label']}\")\n",
        "    print(f\"     S: {row['statement']}\")\n",
        "    print(f\"     C: {row['context']}\")\n",
        "    print(f\"     R: {row['label_reason']}\")\n",
        "    print(f\"     -> fit(ch,lab,ctx,rsn)=({res['chapter_fit']:.2f},\"\n",
        "          f\"{res['label_fit']:.2f},{res['context_quality']:.2f},\"\n",
        "          f\"{res['reason_quality']:.2f}) | overall={res['overall']}\")\n",
        "    if res[\"overall\"] == \"low\" or res[\"reason_quality\"] < 0.6:\n",
        "        print(f\"     fix: {res['suggest_fix'] or '(no suggestion)'}\")\n",
        "    if res[\"comments\"]:\n",
        "        print(f\"     note: {res['comments']}\")\n",
        "    print()\n",
        "    time.sleep(SLEEP_SEC)\n",
        "\n",
        "# === Aggregates ===\n",
        "if results:\n",
        "    ch  = sum(r[1][\"chapter_fit\"] for r in results) / len(results)\n",
        "    lf  = sum(r[1][\"label_fit\"] for r in results) / len(results)\n",
        "    cq  = sum(r[1][\"context_quality\"] for r in results) / len(results)\n",
        "    rq  = sum(r[1][\"reason_quality\"] for r in results) / len(results)\n",
        "    overall_counts = {}\n",
        "    for _, r in results:\n",
        "        overall_counts[r[\"overall\"]] = overall_counts.get(r[\"overall\"], 0) + 1\n",
        "\n",
        "    print(\"=== Aggregate (sample) ===\")\n",
        "    print(f\"chapter_fit avg:     {ch:.3f}\")\n",
        "    print(f\"label_fit avg:       {lf:.3f}\")\n",
        "    print(f\"context_quality avg: {cq:.3f}\")\n",
        "    print(f\"reason_quality avg:  {rq:.3f}\")\n",
        "    print(\"overall ratings:     \", overall_counts)\n",
        "else:\n",
        "    print(\"No results to summarize.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V18HR6u3WGfC",
        "outputId": "98a3ffc0-e895-44ef-deab-7c5cc2be01bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auditing 100 random rows from 5517 total...\n",
            "\n",
            "[01] id=1625 | ch4 (Deep Learning) | label=half-true\n",
            "     S: The DataLoader efficiently manages loading and batching of dataset samples.\n",
            "     C: DataLoader functionality in handling datasets\n",
            "     R: While it efficiently batches data, it may not address all complexities of data handling.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.70) | overall=medium\n",
            "     note: Context is somewhat vague and doesn't enhance understanding.\n",
            "\n",
            "[02] id=1884 | ch6 (Generative AI) | label=TRUE\n",
            "     S: Open-source alternatives to foundation models are increasingly developed globally.\n",
            "     C: open-source alternatives to foundation models\n",
            "     R: Numerous initiatives aim to democratize access to generative AI technologies.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.80,0.70,0.80) | overall=medium\n",
            "     note: Context is somewhat repetitive; label fits but could be clearer.\n",
            "\n",
            "[03] id=3177 | ch2 (Prepping Data for AI) | label=barely-true\n",
            "     S: Most teams invest little time in data preparation for AI projects.\n",
            "     C: data preparation time in AI projects\n",
            "     R: The claim exaggerates, as teams often prioritize data preparation for accurate results.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.80,0.70,0.60) | overall=medium\n",
            "     note: Label reason lacks clarity and direct support for the label.\n",
            "\n",
            "[04] id=5297 | ch5 (Neuron Building Blocks) | label=half-true\n",
            "     S: The T5 model generates outputs using a single prompt format.\n",
            "     C: model generation process with T5\n",
            "     R: While T5 uses a specific prompt, it requires task-specific formatting for optimal performance.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.70) | overall=medium\n",
            "     note: Context lacks depth and clarity regarding T5's prompt usage.\n",
            "\n",
            "[05] id=4780 | ch9 (AI At Scale) | label=half-true\n",
            "     S: Benchmarking provides insights into model performance and engineering improvements.\n",
            "     C: benchmarking insights from model performance\n",
            "     R: The statement captures the essence of benchmarking but oversimplifies the complexity of model evaluation.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.70,0.60,0.50,0.60) | overall=medium\n",
            "     note: Label and context lack depth and clarity.\n",
            "\n",
            "[06] id=3670 | ch4 (Deep Learning) | label=half-true\n",
            "     S: Using Python's pickle module for model saving is generally unreliable.\n",
            "     C: model saving with Python's pickle module\n",
            "     R: While it's true that pickle can break easily, it may work in stable environments.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.80,0.70,0.60) | overall=medium\n",
            "     note: Label and reasoning have minor inconsistencies.\n",
            "\n",
            "[07] id=2953 | ch8 (Deepfake Defense) | label=barely-true\n",
            "     S: Librosa fails to extract meaningful features from raw audio effectively.\n",
            "     C: audio feature extraction using Librosa\n",
            "     R: Librosa is described as excelling at transforming sound into quantifiable features, contradicting the claim.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.60,0.40,0.50,0.30) | overall=low\n",
            "     fix: Revise label to 'misleading' and clarify Librosa's limitations in audio feature extraction.\n",
            "     note: Label and reasoning do not align with the statement.\n",
            "\n",
            "[08] id=1481 | ch6 (Generative AI) | label=barely-true\n",
            "     S: Label smoothing is a widely used technique in generative AI training.\n",
            "     C: training techniques for neural networks\n",
            "     R: While label smoothing is mentioned, it's not universally applicable to all generative AI scenarios.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.60,0.70,0.50) | overall=medium\n",
            "     fix: Label smoothing is relevant but not universally applicable in generative AI training contexts.\n",
            "     note: Label and reason lack clarity and precision.\n",
            "\n",
            "[09] id=2469 | ch8 (Deepfake Defense) | label=TRUE\n",
            "     S: Congress is considering new legislation in response to Taylor Swift deepfakes.\n",
            "     C: calls for legislation regarding deepfake security\n",
            "     R: Legislative discussions directly address the impact of deepfakes on individuals.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.80,0.70,0.90) | overall=medium\n",
            "     note: Context could be more specific to the statement.\n",
            "\n",
            "[10] id=3236 | ch4 (Deep Learning) | label=TRUE\n",
            "     S: The .h5 format is commonly used for quick model saves.\n",
            "     C: model saving techniques in deep learning\n",
            "     R: The passage states that .h5 is simpler and used for quick saves.\n",
            "     -> fit(ch,lab,ctx,rsn)=(1.00,0.80,0.90,0.70) | overall=medium\n",
            "     note: Label reason lacks clarity on why it's true.\n",
            "\n",
            "[11] id=2526 | ch11 (Agentic AI) | label=barely-true\n",
            "     S: AI models produce unpredictable results despite structured prompts.\n",
            "     C: interaction with AI models\n",
            "     R: The claim overlooks that structured prompts enhance consistency and reliability in outputs.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.70,0.60,0.50,0.40) | overall=medium\n",
            "     fix: Revise label to 'partially-true' and clarify reasoning on structured prompts.\n",
            "     note: Label and reasoning do not align well.\n",
            "\n",
            "[12] id=3664 | ch2 (Prepping Data for AI) | label=FALSE\n",
            "     S: Data labeling is unnecessary for model training.\n",
            "     C: data labeling process in AI models\n",
            "     R: Labels are essential for supervised learning, enabling models to learn from data.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.20,0.60,0.90) | overall=medium\n",
            "     note: Label contradicts the statement; context is somewhat relevant.\n",
            "\n",
            "[13] id=5382 | ch0 (Introduction) | label=half-true\n",
            "     S: Big AI has significant capabilities but also notable limitations.\n",
            "     C: discussion of Big AI's performance and constraints\n",
            "     R: While Big AI is impressive, its limitations are crucial and not fully addressed.\n",
            "     -> fit(ch,lab,ctx,rsn)=(1.00,0.80,0.90,0.70) | overall=medium\n",
            "     note: Label slightly misaligns with statement's nuance.\n",
            "\n",
            "[14] id=4381 | ch5 (Neuron Building Blocks) | label=barely-true\n",
            "     S: LSTMs are ineffective for capturing long-term dependencies.\n",
            "     C: LSTM capabilities in RNNs\n",
            "     R: LSTMs are specifically designed to excel at capturing long-term dependencies.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.20,0.10,0.30,0.10) | overall=low\n",
            "     fix: LSTMs are effective for capturing long-term dependencies in RNNs.\n",
            "     note: Label and statement are contradictory.\n",
            "\n",
            "[15] id=2822 | ch9 (AI At Scale) | label=barely-true\n",
            "     S: Saving models allows for efficient reuse and persistence in AI applications.\n",
            "     C: model saving and reloading\n",
            "     R: The claim overlooks the complexities of model sharing beyond just saving.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.40,0.50,0.60,0.50) | overall=medium\n",
            "     fix: The statement simplifies model sharing complexities in AI applications.\n",
            "     note: Label and statement lack strong alignment.\n",
            "\n",
            "[16] id=4626 | ch3 (Classical Machine Learning) | label=TRUE\n",
            "     S: Classical machine learning excels with structured datasets and efficiency.\n",
            "     C: Classical ML general techniques overview\n",
            "     R: The passage supports that classical ML is suitable for structured data and emphasizes its efficiency.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.90,0.80,0.90) | overall=high\n",
            "     note: Strong alignment with chapter theme and label.\n",
            "\n",
            "[17] id=1563 | ch7 (Breaking-Securing AI) | label=half-true\n",
            "     S: Open-source tools enhance AI security through community collaboration.\n",
            "     C: AI security measures and community involvement\n",
            "     R: While collaboration is beneficial, flaws may still persist despite community efforts.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.70) | overall=medium\n",
            "     note: Label and context could be clearer and more aligned.\n",
            "\n",
            "[18] id=5150 | ch6 (Generative AI) | label=FALSE\n",
            "     S: Diffusion models outperform autoregressive models in natural language generation.\n",
            "     C: comparison of model strengths in generative AI\n",
            "     R: Autoregressive models are noted for their dominance in natural language generation, contradicting the claim.\n",
            "     -> fit(ch,lab,ctx,rsn)=(1.00,0.00,0.80,0.50) | overall=low\n",
            "     fix: Label should be 'TRUE' as diffusion models excel in natural language generation.\n",
            "     note: Label contradicts the statement; unclear reasoning.\n",
            "\n",
            "[19] id=1662 | ch8 (Deepfake Defense) | label=FALSE\n",
            "     S: SpeechT5 cannot reliably analyze diverse audio samples.\n",
            "     C: SpeechT5 audio analysis capabilities\n",
            "     R: The passage describes SpeechT5's compatibility with specific audio formats, contradicting the claim.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.50) | overall=medium\n",
            "     fix: Revise label_reason to clarify how formats relate to reliability.\n",
            "     note: Label and context have minor inconsistencies.\n",
            "\n",
            "[20] id=1234 | ch3 (Classical Machine Learning) | label=TRUE\n",
            "     S: Scikit-learn trains regression models using training and testing data splits.\n",
            "     C: model.fit and model.predict in Scikit-learn\n",
            "     R: The passage explains how Scikit-learn utilizes training and testing datasets to train and evaluate models.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.90,0.80,0.90) | overall=high\n",
            "     note: Strong alignment with chapter theme and label.\n",
            "\n",
            "[21] id=2902 | ch11 (Agentic AI) | label=half-true\n",
            "     S: LangChain enables AI to interact intelligently through structured prompts and models.\n",
            "     C: LangChain integration with prompts and models\n",
            "     R: While LangChain supports intelligent interactions, it doesn't guarantee full autonomy or accuracy.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.70) | overall=medium\n",
            "     note: Context is vague and lacks depth.\n",
            "\n",
            "[22] id=1740 | ch11 (Agentic AI) | label=pants-fire\n",
            "     S: Agentic AI devices compromise patient privacy by constantly monitoring behavior.\n",
            "     C: privacy in smart medical devices\n",
            "     R: The claim contradicts the passage's emphasis on security and privacy considerations.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.60,0.70,0.50) | overall=medium\n",
            "     fix: Revise label_reason to clarify how the claim aligns with privacy concerns.\n",
            "     note: Label and reasoning do not align well with the statement.\n",
            "\n",
            "[23] id=457 | ch4 (Deep Learning) | label=FALSE\n",
            "     S: DataLoaders are unnecessary for model training in deep learning.\n",
            "     C: role of DataLoader in deep learning frameworks\n",
            "     R: DataLoaders are essential for organizing and serving data during training.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.20,0.70,0.90) | overall=medium\n",
            "     note: Label contradicts the statement; context is relevant.\n",
            "\n",
            "[24] id=3396 | ch1 (AI Survival Kit) | label=barely-true\n",
            "     S: Hero B is better than Hero A in all scenarios.\n",
            "     C: hero performance evaluation metrics\n",
            "     R: Hero A outperforms Hero B in one challenge, contradicting the claim.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.80) | overall=medium\n",
            "     note: Label and context have minor inconsistencies.\n",
            "\n",
            "[25] id=2680 | ch0 (Foreword) | label=TRUE\n",
            "     S: The individual became a successful eBay seller by age 17.\n",
            "     C: entrepreneurship and eBay selling\n",
            "     R: The passage describes the individual's rise to prominence as a seller on eBay.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.30,0.50,0.40,0.60) | overall=medium\n",
            "     note: Weak alignment with chapter theme and context.\n",
            "\n",
            "[26] id=2409 | ch4 (Deep Learning) | label=mostly-true\n",
            "     S: The transforms.ToTensor() operation converts images to PyTorch tensors.\n",
            "     C: image preprocessing in deep learning models\n",
            "     R: The claim accurately describes the ToTensor() operation's function in data preparation.\n",
            "     -> fit(ch,lab,ctx,rsn)=(1.00,1.00,0.90,1.00) | overall=high\n",
            "     note: Strong alignment with chapter and clear reasoning.\n",
            "\n",
            "[27] id=3375 | ch9 (AI At Scale) | label=half-true\n",
            "     S: The model was published on the Hugging Face Hub for public access.\n",
            "     C: model publication process on Hugging Face Hub\n",
            "     R: The model's availability is accurate, but details about its usage are not discussed.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.70,0.80,0.60,0.70) | overall=medium\n",
            "     note: Context lacks depth and relevance to AI at scale.\n",
            "\n",
            "[28] id=18 | ch2 (Prepping Data for AI) | label=TRUE\n",
            "     S: Well-engineered features enhance model performance and reliability.\n",
            "     C: feature engineering for AI models\n",
            "     R: The passage emphasizes that engineered features lead to improved results for algorithms.\n",
            "     -> fit(ch,lab,ctx,rsn)=(1.00,1.00,0.90,0.90) | overall=high\n",
            "     note: Strong alignment with chapter theme and label.\n",
            "\n",
            "[29] id=1384 | ch6 (Generative AI) | label=mostly-true\n",
            "     S: Larger models generally provide superior performance across diverse tasks.\n",
            "     C: state-of-the-art performance in large models\n",
            "     R: While larger models excel, practical considerations can limit their use.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.80,0.70,0.80) | overall=medium\n",
            "     note: Context is somewhat vague and repetitive.\n",
            "\n",
            "[30] id=3055 | ch11 (Agentic AI) | label=mostly-true\n",
            "     S: CrewAI provides a versatile platform for various AI-driven applications.\n",
            "     C: multi-agent AI frameworks\n",
            "     R: The claim aligns with the passage's emphasis on CrewAI's diverse use cases.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.60,0.70,0.50,0.60) | overall=medium\n",
            "     note: Label and context are somewhat relevant but lack depth.\n",
            "\n",
            "[31] id=1750 | ch4 (Deep Learning) | label=barely-true\n",
            "     S: Batch size impacts evaluation speed but not accuracy.\n",
            "     C: evaluation process with batch size of 1000\n",
            "     R: Claim overlooks that accuracy remains unaffected by data order, but implies batch size influences it.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.70,0.80,0.60) | overall=medium\n",
            "     note: Label does not fully align with the statement's implications.\n",
            "\n",
            "[32] id=2665 | ch2 (Prepping Data for AI) | label=barely-true\n",
            "     S: RAG models learn new knowledge from retrieved snippets during generation.\n",
            "     C: RAG process in generative AI models\n",
            "     R: RAG does not learn new knowledge; it composes answers from retrieved facts.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.80,0.70,0.90) | overall=medium\n",
            "     note: Context is somewhat relevant but could be clearer.\n",
            "\n",
            "[33] id=1056 | ch6 (Generative AI) | label=barely-true\n",
            "     S: Autoregressive models are ineffective for practical forecasting problems.\n",
            "     C: application of autoregressive models in forecasting\n",
            "     R: The passage emphasizes the effectiveness of autoregressive models for various forecasting tasks.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.20,0.10,0.30,0.10) | overall=low\n",
            "     fix: Autoregressive models are often effective for practical forecasting problems.\n",
            "     note: Label and statement contradict each other significantly.\n",
            "\n",
            "[34] id=3539 | ch5 (Neuron Building Blocks) | label=barely-true\n",
            "     S: Transformers do not rely on sequential input processing like RNNs.\n",
            "     C: Transformers vs RNNs in sequence modeling\n",
            "     R: While Transformers improve upon RNNs, they still consider input order in different ways.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.60,0.70,0.50) | overall=medium\n",
            "     fix: Transformers process input differently than RNNs, enhancing but not eliminating sequential considerations.\n",
            "     note: Label does not fully align with the statement's accuracy.\n",
            "\n",
            "[35] id=5153 | ch11 (Agentic AI) | label=TRUE\n",
            "     S: Agentic AI can make decisions and delegate tasks effectively.\n",
            "     C: agent capabilities and attributes\n",
            "     R: The passage outlines how agents can make decisions and delegate tasks.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.90,0.70,0.80) | overall=medium\n",
            "     note: Context is somewhat vague and could be more specific.\n",
            "\n",
            "[36] id=43 | ch4 (Deep Learning) | label=half-true\n",
            "     S: Many popular AI models utilize frameworks like PyTorch and JAX.\n",
            "     C: frameworks used in AI models\n",
            "     R: While most models use these frameworks, some details remain undisclosed.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.50) | overall=medium\n",
            "     fix: Many AI models use frameworks like PyTorch and JAX, but not all disclose their details.\n",
            "     note: Label and reason lack clarity and precision.\n",
            "\n",
            "[37] id=3219 | ch6 (Generative AI) | label=pants-fire\n",
            "     S: The VAE model can generate entirely new shapes from existing data inputs.\n",
            "     C: VAE model training and inference process\n",
            "     R: The claim contradicts the passage, which states variations are slight, not entirely new.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.20,0.50,0.30) | overall=low\n",
            "     fix: Revise label to 'partially accurate' to reflect the statement's intent.\n",
            "     note: Label contradicts the statement's intent.\n",
            "\n",
            "[38] id=3914 | ch2 (Prepping Data for AI) | label=barely-true\n",
            "     S: RAG is sufficient for precise reasoning tasks.\n",
            "     C: RAG's capabilities in model updates\n",
            "     R: RAG only retrieves data and does not learn deeply, limiting its effectiveness for precise tasks.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.40,0.60,0.50,0.70) | overall=medium\n",
            "     note: Label and context do not fully align with the statement.\n",
            "\n",
            "[39] id=979 | ch5 (Neuron Building Blocks) | label=TRUE\n",
            "     S: ResNet50 utilizes general features for effective image predictions.\n",
            "     C: application of ResNet50 in computer vision\n",
            "     R: The model's ability to generalize features enables accurate predictions despite limited training data.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.90,0.70,0.80) | overall=medium\n",
            "     note: Context is relevant but could be more specific to neuron building blocks.\n",
            "\n",
            "[40] id=3262 | ch2 (Prepping Data for AI) | label=FALSE\n",
            "     S: Models consistently produce biased predictions based on flawed data.\n",
            "     C: bias in AI models and data distortion\n",
            "     R: The claim misrepresents how bias impacts model predictions, as not all models over-predict.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.60,0.70,0.50) | overall=medium\n",
            "     fix: Revise label to 'PARTIALLY TRUE' to better reflect the statement's nuance.\n",
            "     note: Label does not fully align with the statement's implications.\n",
            "\n",
            "[41] id=548 | ch7 (Breaking-Securing AI) | label=half-true\n",
            "     S: The model relies heavily on community-contributed examples for learning.\n",
            "     C: community-contributed examples in generative AI\n",
            "     R: While community examples are important, the model's learning is also influenced by other factors.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.70) | overall=medium\n",
            "     note: Label and context are somewhat misaligned.\n",
            "\n",
            "[42] id=1508 | ch6 (Generative AI) | label=FALSE\n",
            "     S: Autoregressive models are ineffective for forecasting practical problems.\n",
            "     C: model performance in forecasting\n",
            "     R: The claim contradicts evidence showing autoregressive models are effective in capturing trends.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.70) | overall=medium\n",
            "     note: Label and statement have minor inconsistencies.\n",
            "\n",
            "[43] id=4208 | ch5 (Neuron Building Blocks) | label=pants-fire\n",
            "     S: Autoencoders cannot generate new data without specific modifications.\n",
            "     C: discussion of autoencoder capabilities\n",
            "     R: The passage mentions that generating new data requires a special kind of autoencoder, implying limitations.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.70) | overall=medium\n",
            "     note: Context is vague and doesn't enhance understanding.\n",
            "\n",
            "[44] id=3317 | ch3 (Classical Machine Learning) | label=pants-fire\n",
            "     S: Using Deities in training leads to higher prediction errors.\n",
            "     C: Mean Squared Error (MSE) in regression analysis\n",
            "     R: Including Deities caused MSE to exceed 1000, indicating poor dataset suitability.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.20,0.40,0.50,0.60) | overall=low\n",
            "     fix: Replace 'Deities' with 'irrelevant features' to improve clarity and relevance.\n",
            "     note: Misalignment with chapter theme and unclear label relevance.\n",
            "\n",
            "[45] id=4238 | ch1 (AI Survival Kit) | label=half-true\n",
            "     S: Prompt engineers enhance generative AI's effectiveness through language and techniques.\n",
            "     C: prompt engineering in AI models\n",
            "     R: While prompt engineers do improve AI outcomes, the passage doesn't claim they are essential.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.80,0.70,0.80) | overall=medium\n",
            "     note: Label and context are relevant but could be clearer.\n",
            "\n",
            "[46] id=2483 | ch0 (Introduction) | label=barely-true\n",
            "     S: AI systems are inherently reliable and rarely produce false answers.\n",
            "     C: AI systems producing confident but false answers\n",
            "     R: The passage highlights that AI can produce incorrect information, contradicting the claim of reliability.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.20,0.80,0.50,0.90) | overall=medium\n",
            "     note: Context contradicts the statement, affecting coherence.\n",
            "\n",
            "[47] id=2764 | ch11 (Agentic AI) | label=mostly-true\n",
            "     S: Early AI versions required adjustments for effective demonstration of agent capabilities.\n",
            "     C: AI-powered trivia challenge development\n",
            "     R: The statement reflects the need for refinement to meet demonstration goals.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.40,0.50,0.30,0.40) | overall=medium\n",
            "     fix: Revise context to focus on agent capabilities in AI development.\n",
            "     note: Context and label do not align well with chapter theme.\n",
            "\n",
            "[48] id=3585 | ch3 (Classical Machine Learning) | label=barely-true\n",
            "     S: Classical machine learning lacks effectiveness in structured data problems.\n",
            "     C: classical machine learning techniques\n",
            "     R: The passage emphasizes classical ML's value in efficiently solving structured data issues.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.60,0.40,0.50,0.30) | overall=low\n",
            "     fix: Classical ML excels in structured data problems, contrary to the statement.\n",
            "     note: Label and statement contradict each other significantly.\n",
            "\n",
            "[49] id=4942 | ch8 (Deepfake Defense) | label=barely-true\n",
            "     S: Restarting the Colab runtime always resolves installation errors.\n",
            "     C: installation process in Colab environment\n",
            "     R: While restarting can help, it doesn't guarantee all errors will be fixed.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.20,0.50,0.40,0.60) | overall=low\n",
            "     fix: Restarting the Colab runtime may help resolve some installation errors, but not all.\n",
            "     note: Weak alignment with chapter theme and label.\n",
            "\n",
            "[50] id=3307 | ch0 (Foreword) | label=mostly-true\n",
            "     S: The conversation emphasizes the potential of open-source AI tools.\n",
            "     C: discussion on open-source tools and innovation\n",
            "     R: The passage expresses confidence in inspiring open-source AI development, aligning with community growth.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.80,0.70,0.80) | overall=medium\n",
            "     note: Label and context are somewhat aligned but could be clearer.\n",
            "\n",
            "[51] id=361 | ch8 (Deepfake Defense) | label=barely-true\n",
            "     S: Knowledge alone effectively mitigates deepfake risks and builds trust.\n",
            "     C: deepfake defense strategies and tools\n",
            "     R: While knowledge is important, it does not fully address deepfake threats.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.70,0.60,0.50,0.60) | overall=medium\n",
            "     note: Label and context lack strong alignment with the statement.\n",
            "\n",
            "[52] id=722 | ch1 (AI Survival Kit) | label=TRUE\n",
            "     S: The starter set includes essential Python libraries for AI development.\n",
            "     C: installation of Python libraries for AI models\n",
            "     R: The statement accurately reflects the importance of the listed libraries for AI.\n",
            "     -> fit(ch,lab,ctx,rsn)=(1.00,1.00,0.80,0.90) | overall=high\n",
            "     note: Strong alignment with chapter theme and label.\n",
            "\n",
            "[53] id=4517 | ch11 (Agentic AI) | label=half-true\n",
            "     S: Tasks require specific agents to achieve desired outcomes.\n",
            "     C: Task attributes and agent responsibilities\n",
            "     R: The statement is correct but overlooks nuances about task context and dependencies.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.70,0.60,0.50,0.70) | overall=medium\n",
            "     note: Label and context need clearer alignment.\n",
            "\n",
            "[54] id=2228 | ch2 (Prepping Data for AI) | label=barely-true\n",
            "     S: Pandas cannot effectively analyze large datasets for missing values.\n",
            "     C: data analysis with Pandas\n",
            "     R: The passage highlights Pandas' effectiveness in quickly identifying sparse fields.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.20,0.50,0.30) | overall=low\n",
            "     fix: Pandas can analyze large datasets, but may struggle with missing values.\n",
            "     note: Label and reason do not align with the statement.\n",
            "\n",
            "[55] id=586 | ch3 (Classical Machine Learning) | label=half-true\n",
            "     S: Cosine similarity indicates clusters capture some common traits.\n",
            "     C: comparison of within-cluster and between-cluster similarity\n",
            "     R: The average similarity values show some valid clustering but also highlight discrepancies.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.70) | overall=medium\n",
            "     note: Context is somewhat relevant but lacks depth.\n",
            "\n",
            "[56] id=89 | ch9 (AI At Scale) | label=barely-true\n",
            "     S: Batch size 32 is the only effective option for model efficiency.\n",
            "     C: batch size optimization in model performance\n",
            "     R: The claim overlooks that batch sizes 32 to 64 also provide benefits.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.70,0.60,0.50,0.70) | overall=medium\n",
            "     note: Label and context lack clarity and precision.\n",
            "\n",
            "[57] id=449 | ch11 (Agentic AI) | label=mostly-true\n",
            "     S: Agentic AI achieves goals by breaking them into manageable steps.\n",
            "     C: process of goal execution in agentic AI\n",
            "     R: The description aligns with the adaptive strategy of agentic AI, emphasizing stepwise planning.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.80,0.70,0.90) | overall=medium\n",
            "     note: Context is somewhat vague and could be more specific.\n",
            "\n",
            "[58] id=1874 | ch5 (Neuron Building Blocks) | label=half-true\n",
            "     S: CNNs primarily focus on detecting edges in images.\n",
            "     C: function of CNNs in pattern recognition\n",
            "     R: While CNNs do detect edges, they also identify other patterns and features.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.80) | overall=medium\n",
            "     note: Context is somewhat relevant but lacks depth.\n",
            "\n",
            "[59] id=2515 | ch2 (Prepping Data for AI) | label=TRUE\n",
            "     S: Data masking and differential privacy enhance data security in AI applications.\n",
            "     C: data masking and differential privacy techniques\n",
            "     R: Both techniques improve privacy without significantly affecting data utility.\n",
            "     -> fit(ch,lab,ctx,rsn)=(1.00,1.00,0.90,1.00) | overall=high\n",
            "     note: Strong alignment with chapter theme and label.\n",
            "\n",
            "[60] id=1886 | ch6 (Generative AI) | label=TRUE\n",
            "     S: Open-source initiatives enhance access to generative AI models.\n",
            "     C: open-source alternatives to foundation models\n",
            "     R: Numerous projects are actively democratizing generative AI, making it more accessible.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.80,0.70,0.80) | overall=medium\n",
            "     note: Context is somewhat vague and could be more specific.\n",
            "\n",
            "[61] id=906 | ch6 (Generative AI) | label=barely-true\n",
            "     S: Continual training prevents catastrophic forgetting in neural networks.\n",
            "     C: catastrophic forgetting in neural networks\n",
            "     R: The passage mentions solutions but states they are beyond its scope.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.60,0.70,0.50) | overall=medium\n",
            "     fix: The statement accurately describes a solution to catastrophic forgetting in neural networks.\n",
            "     note: Label does not fully align with the statement's accuracy.\n",
            "\n",
            "[62] id=1084 | ch2 (Prepping Data for AI) | label=barely-true\n",
            "     S: Faker generates real health records that reveal private information.\n",
            "     C: Faker generating synthetic health records\n",
            "     R: Faker creates fictional data, not real records, ensuring privacy is maintained.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.60,0.70,0.90) | overall=medium\n",
            "     note: Label does not accurately reflect the statement's implication.\n",
            "\n",
            "[63] id=515 | ch2 (Prepping Data for AI) | label=FALSE\n",
            "     S: The method infers species without using a dataset.\n",
            "     C: function for inferring species\n",
            "     R: The claim contradicts the passage, which mentions applying logic across the dataset.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.80,0.70,0.90) | overall=medium\n",
            "     note: Minor misalignment between statement and label.\n",
            "\n",
            "[64] id=4708 | ch1 (AI Survival Kit) | label=mostly-true\n",
            "     S: Open source facilitates the inspection and improvement of AI agent behavior.\n",
            "     C: role of open source in AI development\n",
            "     R: The passage emphasizes open source's role in enhancing AI reliability and adaptability.\n",
            "     -> fit(ch,lab,ctx,rsn)=(1.00,1.00,0.80,0.90) | overall=high\n",
            "     note: Strong alignment with chapter theme and label.\n",
            "\n",
            "[65] id=4303 | ch11 (Agentic AI) | label=mostly-true\n",
            "     S: Agentic AI employs game-based strategies for decision-making and action.\n",
            "     C: Agentic AI and game-based learnings\n",
            "     R: The claim reflects how AI adapts game strategies for practical applications.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.80,0.70,0.80) | overall=medium\n",
            "     note: Context is somewhat vague and could be clearer.\n",
            "\n",
            "[66] id=1185 | ch7 (Breaking-Securing AI) | label=TRUE\n",
            "     S: Building secure AI requires skepticism and proven instincts.\n",
            "     C: defensive strategies in AI development\n",
            "     R: The passage emphasizes the need for skepticism and applying existing knowledge to AI security.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.90,0.70,0.80) | overall=medium\n",
            "     note: Context is somewhat relevant but lacks depth.\n",
            "\n",
            "[67] id=381 | ch3 (Classical Machine Learning) | label=FALSE\n",
            "     S: Baseline accuracy is always lower than training accuracy.\n",
            "     C: baseline accuracy score calculation\n",
            "     R: The passage indicates a baseline accuracy of 65%, contradicting the claim.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.70) | overall=medium\n",
            "     note: Label and statement have minor inconsistencies.\n",
            "\n",
            "[68] id=501 | ch6 (Generative AI) | label=mostly-true\n",
            "     S: Meta-awareness influences the understanding of generative AI concepts.\n",
            "     C: discussion on meta-awareness in generative AI\n",
            "     R: The concept of meta-awareness is acknowledged as impactful, suggesting broader implications for understanding AI.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.80,0.70,0.80) | overall=medium\n",
            "     note: Context is somewhat relevant but lacks depth.\n",
            "\n",
            "[69] id=4567 | ch4 (Deep Learning) | label=pants-fire\n",
            "     S: Adam fails to improve model learning efficiency with noisy data.\n",
            "     C: model training with Adam optimizer\n",
            "     R: The passage states that Adam helps models learn efficiently despite noisy data.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.70,0.40,0.60,0.30) | overall=low\n",
            "     fix: Label as 'inaccurate' and clarify that Adam struggles with noisy data.\n",
            "     note: Label and reason contradict the statement.\n",
            "\n",
            "[70] id=391 | ch2 (Prepping Data for AI) | label=FALSE\n",
            "     S: RAG cannot utilize curated data at query time effectively.\n",
            "     C: RAG utilization in data preparation\n",
            "     R: RAG is explicitly described as utilizing curated data at query time.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.20,0.50,0.10) | overall=low\n",
            "     fix: RAG cannot effectively utilize curated data at query time.\n",
            "     note: Label contradicts the statement; context is vague.\n",
            "\n",
            "[71] id=2164 | ch0 (Introduction) | label=barely-true\n",
            "     S: The next generation of AI will be driven by open-source builders.\n",
            "     C: community-driven development in AI\n",
            "     R: The claim exaggerates the role of open-source builders while downplaying corporate contributions.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.70) | overall=medium\n",
            "     note: Context is somewhat relevant but lacks depth.\n",
            "\n",
            "[72] id=608 | ch2 (Prepping Data for AI) | label=mostly-true\n",
            "     S: Sensitive data includes personal details, financial records, and health information.\n",
            "     C: definition of sensitive data in AI preparation\n",
            "     R: The statement accurately describes types of sensitive data mentioned in the passage.\n",
            "     -> fit(ch,lab,ctx,rsn)=(1.00,1.00,0.90,1.00) | overall=high\n",
            "     note: Strong alignment with chapter theme and clear reasoning.\n",
            "\n",
            "[73] id=740 | ch7 (Breaking-Securing AI) | label=mostly-true\n",
            "     S: AI systems can generate convincing but fabricated information.\n",
            "     C: Hallucinated Confidence concept in AI systems\n",
            "     R: The passage highlights how AI can produce believable yet false information, leading to potential misuse.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.80,0.70,0.90) | overall=medium\n",
            "     note: Context is somewhat relevant but lacks depth.\n",
            "\n",
            "[74] id=403 | ch2 (Prepping Data for AI) | label=pants-fire\n",
            "     S: Ymir chooses to abandon his powers permanently after saving his sister.\n",
            "     C: Ymir's sacrifice and embrace of a quiet life\n",
            "     R: The claim contradicts the passage, as it suggests a permanent loss of powers not mentioned.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.20,0.30,0.40,0.50) | overall=low\n",
            "     fix: Revise label to reflect Ymir's sacrifice and clarify context relevance.\n",
            "     note: Label and context do not align with the chapter theme.\n",
            "\n",
            "[75] id=4193 | ch5 (Neuron Building Blocks) | label=FALSE\n",
            "     S: Reading notes in order is not an effective learning strategy.\n",
            "     C: discussion on reading strategies\n",
            "     R: The passage implies that sequential reading is less effective than comprehensive reading.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.70) | overall=medium\n",
            "     note: Label and context somewhat misaligned with chapter theme.\n",
            "\n",
            "[76] id=3977 | ch4 (Deep Learning) | label=TRUE\n",
            "     S: The training loop helps deep neural networks improve over time.\n",
            "     C: parameter update in deep learning models\n",
            "     R: The passage describes how the training loop enhances model performance through parameter adjustments.\n",
            "     -> fit(ch,lab,ctx,rsn)=(1.00,1.00,0.90,1.00) | overall=high\n",
            "     note: Strong alignment with chapter and clear reasoning.\n",
            "\n",
            "[77] id=1127 | ch7 (Breaking-Securing AI) | label=TRUE\n",
            "     S: Tools like OpenSSF Scorecards enhance code quality by identifying issues early.\n",
            "     C: IDE tools improving code quality\n",
            "     R: The passage emphasizes the value of tools that surface problems before code deployment.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.70,0.80,0.60,0.70) | overall=medium\n",
            "     note: Context is vague and doesn't directly relate to AI security.\n",
            "\n",
            "[78] id=4231 | ch11 (Agentic AI) | label=TRUE\n",
            "     S: Developers must understand prompt design to effectively use AI models.\n",
            "     C: AI-enabled application development\n",
            "     R: Effective use of AI models relies on understanding prompt design principles.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.60,0.80,0.50,0.70) | overall=medium\n",
            "     note: Context is vague and doesn't directly relate to Agentic AI.\n",
            "\n",
            "[79] id=3722 | ch5 (Neuron Building Blocks) | label=half-true\n",
            "     S: The model employs a loop for continuous English-to-French translation.\n",
            "     C: command-line loop for user interaction\n",
            "     R: While the loop exists, it oversimplifies the model's capabilities and focus.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.40,0.60,0.50,0.70) | overall=medium\n",
            "     note: Context is somewhat relevant but lacks depth.\n",
            "\n",
            "[80] id=2965 | ch5 (Neuron Building Blocks) | label=TRUE\n",
            "     S: A pretrained question-answering model processes user questions about uploaded PDF content.\n",
            "     C: question-answering model with document text\n",
            "     R: The implementation directly utilizes a pretrained model to answer questions based on extracted text.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.70,0.80,0.60,0.70) | overall=medium\n",
            "     note: Context is somewhat relevant but lacks depth.\n",
            "\n",
            "[81] id=3363 | ch0 (Introduction) | label=half-true\n",
            "     S: Curious builders require collaboration to effectively shape AI.\n",
            "     C: open-source community and AI development\n",
            "     R: While builders value collaboration, the statement implies exclusivity in need, which is misleading.\n",
            "     -> fit(ch,lab,ctx,rsn)=(1.00,0.70,0.80,0.60) | overall=medium\n",
            "     note: Label reason lacks clarity and direct support for the label.\n",
            "\n",
            "[82] id=3514 | ch3 (Classical Machine Learning) | label=mostly-true\n",
            "     S: Median imputation is a practical method for handling missing numeric values.\n",
            "     C: data-prep with SimpleImputer\n",
            "     R: Median imputation is effective but can introduce bias if missingness is not random.\n",
            "     -> fit(ch,lab,ctx,rsn)=(1.00,0.80,0.90,0.70) | overall=medium\n",
            "     note: Label slightly misaligns with statement's implication.\n",
            "\n",
            "[83] id=1739 | ch4 (Deep Learning) | label=TRUE\n",
            "     S: Adam optimizer and cross-entropy loss are used for training models.\n",
            "     C: training process with Adam and cross-entropy\n",
            "     R: The passage details the use of Adam and cross-entropy during model training.\n",
            "     -> fit(ch,lab,ctx,rsn)=(1.00,1.00,0.80,1.00) | overall=high\n",
            "     note: Strong alignment with chapter theme and clear reasoning.\n",
            "\n",
            "[84] id=1748 | ch9 (AI At Scale) | label=FALSE\n",
            "     S: Voice-cloning models primarily rely on internal tuning for scalability.\n",
            "     C: model behavior during benchmarking and scaling\n",
            "     R: Scalability is largely influenced by external factors, not just model tuning.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.70,0.60,0.50,0.60) | overall=medium\n",
            "     note: Label and context have minor inconsistencies.\n",
            "\n",
            "[85] id=3316 | ch3 (Classical Machine Learning) | label=TRUE\n",
            "     S: Mean Squared Error indicates prediction accuracy in regression analysis.\n",
            "     C: Mean Squared Error metric in regression\n",
            "     R: The statement reflects the role of MSE in assessing model performance.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,1.00,0.80,0.90) | overall=high\n",
            "     note: Strong alignment with chapter and clear reasoning.\n",
            "\n",
            "[86] id=2687 | ch11 (Agentic AI) | label=barely-true\n",
            "     S: Few-shot learning does not enhance model understanding of nuance effectively.\n",
            "     C: few-shot learning and model clarity\n",
            "     R: The passage indicates that few-shot learning improves clarity and understanding, contradicting this claim.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.40,0.20,0.50,0.30) | overall=low\n",
            "     fix: Revise label to 'false' for clarity on the statement's accuracy.\n",
            "     note: Label and statement are contradictory.\n",
            "\n",
            "[87] id=3098 | ch9 (AI At Scale) | label=mostly-true\n",
            "     S: AI at scale can enhance media forensics by promoting transparency.\n",
            "     C: AI at scale and media forensics\n",
            "     R: The claim reflects the passage's emphasis on transparency in AI, supporting reliable outputs.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.80,0.70,0.80) | overall=medium\n",
            "     note: Context is vague; could be more specific.\n",
            "\n",
            "[88] id=383 | ch3 (Classical Machine Learning) | label=TRUE\n",
            "     S: Baseline accuracy improved to 65% by focusing on two classes.\n",
            "     C: baseline accuracy score calculation\n",
            "     R: The statement reflects the passage's result of narrowing classes to increase accuracy.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.90,0.70,0.80) | overall=medium\n",
            "     note: Context is somewhat relevant but lacks depth.\n",
            "\n",
            "[89] id=4011 | ch8 (Deepfake Defense) | label=pants-fire\n",
            "     S: Cloning voices for defense training is a deceptive practice.\n",
            "     C: voice cloning as a hands-on exercise\n",
            "     R: The passage states the exercise aims to teach defense, not deception.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.70) | overall=medium\n",
            "     note: Context lacks depth; label could be clearer.\n",
            "\n",
            "[90] id=1514 | ch0 (Foreword) | label=mostly-true\n",
            "     S: The Hugging Face emoji name originated as a joke but gained community support.\n",
            "     C: community adoption of the emoji name\n",
            "     R: The statement accurately reflects the origin and community involvement, with minor details about its humorous start.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.80,0.70,0.80) | overall=medium\n",
            "     note: Context could be more specific to the statement.\n",
            "\n",
            "[91] id=1901 | ch0 (Foreword) | label=barely-true\n",
            "     S: Working with generative tools normalizes synthetic content for future AI literacy.\n",
            "     C: open collaboration in AI development\n",
            "     R: The claim overstates the role of generative tools in normalizing synthetic content.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.70) | overall=medium\n",
            "     note: Context is vague and doesn't directly support the statement.\n",
            "\n",
            "[92] id=1344 | ch0 (Foreword) | label=half-true\n",
            "     S: Robo is an AI developed with open-source software to facilitate learning interactions.\n",
            "     C: AI designed for learning interactions\n",
            "     R: While Robo is built on open-source software, details about her effectiveness in learning are unclear.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.70) | overall=medium\n",
            "     note: Label and context need clearer alignment.\n",
            "\n",
            "[93] id=2225 | ch3 (Classical Machine Learning) | label=half-true\n",
            "     S: Encoding categorical data into integers is a necessary step for AI models.\n",
            "     C: data preparation and encoding process\n",
            "     R: While encoding is crucial, the statement overlooks potential challenges in data consistency.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.70) | overall=medium\n",
            "     note: Label and context could be clearer and more aligned.\n",
            "\n",
            "[94] id=1519 | ch2 (Prepping Data for AI) | label=TRUE\n",
            "     S: Open-source libraries like Scikit-learn enable data outcome predictions.\n",
            "     C: tools for data preparation and prediction\n",
            "     R: The statement accurately reflects the use of Scikit-learn for predicting outcomes.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.90,0.90,0.80,0.90) | overall=high\n",
            "     note: Strong alignment with chapter and label.\n",
            "\n",
            "[95] id=1593 | ch6 (Generative AI) | label=barely-true\n",
            "     S: Selective fine-tuning is rarely necessary for complex real-world applications.\n",
            "     C: Selective Fine-Tuning techniques in GANs\n",
            "     R: The passage states selective fine-tuning is essential for real-world domains.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.40,0.30,0.50,0.20) | overall=low\n",
            "     fix: Selective fine-tuning is often unnecessary in complex applications, contrary to the label's implication.\n",
            "     note: Label and reason contradict the statement; context is tangential.\n",
            "\n",
            "[96] id=356 | ch9 (AI At Scale) | label=barely-true\n",
            "     S: Deploying models as a service limits user installation requirements.\n",
            "     C: model deployment as a service\n",
            "     R: While it reduces installation needs, it does not restrict flexibility in usage.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.70,0.60,0.50,0.60) | overall=medium\n",
            "     note: Label and context somewhat misaligned with chapter theme.\n",
            "\n",
            "[97] id=1067 | ch8 (Deepfake Defense) | label=pants-fire\n",
            "     S: Fine-tuning SpeechT5 on unique vocal signatures is ineffective for accurate voice synthesis.\n",
            "     C: model fine-tuning for speech synthesis\n",
            "     R: Claim contradicts the method's purpose of improving voice mapping accuracy.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.70,0.60,0.70) | overall=medium\n",
            "     note: Label and context somewhat misaligned with chapter theme.\n",
            "\n",
            "[98] id=343 | ch3 (Classical Machine Learning) | label=barely-true\n",
            "     S: Logistic regression accurately predicts Cyborgs in most cases.\n",
            "     C: Model performance metrics for superhero Species classification\n",
            "     R: Cyborgs were missed entirely, indicating no accurate predictions.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.20,0.10,0.30,0.20) | overall=low\n",
            "     fix: Logistic regression fails to predict Cyborgs, leading to inaccurate results.\n",
            "     note: Significant misalignment between statement and label.\n",
            "\n",
            "[99] id=2426 | ch2 (Prepping Data for AI) | label=FALSE\n",
            "     S: Imputing data does not improve model performance.\n",
            "     C: feature engineering and model performance\n",
            "     R: The passage states that careful feature engineering enhances model performance, contradicting the claim.\n",
            "     -> fit(ch,lab,ctx,rsn)=(0.80,0.60,0.70,0.50) | overall=medium\n",
            "     fix: Imputing data may not enhance model performance, contrary to feature engineering benefits.\n",
            "     note: Label and reasoning do not align well.\n",
            "\n",
            "[100] id=4817 | ch6 (Generative AI) | label=mostly-true\n",
            "     S: Transformers excel at complex tasks beyond text, including music and video generation.\n",
            "     C: applications of Transformer architecture\n",
            "     R: Transformers' self-attention enables them to perform well in diverse generation tasks.\n",
            "     -> fit(ch,lab,ctx,rsn)=(1.00,1.00,0.90,1.00) | overall=high\n",
            "     note: Strong alignment with chapter and clear reasoning.\n",
            "\n",
            "=== Aggregate (sample) ===\n",
            "chapter_fit avg:     0.759\n",
            "label_fit avg:       0.669\n",
            "context_quality avg: 0.637\n",
            "reason_quality avg:  0.673\n",
            "overall ratings:      {'medium': 72, 'low': 15, 'high': 13}\n"
          ]
        }
      ]
    }
  ]
}