{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building the BYOAI_LIAR Dataset\n",
        "*A Practical Example of Building Your Own AI, with a little help from AI*\n",
        "\n",
        "## Why we built a LIAR-style dataset for the book\n",
        "For the chapter on scaling, we wanted a model that shows more than speed or accuracy. We wanted something that also explores how to scale trust. The classic LIAR dataset is a good inspiration. It labels short statements on a six-level truth scale: TRUE, mostly-true, half-true, barely-true, FALSE, and pants-fire. That framing fit our goals and also supports teaching. One of the authors teaches trustworthy AI and this gives a steady source of chapter-anchored multiple choice questions.\n",
        "\n",
        "> The LIAR dataset is a large collection of 12,836 short, human-labeled statements about their truthfulness, collected from the fact-checking website PolitiFact.com. It was created by William Yang Wang and released in a 2017 paper titled \"Liar, Liar Pants on Fire”: A New Benchmark Dataset for Fake News Detection, making it a valuable resource for fake news detection research.\n",
        "\n",
        "\n",
        "Our version, which we call **byoai_liar**, uses passages from the book’s chapters to generate short labeled statements. A future classifier can then learn to predict the same six labels. The result doubles as a training set and as an educational asset tied to the book’s topics.\n",
        "\n",
        "## Planning the build with a code assistant\n",
        "We decided to treat the whole exercise as a worked example of using a coding copilot. Most of the work was done with ChatGPT and Gemini. The plan had two parts:\n",
        "\n",
        "1. **Chunk the chapters** into small windows of text that carry enough context to inspire grounded statements.  \n",
        "2. **Generate labeled statements** from those windows, along with a short context phrase and a short reason for the label.\n",
        "\n",
        "We aimed for about 2,500 to 5,000 rows so a small model like T5-small would have something meaningful to learn from. With more than 1,800 chunks available from the drafts, that target looked realistic.\n",
        "\n",
        "## Data preparation\n",
        "We took a fresh snapshot of the chapter drafts and exported each chapter as a UTF-16 plain text file. That ensured simple, repeatable inputs. We then defined the six labels as constants, based on the LIAR model card, and kept them front and center in every prompt and function.\n",
        "\n",
        "Next, we added **subject tags**. A chapter title alone does not always convey the core topics. We asked the code assistant to propose three concise tags per chapter. These tags appear in the chunk file and later guide generation. For example, the deepfake chapter carries tags like media-forensics, voice-cloning, deepfake. The agentic AI chapter carries agentic-ai, planning, tools. These tags flow through the pipeline and help the generator ground contexts without long prompts.\n",
        "\n",
        ">**Tip:** Before writing any code, you can use a chat-based content analysis to define data structures like we did with: `SUBJECT_TAGS`. Here's what we did.\n",
        "Create a new ChatGPT project, upload your chapter `.txt` files into its file folder, and prompt:  \n",
        "*\"Scan each file and extract three concise tags that best represent its subject or theme. Then output a Python constant named SUBJECT_TAGS mapping chapter numbers to those tags.\"*  \n",
        "Review the results, refine the prompt if any tags feel off, and rerun until each chapter is well represented. This step anchors your later code generation in real content, ensuring your constants match the book’s themes rather than generic guesses.\n",
        "\n",
        "## Listing 1: the Chunker\n",
        "The chunker walks each chapter, breaks it into overlapping windows, and emits a `chunks.csv` with:\n",
        "\n",
        "- `chapter`, `chapter_title`, `chunk_id`, `chunk_text`, `subject_tags`\n",
        "\n",
        "The window size took some tuning. We tested different widths and strides, looking for enough context to inspire a claim, but short enough to keep the model focused. We added simple summary prints to show counts, token ranges, and per-chapter coverage. Those prints paid off. Each round of changes was based on facts rather than gut feel.\n",
        "\n",
        "A key decision was to produce enough chunks so the generator could sample several labels per chunk later. That matters because a single passage can inspire both supported claims and claims that go a bit too far. We want the generator to produce that contrast so the classifier has something to learn.\n",
        "\n",
        ">**Tip:** Let your AI code assistant help you find a practical window size before finalizing the chunker.  \n",
        "Upload one or two representative chapter text files and ask:  \n",
        "*\"Write a short script to split this text into overlapping windows and report average word counts per window for different stride and size values.\"*  \n",
        "Run the suggested code, share the output back with the assistant, and ask:  \n",
        "*\"Based on these stats, what window and stride would balance context and focus for training data generation?\"*  \n",
        "This experiment-driven loop helps you converge on numbers that fit your chapters, rather than guessing.\n",
        "\n",
        "## Listing 2: the single-pass Generator\n",
        "Early versions tried a generator plus a separate QA reviewer. That added complexity. The final approach is simpler and more reliable for this use case:\n",
        "\n",
        "**One prompt. One call per row. No second pass.**\n",
        "\n",
        "The generator takes `chunks.csv` and a few simple constants:\n",
        "\n",
        "- `MAX_CHUNKS` controls how many chunks to process in this run.\n",
        "- `N_PER_CHUNK_TARGET` controls how many rows to attempt per chunk.\n",
        "- `LABELS` and `LABEL_WEIGHTS` control the truth distribution.\n",
        "\n",
        "For each chunk, we sample a label according to the weights and create a single prompt that includes the target label, the chapter info, the subject tags, and the chunk text. The model returns exactly three fields:\n",
        "\n",
        "- `statement` in 20 words or fewer  \n",
        "- `context` in 12 words or fewer  \n",
        "- `label_reason` in 8 to 28 words\n",
        "\n",
        "The rules in the prompt keep things consistent: the label drives the claim, the context anchors to a specific term from the passage or tags, and the reason explains the label in natural language. We ask for varied phrasing and concrete terms such as tools, models, datasets, or metrics. We also ask for natural justification wording so we do not get repetitive openings like “The passage states…”\n",
        "\n",
        ">**Tip:** Use your AI code assistant as a **prompt design partner**, not just a coder.  \n",
        "Upload your `chunks.csv` file into the project, then ask:  \n",
        "*\"Given this dataset, help me design a single, efficient prompt that generates a labeled statement, short context, and natural justification for each chunk and target label.\"*  \n",
        "Experiment by pasting a few sample chunks and iterating on the wording of the generation rules together.  \n",
        "When the model starts returning well-balanced examples, copy that exact text into your `GEN_PROMPT` constant.  \n",
        "This interactive process helps you tune the generator’s behavior before you automate it in code.\n",
        "\n",
        "## Light validation instead of heavy post-processing\n",
        "After each generation, we run quick checks:\n",
        "\n",
        "- Word count bounds  \n",
        "- No meta words like “chapter” or “this statement”  \n",
        "- A short-hash dedupe on the statement\n",
        "\n",
        "If a row fails a check, we retry once. If it fails again, we skip it. These small rules keep the output clean without slowing the pipeline.\n",
        "\n",
        "## Output schema and reproducibility\n",
        "The generator writes `byoai_liar.csv` with:\n",
        "\n",
        "- `id, chunk_id, label, statement, context, label_reason, subject_tags, chapter, chapter_title`\n",
        "\n",
        "This makes it easy to trace any statement back to its origin. The fields also give you the ingredients a future classifier or evaluator needs, including a short rationale that can support training or auditing.\n",
        "\n",
        "## Scaling up without surprises\n",
        "To reach 4,000 to 5,000 rows, pick a combination of `MAX_CHUNKS` and `N_PER_CHUNK_TARGET` that hits your target. For example, 600 chunks with 8 rows per chunk yields about 4,800 attempts. After dedupe and bounds checks, the final count lands near the goal. For variety, set temperature around 0.65 to 0.7 and top-p around 0.9. If you want even coverage across chapters, switch the chunk selection from random to a balanced sampler.\n",
        "\n",
        "## A small audit tool helps you stay honest\n",
        "We added a tiny “coherence and label audit” tool that samples random rows from the final CSV and asks a model to rate:\n",
        "\n",
        "- chapter fit  \n",
        "- label fit  \n",
        "- context quality  \n",
        "- reason quality\n",
        "\n",
        "It prints a few lines per row and then averages. If context quality dips, nudge the prompt to ask for a named anchor term from the passage or tags. If reasons look mechanical, remind the model to explain the relationship naturally without report-style phrases. This loop is fast and keeps the dataset from drifting.\n",
        "\n",
        "## What an AI builder learns from this pattern\n",
        "**Design the target first.** The six labels and the output schema framed every decision that followed.  \n",
        "**Break the work into two clear listings.** One prepares focused context. One generates structured rows.  \n",
        "**Keep prompts compact and precise.** One well-designed prompt beats a long chain of fragile steps.  \n",
        "**Use simple checks.** Bounds and dedupe give most of the benefits of a second pass, with a fraction of the effort.  \n",
        "**Iterate on small samples.** Run ten chunks, study the rows, adjust one rule, and run again.  \n",
        "**Instrument the code.** Print basic stats after each run. Simple numbers are better than guesswork.  \n",
        "**Add a tiny auditor.** A lightweight, randomized audit helps you catch drift early and gives you confidence at scale.\n",
        "\n",
        "## Where you can take it next\n",
        "Once the dataset looks good, you can train a small text classifier such as T5-small or a compact encoder-based model. You can also slice the CSV per chapter or per tag to build practice sets for a class. Since each row has a short reason, you can experiment with explanation-aware training, evaluation rubrics, or teaching assistants that quiz readers on chapter topics.\n",
        "\n",
        "That is the core pattern. Start with plain text chapters. Chunk them. Drive a single-pass generator with clear rules. Keep the checks light, the prompts short, and the iterations quick. You will end up with a dataset that is useful for training and also clear enough to support teaching and review.\n"
      ],
      "metadata": {
        "id": "lacQJAvW9OJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 1: BYOAI Chapter Chunker → `byoai_book_chunks.csv`\n",
        "\n",
        "This script scans all chapter `.txt` files, slices them into overlapping\n",
        "sentence windows, and writes the results to `byoai_book_chunks.csv`. Each record\n",
        "contains the chapter number, title, chunk ID, text window, and up to three\n",
        "subject tags representing the chapter’s main themes.\n",
        "\n",
        "Text is read as **UTF-16** (falling back to **UTF-8**), cleaned for encoding\n",
        "artifacts, split into sentences, and filtered to omit short fragments.\n",
        "Helper functions include:\n",
        "`read_text_any()` for robust reading,\n",
        "`sentence_split()` for segmentation,\n",
        "and `make_windows()` for constructing rolling windows using\n",
        "`WINDOW_SIZE` and `WINDOW_STRIDE`.\n",
        "\n",
        "**Output schema:**\n",
        "*chapter | chapter_title | chunk_id | chunk_text | subject_tags*\n",
        "\n",
        "**Example:**\n",
        "\n",
        "*5 | Deep Learning | 42 | “PyTorch introduced eager execution…” | “deep-learning,frameworks,tensors”*\n",
        "\n",
        "The resulting `byoai_book_chunks.csv` provides consistent, context-rich samples for generation, labeling, and analysis while preserving chapter context and\n",
        "subject tags.\n"
      ],
      "metadata": {
        "id": "fVN0mLepIsHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BYOAI Chapter Chunker v3 → chunks.csv -----------------------------------\n",
        "# Scans all *.txt chapters (UTF-16 preferred; falls back to UTF-8),\n",
        "# slices rolling sentence windows, and writes a lean CSV for generators.\n",
        "#\n",
        "# Output schema (per row):\n",
        "#   chapter (int) | chapter_title (str) | chunk_id (int) | chunk_text (str)\n",
        "#   subject_tags (str; comma-separated, ≤3 per chapter)\n",
        "\n",
        "import re\n",
        "import csv\n",
        "import glob\n",
        "import pathlib\n",
        "import collections\n",
        "\n",
        "# ===================== CONFIG =====================\n",
        "CHAPTER_GLOB   = \"*.txt\"     # pattern for chapter text files\n",
        "OUTPUT_CSV     = \"byoai_book_chunks.csv\"\n",
        "WINDOW_SIZE    = 4           # sentences per window\n",
        "WINDOW_STRIDE  = 3           # step size between windows\n",
        "MIN_CHARS      = 260         # drop very short/meta fragments\n",
        "# ==================================================\n",
        "\n",
        "# Canonical chapter titles (0: Foreword, 1: Introduction, \"Chapter N\" → N+1)\n",
        "CHAPTER_TITLES = {\n",
        "     0: \"Foreword – Robo Interviews Clément Delangue\",\n",
        "     1: \"Introduction – The Gold Rush Paradox\",\n",
        "     2: \"AI Survival Kit\",\n",
        "     3: \"Prepping Data for AI\",\n",
        "     4: \"Classical Machine Learning\",\n",
        "     5: \"Deep Learning\",\n",
        "     6: \"Neuron Building Blocks\",\n",
        "     7: \"Generative AI\",\n",
        "     8: \"Breaking-Securing AI\",\n",
        "     9: \"Deepfake Defense\",\n",
        "    10: \"AI At Scale\",\n",
        "    11: \"AI Ethics and Governance\",\n",
        "    12: \"Agentic AI\",\n",
        "    13: \"Commit to Contribute\",\n",
        "}\n",
        "\n",
        "# === SUBJECT TAGS (3 per chapter max) ===\n",
        "# Used for guiding statement placement and downstream grouping.\n",
        "SUBJECT_TAGS = {\n",
        "     0: [\"open-source\", \"community\", \"ai\"],                 # Foreword\n",
        "     1: [\"ai\", \"open-source\", \"builder\"],                   # Introduction\n",
        "     2: [\"ai\", \"tool-chain\", \"notebooks\"],                  # Ch1 – Survival Kit\n",
        "     3: [\"data-prep\", \"feature-engineering\", \"rag\"],        # Ch2 – Prepping Data\n",
        "     4: [\"machine-learning\", \"classification\", \"evaluation\"],# Ch3 – Classical ML\n",
        "     5: [\"deep-learning\", \"frameworks\", \"tensors\"],         # Ch4 – Deep Learning\n",
        "     6: [\"neural-networks\", \"cnn\", \"transformers\"],         # Ch5 – Neuron Blocks\n",
        "     7: [\"generative-ai\", \"diffusion\", \"gans\"],             # Ch6 – Gen AI\n",
        "     8: [\"security\", \"red-team\", \"guardrails\"],             # Ch7 – Break/Secure\n",
        "     9: [\"media-forensics\", \"voice-cloning\", \"deepfake\"],   # Ch8 – Deepfake Def.\n",
        "    10: [\"mlops\", \"scaling\", \"deployment\"],                 # Ch9 – At Scale\n",
        "    11: [\"ethics\", \"governance\", \"privacy\"],                # Ch10 – Ethics/Gov\n",
        "    12: [\"agentic-ai\", \"planning\", \"tools\"],                # Ch11 – Agentic AI\n",
        "    13: [\"open-source\", \"community\", \"contribution\"],       # Ch12 – Commit\n",
        "}\n",
        "\n",
        "def fix_mojibake(t: str) -> str:\n",
        "    \"\"\"Light cleanup for common encoding artifacts from word processors.\"\"\"\n",
        "    repl = {\n",
        "        \"‚Äôs\": \"’s\", \"‚Äô\": \"’\", \"‚Äú\": \"“\", \"‚Äù\": \"”\",\n",
        "        \"‚Äì\": \"–\",  \"‚Äî\": \"—\", \"‚Ä¶\": \"…\", \"Â\": \"\"\n",
        "    }\n",
        "    for k, v in repl.items():\n",
        "        t = t.replace(k, v)\n",
        "    return t\n",
        "\n",
        "def read_text_any(path: str) -> str:\n",
        "    \"\"\"Read file as UTF-16 first, then UTF-8 as a fallback; normalize newlines.\"\"\"\n",
        "    try:\n",
        "        txt = open(path, \"r\", encoding=\"utf-16\").read()\n",
        "    except Exception:\n",
        "        txt = open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\").read()\n",
        "    return fix_mojibake(txt.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\"))\n",
        "\n",
        "def sentence_split(text: str):\n",
        "    \"\"\"Split text into sentences using simple punctuation-based rule.\"\"\"\n",
        "    parts = re.split(r\"(?<=[.!?])\\s+\", text)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "def make_windows(text: str, n: int, stride: int):\n",
        "    \"\"\"Create rolling windows of n sentences with a given stride.\"\"\"\n",
        "    sents = sentence_split(text)\n",
        "    return [\" \".join(sents[i:i+n]) for i in range(0, len(sents), stride)]\n",
        "\n",
        "def parse_chapter_from_filename(fname: str):\n",
        "    \"\"\"Infer (chapter_index, canonical_title) from filename text.\"\"\"\n",
        "    low = fname.lower()\n",
        "    if \"foreword\" in low:\n",
        "        return 0, CHAPTER_TITLES[0]\n",
        "    if \"introduction\" in low:\n",
        "        return 1, CHAPTER_TITLES[1]\n",
        "    m = re.search(r\"chapter\\D*?(\\d{1,2})\", low)\n",
        "    if m:\n",
        "        n = int(m.group(1)) + 1\n",
        "        return n, CHAPTER_TITLES.get(n, pathlib.Path(fname).stem)\n",
        "    # Fallback to Introduction if naming is irregular\n",
        "    return 1, CHAPTER_TITLES[1]\n",
        "\n",
        "def tags_for_chapter(chapter: int) -> str:\n",
        "    \"\"\"Return comma-separated subject tags for a chapter (≤3).\"\"\"\n",
        "    tags = SUBJECT_TAGS.get(chapter, [\"ai\"])\n",
        "    return \",\".join(tags[:3]) if tags else \"ai\"\n",
        "\n",
        "def main():\n",
        "    \"\"\"Scan chapter files, window sentences, filter, and write CSV.\"\"\"\n",
        "    files = sorted(glob.glob(CHAPTER_GLOB))\n",
        "    if not files:\n",
        "        raise SystemExit(f\"No files matched {CHAPTER_GLOB}\")\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    # Iterate each text file and produce rolling windows\n",
        "    for path in files:\n",
        "        if not path.lower().endswith(\".txt\"):\n",
        "            continue\n",
        "\n",
        "        # Parse chapter index and canonical title\n",
        "        chapter, title = parse_chapter_from_filename(path)\n",
        "\n",
        "        # Read raw text and skip empties\n",
        "        text = read_text_any(path)\n",
        "        if not text.strip():\n",
        "            continue\n",
        "\n",
        "        # Build sentence windows for this chapter\n",
        "        windows = make_windows(text, WINDOW_SIZE, WINDOW_STRIDE)\n",
        "\n",
        "        # Collect non-trivial windows with metadata and tags\n",
        "        chunk_id = 0\n",
        "        for w in windows:\n",
        "            if len(w) < MIN_CHARS:\n",
        "                continue\n",
        "            rows.append({\n",
        "                \"chapter\": chapter,\n",
        "                \"chapter_title\": title,\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"chunk_text\": w,\n",
        "                \"subject_tags\": tags_for_chapter(chapter),\n",
        "            })\n",
        "            chunk_id += 1\n",
        "\n",
        "    # Write CSV with BOM for spreadsheet-friendly behavior\n",
        "    with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
        "        w = csv.DictWriter(\n",
        "            f,\n",
        "            fieldnames=[\n",
        "                \"chapter\", \"chapter_title\", \"chunk_id\",\n",
        "                \"chunk_text\", \"subject_tags\"\n",
        "            ]\n",
        "        )\n",
        "        w.writeheader()\n",
        "        for r in rows:\n",
        "            w.writerow(r)\n",
        "\n",
        "    # Console summary to sanity-check coverage\n",
        "    by_chapter = collections.Counter(r[\"chapter\"] for r in rows)\n",
        "    print(f\"Wrote {len(rows)} rows to {OUTPUT_CSV}\")\n",
        "    print(\"Windows per chapter:\", dict(sorted(by_chapter.items())))\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "Lis2S_MxlzZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 1A: BYOAI Chapter Chunk Quality Check\n",
        "\n",
        "This script validates and summarizes the `byoai_book_chunks.csv` output from Listing 1.\n",
        "It checks structural integrity, verifies column presence, and reports per-chapter\n",
        "and global statistics such as word and sentence counts. It also measures\n",
        "distribution balance, identifies overly short or long chunks, and prints\n",
        "keyword and subject-tag coverage to assess content diversity.\n",
        "\n",
        "Key outputs include per-chapter averages (`mean_words`, `mean_sents`), overall\n",
        "word-length ranges, and tag frequency rankings. Chunks below `SHORT_LIMIT`\n",
        "or above `LONG_LIMIT` are flagged, with a sample preview printed for inspection.\n",
        "Together, these diagnostics ensure the dataset remains consistent, balanced,\n",
        "and thematically representative for downstream text generation and analysis."
      ],
      "metadata": {
        "id": "ll40MnmbI9Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === BYOAI Chapter Chunk Quality Check ========================================\n",
        "# Validates the integrity, balance, and richness of 'chunks.csv' produced by\n",
        "# the BYOAI Chapter Chunker (Listing 1). Reports stats, keyword coverage,\n",
        "# and short/long chunk distributions to guide downstream generation.\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from shutil import get_terminal_size\n",
        "\n",
        "# ===================== CONFIG =====================\n",
        "CHUNKS_CSV  = \"byoai_book_chunks.csv\"\n",
        "REQ_COLS    = {\"chapter\", \"chapter_title\", \"chunk_id\",\n",
        "               \"chunk_text\", \"subject_tags\"}\n",
        "SHORT_LIMIT = 50      # minimum acceptable word count per chunk\n",
        "LONG_LIMIT  = 220     # threshold for overly long windows\n",
        "# ==================================================\n",
        "\n",
        "# --- Load CSV and verify columns ---\n",
        "df = pd.read_csv(CHUNKS_CSV, encoding=\"utf-8-sig\")\n",
        "print(f\"Loaded {len(df):,} chunks from {CHUNKS_CSV}\")\n",
        "\n",
        "missing = REQ_COLS - set(df.columns)\n",
        "if missing:\n",
        "    raise SystemExit(f\"Missing columns: {sorted(missing)}\")\n",
        "if df.empty:\n",
        "    raise SystemExit(\"No rows found in chunks.csv\")\n",
        "\n",
        "# --- Basic metrics per chunk ---\n",
        "df[\"chunk_text\"] = df[\"chunk_text\"].astype(str)\n",
        "df[\"word_len\"]   = df[\"chunk_text\"].apply(lambda t: len(t.split()))\n",
        "df[\"sent_count\"] = df[\"chunk_text\"].apply(\n",
        "    lambda t: len(re.findall(r\"[.!?](?:\\s|$)\", t))\n",
        ")\n",
        "\n",
        "# --- Per-chapter summary ---\n",
        "summary = (\n",
        "    df.groupby([\"chapter\", \"chapter_title\"], as_index=False)\n",
        "      .agg(\n",
        "          chunks=(\"chunk_id\", \"count\"),\n",
        "          mean_words=(\"word_len\", \"mean\"),\n",
        "          min_words=(\"word_len\", \"min\"),\n",
        "          max_words=(\"word_len\", \"max\"),\n",
        "          mean_sents=(\"sent_count\", \"mean\"),\n",
        "      )\n",
        "      .round(2)\n",
        ")\n",
        "summary[\"share_pct\"] = (summary[\"chunks\"] / len(df) * 100).round(1)\n",
        "\n",
        "print(\"\\n=== Per-Chapter Chunk Stats ===\")\n",
        "term_width = get_terminal_size((120, 20)).columns\n",
        "print(summary.sort_values(\"chapter\")\n",
        "              .to_string(index=False, line_width=term_width))\n",
        "\n",
        "# --- Global summary ---\n",
        "print(\"\\n=== Global Summary ===\")\n",
        "print(f\"Mean words per chunk: {df['word_len'].mean():.1f}\")\n",
        "print(f\"Shortest chunk: {df['word_len'].min()} words\")\n",
        "print(f\"Longest chunk:  {df['word_len'].max()} words\")\n",
        "print(f\"Mean sentences per chunk: {df['sent_count'].mean():.2f}\")\n",
        "\n",
        "# --- Subject tag coverage ---\n",
        "def split_tags(s):\n",
        "    return [t.strip() for t in str(s).split(\",\") if t.strip()]\n",
        "\n",
        "tag_counts = Counter(tag for tags in df[\"subject_tags\"].map(split_tags)\n",
        "                     for tag in tags)\n",
        "\n",
        "print(\"\\n=== Subject Tag Coverage (top 20) ===\")\n",
        "for tag, count in tag_counts.most_common(20):\n",
        "    print(f\"{tag:<20} {count}\")\n",
        "\n",
        "# --- Keyword occurrence overview ---\n",
        "KEY_TERMS = [\"ai\", \"data\", \"model\", \"learning\",\n",
        "             \"open\", \"ethics\", \"voice\", \"agent\"]\n",
        "totals = Counter()\n",
        "for text in df[\"chunk_text\"]:\n",
        "    text_l = text.lower()\n",
        "    for term in KEY_TERMS:\n",
        "        totals[term] += text_l.count(term)\n",
        "\n",
        "print(\"\\n=== Keyword Occurrence Totals ===\")\n",
        "for term, count in sorted(totals.items(), key=lambda kv: -kv[1]):\n",
        "    print(f\"{term:<12} {count}\")\n",
        "\n",
        "# --- Flag short and long chunks ---\n",
        "short_df = df[df[\"word_len\"] < SHORT_LIMIT]\n",
        "long_df  = df[df[\"word_len\"] > LONG_LIMIT]\n",
        "\n",
        "print(f\"\\nChunks under {SHORT_LIMIT} words: {len(short_df)} \"\n",
        "      f\"({len(short_df)/len(df):.1%})\")\n",
        "print(f\"Chunks over {LONG_LIMIT} words: {len(long_df)} \"\n",
        "      f\"({len(long_df)/len(df):.1%})\")\n",
        "\n",
        "# --- Notebook-friendly preview ---\n",
        "def preview(frame, label):\n",
        "    if frame.empty:\n",
        "        return\n",
        "    cols = [\"chapter\",\"chapter_title\",\"chunk_id\",\"word_len\",\"chunk_text\"]\n",
        "    print(f\"\\nExample {label} chunks:\")\n",
        "    try:\n",
        "        from IPython.display import display\n",
        "        display(frame.sample(min(5, len(frame)))[cols])\n",
        "    except Exception:\n",
        "        print(frame.sample(min(3, len(frame)))[cols].to_string(index=False))\n",
        "\n",
        "preview(short_df, \"short\")\n",
        "preview(long_df, \"long\")"
      ],
      "metadata": {
        "id": "Xv8T3223nPp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2: BYOAI LIAR-Style Generator → `byoai_liar.csv`\n",
        "\n",
        "This generator transforms chapter chunks into concise, labeled statements\n",
        "using the OpenAI API. Each row in the output CSV contains a short claim,\n",
        "context phrase, and brief justification paired with a truthfulness label\n",
        "drawn from the classic *LIAR* dataset categories (`TRUE`, `mostly-true`,\n",
        "`half-true`, `barely-true`, `FALSE`, `pants-fire`). Labels are sampled using\n",
        "weighted probabilities to maintain a balanced overall mix.\n",
        "\n",
        "For every passage in `byoai_book_chunks.csv`, the script composes a compact\n",
        "prompt embedding the target label, chapter metadata, and subject tags. The\n",
        "model (`gpt-4o-mini`) returns a JSON response containing three fields:\n",
        "`statement`, `context`, and `label_reason`. Lightweight validators enforce\n",
        "length limits, remove meta references (like “chapter” or “book”), and ensure\n",
        "syntactic completeness. Deduplication guards prevent near-identical\n",
        "statements from repeating.\n",
        "\n",
        "Output fields include:\n",
        "\n",
        "*id, chunk_id, label, statement, context, label_reason,\n",
        "subject_tags, chapter, chapter_title*\n",
        "\n",
        "Together, these labeled micro-claims form a synthetic fact-checking corpus\n",
        "aligned to the book’s themes. The dataset can be used to train or evaluate\n",
        "classification models that reason about factual accuracy, context grounding,\n",
        "and evidence alignment—mirroring LIAR’s structure while drawing from\n",
        "BYOAI’s original chapter content.\n",
        "\n",
        "In short, this script lets AI **generate a dataset about itself** —\n",
        "a living demonstration of the book’s theme: *using AI to build your own AI.*\n",
        "\n",
        "**REQUIRES:**  \n",
        "`byoai_book_chunks.csv` (from Listings 1), an active OpenAI API key stored via  \n",
        "`google.colab.userdata`, and the `openai` + `pandas` libraries."
      ],
      "metadata": {
        "id": "palgEvuZJnpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === BYOAI LIAR-Style Generator (single-pass; OpenAI >= 1.0) ================\n",
        "# Output CSV schema:\n",
        "#   id,chunk_id,label,statement,context,label_reason,subject_tags,chapter,chapter_title\n",
        "#\n",
        "# Purpose:\n",
        "#   Generate concise, labeled statements with brief context and justification\n",
        "#   directly from chunks.csv using a single, tight GEN prompt per row.\n",
        "#\n",
        "# Requirements:\n",
        "#   pip install --quiet openai pandas\n",
        "#   from google.colab import userdata\n",
        "#   userdata.set('OPENAI_API_KEY', '...')\n",
        "# ============================================================================\n",
        "\n",
        "import os, csv, json, re, time, random, math, hashlib\n",
        "from dataclasses import dataclass, asdict\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# ===================== CONFIG =====================\n",
        "CHUNKS_CSV               = \"byoai_book_chunks.csv\"\n",
        "OUTPUT_CSV               = \"byoai_liar.csv\"\n",
        "\n",
        "# Sampling targets\n",
        "MAX_CHUNKS               = 20           # how many chunks to process\n",
        "N_PER_CHUNK_TARGET       = 2            # rows to generate per chunk\n",
        "RETRIES_PER_ROW          = 1            # retry attempts if checks fail\n",
        "\n",
        "# IDs and pacing\n",
        "BASE_ID                  = 100000\n",
        "RANDOM_SEED              = 42\n",
        "PAUSE_SEC                = 0.02         # gentle pacing\n",
        "\n",
        "# Labels & weights (global target mix)\n",
        "LABELS = [\"TRUE\", \"mostly-true\", \"half-true\", \"barely-true\", \"FALSE\", \"pants-fire\"]\n",
        "LABEL_WEIGHTS = [0.20, 0.18, 0.24, 0.18, 0.12, 0.08]\n",
        "\n",
        "# Chapter-tag mapping (fallback if subject_tags column not present)\n",
        "SUBJECT_TAGS = {\n",
        "     0: [\"open-source\", \"community\", \"ai\"],                 # Foreword\n",
        "     1: [\"ai\", \"open-source\", \"builder\"],                   # Introduction\n",
        "     2: [\"ai\", \"tool-chain\", \"notebooks\"],                  # Ch1 – Survival Kit\n",
        "     3: [\"data-prep\", \"feature-engineering\", \"rag\"],        # Ch2 – Prepping Data\n",
        "     4: [\"machine-learning\", \"classification\", \"evaluation\"],# Ch3 – Classical ML\n",
        "     5: [\"deep-learning\", \"frameworks\", \"tensors\"],         # Ch4 – Deep Learning\n",
        "     6: [\"neural-networks\", \"cnn\", \"transformers\"],         # Ch5 – Neuron Blocks\n",
        "     7: [\"generative-ai\", \"diffusion\", \"gans\"],             # Ch6 – Gen AI\n",
        "     8: [\"security\", \"red-team\", \"guardrails\"],             # Ch7 – Break/Secure\n",
        "     9: [\"media-forensics\", \"voice-cloning\", \"deepfake\"],   # Ch8 – Deepfake Def.\n",
        "    10: [\"mlops\", \"scaling\", \"deployment\"],                 # Ch9 – At Scale\n",
        "    11: [\"ethics\", \"governance\", \"privacy\"],                # Ch10 – Ethics/Gov\n",
        "    12: [\"agentic-ai\", \"planning\", \"tools\"],                # Ch11 – Agentic AI\n",
        "    13: [\"open-source\", \"community\", \"contribution\"],       # Ch12 – Commit\n",
        "}\n",
        "\n",
        "# Light validation bounds\n",
        "META_BAN     = re.compile(r\"\\b(chapter|this chapter|book|this book|this statement)\\b\", re.I)\n",
        "STMT_MAX_W   = 20\n",
        "CTX_MAX_W    = 12\n",
        "RSN_MIN_W    = 8\n",
        "RSN_MAX_W    = 28\n",
        "\n",
        "# Dedupe control\n",
        "DEDUP_HASH_LEN = 60       # portion of statement to hash\n",
        "# ==================================================\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# ----------------- Model configs (switchable) -----------------\n",
        "# Each entry declares the model name and any supported controls.\n",
        "# GPT-5 family introduces:\n",
        "#   - verbosity: \"low\" | \"medium\" | \"high\"\n",
        "#   - reasoning_effort: \"minimal\" | \"medium\" | \"high\"\n",
        "# NOTE: GPT-5 and GPT-5-mini do NOT support temperature, top_p,\n",
        "#       frequency_penalty, or presence_penalty parameters.\n",
        "# Docs: https://platform.openai.com/docs/guides/latest-model\n",
        "\n",
        "MODEL_CONFIGS = {\n",
        "    # Fast, inexpensive baseline (sampling controls allowed)\n",
        "    \"gpt-4o-mini\": {\n",
        "        \"model\": \"gpt-4o-mini\",\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.9,\n",
        "    },\n",
        "\n",
        "    # GPT-5-mini: newer control scheme, no sampling params\n",
        "    \"gpt-5-mini\": {\n",
        "        \"model\": \"gpt-5-mini\",\n",
        "        \"verbosity\": \"medium\",         # controls level of elaboration\n",
        "        \"reasoning_effort\": \"minimal\", # controls internal reasoning depth\n",
        "    },\n",
        "\n",
        "    # Full GPT-5: highest quality; supports new control params only\n",
        "    \"gpt-5\": {\n",
        "        \"model\": \"gpt-5\",\n",
        "        \"verbosity\": \"high\",           # more expansive responses\n",
        "        \"reasoning_effort\": \"medium\",  # balanced reasoning depth\n",
        "    },\n",
        "}\n",
        "\n",
        "# Pick which config is \"in play\" for this run:\n",
        "ACTIVE_MODEL_KEY = \"gpt-5-mini\"   # <-- change to \"gpt-5-mini\" or \"gpt-5\"\n",
        "CFG = MODEL_CONFIGS[ACTIVE_MODEL_KEY]\n",
        "\n",
        "# ----------------- Client init -----------------\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise SystemExit(\"Missing OPENAI_API_KEY in Colab userdata.\")\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# ----------------- Helper: build request params -----------------\n",
        "def build_chat_params(prompt: str, cfg: dict) -> dict:\n",
        "    \"\"\"Safely map model config to Chat Completions parameters.\"\"\"\n",
        "    params = {\n",
        "        \"model\": cfg[\"model\"],\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "    }\n",
        "\n",
        "    # Include only supported params\n",
        "    if \"temperature\" in cfg:\n",
        "        params[\"temperature\"] = cfg[\"temperature\"]\n",
        "    if \"top_p\" in cfg:\n",
        "        params[\"top_p\"] = cfg[\"top_p\"]\n",
        "    if \"verbosity\" in cfg:\n",
        "        params[\"verbosity\"] = cfg[\"verbosity\"]\n",
        "    if \"reasoning_effort\" in cfg:\n",
        "        params[\"reasoning_effort\"] = cfg[\"reasoning_effort\"]\n",
        "\n",
        "    return params\n",
        "\n",
        "# ============ LLM call ============\n",
        "def llm_generate_row(label, passage, chapter, chapter_title, tags_str, cfg=CFG):\n",
        "    \"\"\"Generate one labeled row using the active model configuration.\"\"\"\n",
        "    prompt = GEN_PROMPT.format(\n",
        "        target_label=label,\n",
        "        chapter=chapter,\n",
        "        chapter_title=chapter_title,\n",
        "        subject_tags=tags_str,\n",
        "        passage=passage,\n",
        "    )\n",
        "\n",
        "    params = build_chat_params(prompt, cfg)\n",
        "    # For structured output:\n",
        "    # params[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "    r = client.chat.completions.create(**params)\n",
        "    return safe_parse_json(r.choices[0].message.content)\n",
        "\n",
        "# ============ CSV model ============\n",
        "HEADERS = [\n",
        "    \"id\",\"chunk_id\",\"label\",\"statement\",\"context\",\"label_reason\",\n",
        "    \"subject_tags\",\"chapter\",\"chapter_title\"\n",
        "]\n",
        "\n",
        "@dataclass\n",
        "class Row:\n",
        "    id: int\n",
        "    chunk_id: int\n",
        "    label: str\n",
        "    statement: str\n",
        "    context: str\n",
        "    label_reason: str\n",
        "    subject_tags: str\n",
        "    chapter: int\n",
        "    chapter_title: str\n",
        "\n",
        "def write_csv(rows, path):\n",
        "    with open(path, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow(HEADERS)\n",
        "        for r in rows:\n",
        "            w.writerow([asdict(r)[h] for h in HEADERS])\n",
        "\n",
        "# ============ Utilities ============\n",
        "def wc(s: str) -> int:\n",
        "    return len(str(s).strip().split())\n",
        "\n",
        "def ensure_sentence(s: str) -> str:\n",
        "    s = str(s or \"\").strip()\n",
        "    if not s:\n",
        "        return s\n",
        "    if s[0].isalpha() and not s[0].isupper():\n",
        "        s = s[0].upper() + s[1:]\n",
        "    if s[-1] not in \".!?\":\n",
        "        s += \".\"\n",
        "    return s\n",
        "\n",
        "def ensure_phrase(s: str) -> str:\n",
        "    # keep concise phrase/clause; period not required\n",
        "    return str(s or \"\").strip().rstrip(\".\")\n",
        "\n",
        "def safe_parse_json(txt: str):\n",
        "    txt = str(txt).strip()\n",
        "    i, j = txt.find(\"{\"), txt.rfind(\"}\") + 1\n",
        "    snippet = txt[i:j] if i != -1 and j > 0 else txt\n",
        "    try:\n",
        "        return json.loads(snippet)\n",
        "    except Exception:\n",
        "        pairs = re.findall(r'\"(\\w+)\":\\s*\"([^\"]+)\"', snippet)\n",
        "        return {k: v for k, v in pairs}\n",
        "\n",
        "def short_hash(statement: str) -> str:\n",
        "    base = statement[:DEDUP_HASH_LEN].strip().lower()\n",
        "    return hashlib.md5(base.encode(\"utf-8\")).hexdigest()[:10]\n",
        "\n",
        "def ok_row(stmt: str, ctx: str, rsn: str) -> bool:\n",
        "    if not stmt or not ctx or not rsn:\n",
        "        return False\n",
        "    if META_BAN.search(stmt) or META_BAN.search(ctx) or META_BAN.search(rsn):\n",
        "        return False\n",
        "    if wc(stmt) > STMT_MAX_W:\n",
        "        return False\n",
        "    if wc(ctx) > CTX_MAX_W:\n",
        "        return False\n",
        "    n = wc(rsn)\n",
        "    if n < RSN_MIN_W or n > RSN_MAX_W:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# ============ Prompt ============\n",
        "GEN_PROMPT = r\"\"\"Return ONLY compact JSON with keys:\n",
        "statement, context, label_reason.\n",
        "\n",
        "Target label: {target_label}\n",
        "Chapter: {chapter} – {chapter_title}\n",
        "Subject tags: {subject_tags}\n",
        "\n",
        "Rules:\n",
        "- statement: ONE concise, third-person, declarative sentence (≤20 words) about the passage topic.\n",
        "  It MUST be written to MATCH the Target label.\n",
        "- Label guidance:\n",
        "  TRUE = directly supported by passage;\n",
        "  mostly-true = broadly supported, minor caveat omitted;\n",
        "  half-true = mix of correct and incorrect specifics;\n",
        "  barely-true = largely unsupported, notable error or overreach;\n",
        "  FALSE = contradicts passage facts;\n",
        "  pants-fire = extreme or implausible contradiction.\n",
        "- context: ONE concise noun phrase or brief clause (≤12 words) showing where/how the claim fits.\n",
        "  Include at least one specific term from the passage or tags (e.g., dataset, model, metric, tool, or concept).\n",
        "- label_reason: 8–18 words explaining WHY the label fits, citing evidence, omissions, or contradictions.\n",
        "  For FALSE or pants-fire, name the detail or assumption being contradicted.\n",
        "  Avoid starting with phrases like “The passage…”, “The claim…”, or similar. Instead, describe the relationship or mismatch directly (e.g., “RAG supports complex use cases” or\n",
        "  “Feature design improves accuracy in most cases.”).\n",
        "\n",
        "Style:\n",
        "- Write label_reason in a natural explanatory tone, not as a citation or report.\n",
        "- Use varied openings and sentence structures across rows.\n",
        "- Prefer concrete mechanisms, datasets, or named tools over generalities.\n",
        "- Maintain a factual, neutral tone; no meta language or self-reference.\n",
        "\n",
        "Avoid: “chapter”, “book”, “this statement”, or similar meta words.\n",
        "\n",
        "Passage:\n",
        "<<<\n",
        "{passage}\n",
        ">>>\n",
        "\n",
        "JSON:\n",
        "\"\"\"\n",
        "\n",
        "# ============ Label selection with drift guard ============\n",
        "def pick_label(counts: Counter, total_made: int, total_target: int) -> str:\n",
        "    # sample by weights; if one label is ahead of its expected share by 25%, resample once\n",
        "    lbl = random.choices(LABELS, weights=LABEL_WEIGHTS, k=1)[0]\n",
        "    if total_made == 0:\n",
        "        return lbl\n",
        "    # expected share so far (based on target totals)\n",
        "    expected = {L: LABEL_WEIGHTS[i] * (total_target) for i, L in enumerate(LABELS)}\n",
        "    ahead = counts[lbl] > 1.25 * (expected[lbl] * (total_made / max(1, total_target)))\n",
        "    if ahead:\n",
        "        lbl2 = random.choices(LABELS, weights=LABEL_WEIGHTS, k=1)[0]\n",
        "        return lbl2\n",
        "    return lbl\n",
        "\n",
        "# ============ Main ============\n",
        "def main():\n",
        "    df = pd.read_csv(CHUNKS_CSV, encoding=\"utf-8-sig\")\n",
        "\n",
        "    for col in (\"chapter\", \"chapter_title\", \"chunk_id\", \"chunk_text\"):\n",
        "        if col not in df.columns:\n",
        "            raise SystemExit(f\"chunks.csv missing required column: {col}\")\n",
        "\n",
        "    # choose which chunks to process (simple shuffle for coverage)\n",
        "    idxs = list(df.index)\n",
        "    random.shuffle(idxs)\n",
        "    idxs = idxs[:min(MAX_CHUNKS, len(idxs))]\n",
        "\n",
        "    total_target = len(idxs) * N_PER_CHUNK_TARGET\n",
        "\n",
        "    next_id = BASE_ID\n",
        "    rows = []\n",
        "    seen_stmt = set()\n",
        "    label_counts = Counter()\n",
        "\n",
        "    print(f\"Processing {len(idxs)} chunks; target rows ≈ {total_target}\")\n",
        "\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        row = df.loc[idx]\n",
        "        passage = str(row[\"chunk_text\"]).strip()\n",
        "        if not passage:\n",
        "            continue\n",
        "\n",
        "        chapter = int(row[\"chapter\"])\n",
        "        chapter_title = str(row[\"chapter_title\"])\n",
        "        chunk_id = int(row[\"chunk_id\"])\n",
        "\n",
        "        # prefer subject_tags column; fallback to map\n",
        "        if \"subject_tags\" in df.columns and str(row[\"subject_tags\"]).strip():\n",
        "            tags_str = str(row[\"subject_tags\"])\n",
        "        else:\n",
        "            tags_str = \", \".join(SUBJECT_TAGS.get(chapter, []))\n",
        "\n",
        "        # per-chunk generation\n",
        "        for _ in range(N_PER_CHUNK_TARGET):\n",
        "            attempts = 0\n",
        "            while attempts <= RETRIES_PER_ROW:\n",
        "                attempts += 1\n",
        "\n",
        "                label = pick_label(label_counts, len(rows), total_target)\n",
        "                out = llm_generate_row(label, passage, chapter, chapter_title, tags_str)\n",
        "\n",
        "                stmt = ensure_sentence(out.get(\"statement\", \"\"))\n",
        "                ctx  = ensure_phrase(out.get(\"context\", \"\"))\n",
        "                rsn  = ensure_sentence(out.get(\"label_reason\", \"\"))\n",
        "\n",
        "                if not ok_row(stmt, ctx, rsn):\n",
        "                    if attempts <= RETRIES_PER_ROW:\n",
        "                        continue\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                key = short_hash(stmt)\n",
        "                if key in seen_stmt:\n",
        "                    if attempts <= RETRIES_PER_ROW:\n",
        "                        continue\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                # accept\n",
        "                seen_stmt.add(key)\n",
        "                label_counts[label] += 1\n",
        "                rows.append(\n",
        "                    Row(\n",
        "                        id=next_id,\n",
        "                        chunk_id=chunk_id,\n",
        "                        label=label,\n",
        "                        statement=stmt,\n",
        "                        context=ctx,\n",
        "                        label_reason=rsn,\n",
        "                        subject_tags=tags_str,\n",
        "                        chapter=chapter,\n",
        "                        chapter_title=chapter_title,\n",
        "                    )\n",
        "                )\n",
        "                next_id += 1\n",
        "                time.sleep(PAUSE_SEC)\n",
        "                break  # next target row for this chunk\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print(f\"[progress] chunks {i}/{len(idxs)} | rows {len(rows)}\")\n",
        "\n",
        "    # write output\n",
        "    write_csv(rows, OUTPUT_CSV)\n",
        "\n",
        "    # summary\n",
        "    print(f\"\\nWrote {len(rows)} rows to {OUTPUT_CSV}\")\n",
        "    print(\"Label counts:\", label_counts)\n",
        "    print(\"Chapters:\", Counter(r.chapter for r in rows))\n",
        "    print(\"\\nSample:\")\n",
        "    for s in rows[:10]:\n",
        "        print(f\"{s.id} [{s.label}] {s.statement} :: {s.context} | tags={s.subject_tags} | ch{s.chapter} – {s.chapter_title}\")\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "3YAKR_PzNuvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2A: BYOAI LIAR Coherence & Label Audit\n",
        "\n",
        "This script performs a lightweight quality audit of `byoai_liar.csv` using\n",
        "GPT-based evaluation. It randomly samples dataset rows and asks the model\n",
        "to rate coherence across four dimensions—**chapter_fit**, **label_fit**,\n",
        "**context_quality**, and **reason_quality**—each scored from 0 to 1. The\n",
        "model also assigns an overall rating (`high`, `medium`, or `low`) and, when\n",
        "needed, suggests a brief fix for weak or inconsistent entries.\n",
        "\n",
        "Each row is evaluated through a structured JSON prompt that ensures\n",
        "consistent scoring and concise feedback. Results are printed with detailed\n",
        "row-level metrics and a short summary of average fit scores and overall\n",
        "distribution. This audit helps verify whether generated statements align\n",
        "with their chapters, whether labels are logically justified, and whether\n",
        "context and reasoning remain distinct and informative—providing an early,\n",
        "LLM-assisted sanity check on the coherence and factual structure of the\n",
        "synthetic BYOAI LIAR dataset."
      ],
      "metadata": {
        "id": "WZKCLyhTutmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === BYOAI LIAR Coherence & Label Audit (LLM-assisted, randomized) ============\n",
        "# Randomly samples rows from 'byoai_liar.csv' and asks gpt-4o-mini to rate:\n",
        "# - chapter_fit: does the statement/context align with the chapter’s theme?\n",
        "# - label_fit: is the label logically consistent with the statement and justification?\n",
        "# - reason_quality: does label_reason explain the label clearly and concisely?\n",
        "# - context_quality: does context add distinct, relevant detail (not restate statement)?\n",
        "# Also returns overall rating and a suggested fix when coherence is low.\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# -------------------- CONFIG --------------------\n",
        "CSV_PATH       = \"byoai_liar.csv\"\n",
        "SAMPLE_CHECKS  = 10         # how many random rows to audit\n",
        "RANDOM_SEED    = 42\n",
        "SLEEP_SEC      = 0.05\n",
        "# ------------------------------------------------\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(CSV_PATH, encoding=\"utf-8-sig\")\n",
        "required = [\n",
        "    \"id\",\"label\",\"statement\",\"context\",\"label_reason\",\n",
        "    \"subject_tags\",\"chapter\",\"chapter_title\"\n",
        "]\n",
        "missing = [c for c in required if c not in df.columns]\n",
        "if missing:\n",
        "    raise SystemExit(f\"Missing columns: {missing}\")\n",
        "\n",
        "# Sample subset\n",
        "sample_df = (\n",
        "    df.sample(SAMPLE_CHECKS, random_state=RANDOM_SEED)\n",
        "    if SAMPLE_CHECKS < len(df)\n",
        "    else df.copy()\n",
        ")\n",
        "\n",
        "# Prompt setup\n",
        "SYSTEM = (\n",
        "    \"You are a concise dataset auditor. \"\n",
        "    \"Return ONLY compact JSON. No preface, no commentary.\"\n",
        ")\n",
        "USER_TMPL = \"\"\"Evaluate the coherence and labeling of this BYOAI dataset row.\n",
        "\n",
        "Return JSON with keys EXACTLY:\n",
        "- chapter_fit: float in [0,1]\n",
        "- label_fit: float in [0,1]\n",
        "- context_quality: float in [0,1]\n",
        "- reason_quality: float in [0,1]\n",
        "- overall: one of [\"high\",\"medium\",\"low\"]\n",
        "- comments: short string (≤25 words)\n",
        "- suggest_fix: short rewrite (≤28 words) improving label_reason or context\n",
        "\n",
        "Guidelines:\n",
        "- chapter_fit: statement+context align with the chapter theme ({chapter_title})?\n",
        "- label_fit: does the label logically match the statement and label_reason?\n",
        "- context_quality: does the context add value without repeating the statement?\n",
        "- reason_quality: is label_reason clear, concise, and directly supportive of the label?\n",
        "- overall: rate overall coherence (high=strong fit, medium=minor flaws, low=clear mismatch)\n",
        "\n",
        "Row:\n",
        "chapter_title: \"{chapter_title}\"\n",
        "chapter: {chapter}\n",
        "label: \"{label}\"\n",
        "subject_tags: \"{subject_tags}\"\n",
        "statement: \"{statement}\"\n",
        "context: \"{context}\"\n",
        "label_reason: \"{label_reason}\"\n",
        "\n",
        "Return JSON:\"\"\"\n",
        "\n",
        "# === Utility functions ===\n",
        "def safe_parse_json(txt: str) -> dict:\n",
        "    txt = (txt or \"\").strip()\n",
        "    i, j = txt.find(\"{\"), txt.rfind(\"}\") + 1\n",
        "    snippet = txt[i:j] if i != -1 and j > 0 else txt\n",
        "    try:\n",
        "        return json.loads(snippet)\n",
        "    except Exception:\n",
        "        pairs = re.findall(r'\"(\\w+)\":\\s*\"([^\"]+)\"', snippet)\n",
        "        return {k: v for k, v in pairs}\n",
        "\n",
        "def clamp01(x):\n",
        "    try:\n",
        "        v = float(x)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "    return max(0.0, min(1.0, v))\n",
        "\n",
        "def audit_row(row):\n",
        "    prompt = USER_TMPL.format(\n",
        "        chapter=row[\"chapter\"],\n",
        "        chapter_title=str(row[\"chapter_title\"]),\n",
        "        label=str(row[\"label\"]),\n",
        "        subject_tags=str(row[\"subject_tags\"]),\n",
        "        statement=str(row[\"statement\"]),\n",
        "        context=str(row[\"context\"]),\n",
        "        label_reason=str(row[\"label_reason\"]),\n",
        "    )\n",
        "    r = client.chat.completions.create(\n",
        "        model=\"gpt-5\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        verbosity=\"high\",          # controls how detailed the answer is\n",
        "        reasoning_effort=\"medium\", # controls how much reasoning the model performs\n",
        "    )\n",
        "    out = safe_parse_json(r.choices[0].message.content)\n",
        "    # normalize\n",
        "    cfit = clamp01(out.get(\"chapter_fit\", 0))\n",
        "    lfit = clamp01(out.get(\"label_fit\", 0))\n",
        "    cq   = clamp01(out.get(\"context_quality\", 0))\n",
        "    rq   = clamp01(out.get(\"reason_quality\", 0))\n",
        "    overall = str(out.get(\"overall\",\"\")).lower().strip()\n",
        "    if overall not in {\"high\",\"medium\",\"low\"}:\n",
        "        avg = (cfit + lfit + cq + rq) / 4.0\n",
        "        overall = \"high\" if avg >= 0.75 else \"medium\" if avg >= 0.5 else \"low\"\n",
        "    comments = str(out.get(\"comments\",\"\")).strip()\n",
        "    fix = str(out.get(\"suggest_fix\",\"\")).strip()\n",
        "    return {\n",
        "        \"chapter_fit\": cfit,\n",
        "        \"label_fit\": lfit,\n",
        "        \"context_quality\": cq,\n",
        "        \"reason_quality\": rq,\n",
        "        \"overall\": overall,\n",
        "        \"comments\": comments,\n",
        "        \"suggest_fix\": fix,\n",
        "    }\n",
        "\n",
        "# === Run audit ===\n",
        "results = []\n",
        "print(f\"Auditing {len(sample_df)} random rows from {len(df)} total...\\n\")\n",
        "for i, (_, row) in enumerate(sample_df.iterrows(), 1):\n",
        "    res = audit_row(row)\n",
        "    results.append((row, res))\n",
        "    print(f\"[{i:02d}] id={row['id']} | ch{row['chapter']} \"\n",
        "          f\"({row['chapter_title']}) | label={row['label']}\")\n",
        "    print(f\"     S: {row['statement']}\")\n",
        "    print(f\"     C: {row['context']}\")\n",
        "    print(f\"     R: {row['label_reason']}\")\n",
        "    print(f\"     -> fit(ch,lab,ctx,rsn)=({res['chapter_fit']:.2f},\"\n",
        "          f\"{res['label_fit']:.2f},{res['context_quality']:.2f},\"\n",
        "          f\"{res['reason_quality']:.2f}) | overall={res['overall']}\")\n",
        "    if res[\"overall\"] == \"low\" or res[\"reason_quality\"] < 0.6:\n",
        "        print(f\"     fix: {res['suggest_fix'] or '(no suggestion)'}\")\n",
        "    if res[\"comments\"]:\n",
        "        print(f\"     note: {res['comments']}\")\n",
        "    print()\n",
        "    time.sleep(SLEEP_SEC)\n",
        "\n",
        "# === Aggregates ===\n",
        "if results:\n",
        "    ch  = sum(r[1][\"chapter_fit\"] for r in results) / len(results)\n",
        "    lf  = sum(r[1][\"label_fit\"] for r in results) / len(results)\n",
        "    cq  = sum(r[1][\"context_quality\"] for r in results) / len(results)\n",
        "    rq  = sum(r[1][\"reason_quality\"] for r in results) / len(results)\n",
        "    overall_counts = {}\n",
        "    for _, r in results:\n",
        "        overall_counts[r[\"overall\"]] = overall_counts.get(r[\"overall\"], 0) + 1\n",
        "\n",
        "    print(\"=== Aggregate (sample) ===\")\n",
        "    print(f\"chapter_fit avg:     {ch:.3f}\")\n",
        "    print(f\"label_fit avg:       {lf:.3f}\")\n",
        "    print(f\"context_quality avg: {cq:.3f}\")\n",
        "    print(f\"reason_quality avg:  {rq:.3f}\")\n",
        "    print(\"overall ratings:     \", overall_counts)\n",
        "else:\n",
        "    print(\"No results to summarize.\")"
      ],
      "metadata": {
        "id": "V18HR6u3WGfC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}