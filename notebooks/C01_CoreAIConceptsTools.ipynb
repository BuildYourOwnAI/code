{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7MCZqg-engg"
   },
   "source": [
    "# Chapter 1 - Core AI Concepts and Tools\n",
    "This notebook sets up your Python environment and explores core AI concepts such as machine learning, deep learning, and generative models. It demonstrates how to integrate AI into applications while maintaining ethical standards using popular frameworks like Scikit-learn, Hugging Face, and Stable Diffusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwV8U_hffGnr"
   },
   "source": [
    "### Listing 1-1: Installing the Python Starter Set\n",
    "Let's kick things off by installing your 'survival kit' of Python libraries using **pip**, so you're fully equipped with the tools you'll need for development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLzXuunCfTYK"
   },
   "outputs": [],
   "source": [
    "# Install fundamental Python libraries for data manipulation, visualization, and AI models\n",
    "%pip install numpy pandas matplotlib scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejg1G5rLftA_"
   },
   "source": [
    "### Block 2: Importing Core Libraries\n",
    "In this block, we import core libraries like **NumPy, Pandas, and Matplotlib**, verifying that our setup is configured correctly for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06wCLK-Pf5pk",
    "outputId": "c9ac835e-fe69-4d5d-841b-91163f4ffbd8"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "# Confirm that libraries are set up correctly\n",
    "print('Libraries installed, imported, and ready to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsEJSvLhgCZX"
   },
   "source": [
    "### Block 3: Cleansing a Dataset\n",
    "We’ll cleanse a dataset of superhero attributes, handling missing values and dropping incomplete rows, ensuring it’s ready for analysis or modeling using **Pandas' Dataframe**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbuOR5mUgNvc"
   },
   "outputs": [],
   "source": [
    "# Sample dataset of fictional superheroes\n",
    "data = {'Name': ['Captain Valor', 'Mighty Max', 'Lady Storm', None],\n",
    "        'Age': [35, 29, 28, None],\n",
    "        'Power_Level': [95, 89, 93, 88]}\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original DataFrame\n",
    "print('Original DataFrame:', df, sep='\\n')\n",
    "\n",
    "# Fill missing values in the 'Age' column with the mean age\n",
    "df = df.assign(Age=df['Age'].fillna(df['Age'].mean()))\n",
    "\n",
    "# Remove rows where the 'Name' field is missing\n",
    "df = df.dropna(subset=['Name'])\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print('\\nCleaned DataFrame:', df, sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQCPCsiMgUnr"
   },
   "source": [
    "### Block 4: Comparing Data with Element-Wise Operations\n",
    "We'll use **NumPy** for element-wise operations, comparing superhero abilities and challenges to determine suitability based on strength and speed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ps5BY3QliNZV",
    "outputId": "c346aa57-c34b-483e-8113-f8e78ac61511"
   },
   "outputs": [],
   "source": [
    "# Superhero power matrix (rows: heroes, columns: strength, speed)\n",
    "# Hero A (Speedster), Hero B (Strongman)\n",
    "heroes = np.array([[90, 75],  # High speed, moderate strength\n",
    "                   [60, 95]]) # Low speed, high strength\n",
    "\n",
    "# Object challenge matrix (rows: strength, speed requirements)\n",
    "# Object 1: Train, Object 2: Speeding Bullet\n",
    "objects = np.array([[80, 50],  # High strength, moderate speed required\n",
    "                    [40, 100]])# Low strength, very high speed required\n",
    "\n",
    "# Element-wise division to compare powers to challenges\n",
    "result = heroes / objects\n",
    "\n",
    "# Print the power matrices and the result\n",
    "print(\"Hero Power Levels:\\n\", heroes)\n",
    "print(\"\\nObject Challenges:\\n\", objects)\n",
    "print(\"\\nHero-to-Object Power Ratio (Division):\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgYVlLifiT9i"
   },
   "source": [
    "### **Block 5: Calculating dot product and Cosine Similarity**\n",
    "We use the **NumPy** dot product to calculate the similarity between two superheroes' attributes (strength, speed, intelligence). We then compute the cosine similarity to interpret how closely aligned their power profiles are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGLUzH_didLT"
   },
   "outputs": [],
   "source": [
    "# Create two superhero power vectors (strength, speed, intelligence)\n",
    "hero_1 = np.array([85, 90, 75])  # Hero A: strong, fast, smart\n",
    "hero_2 = np.array([80, 85, 70])  # Hero B: slightly less in all areas\n",
    "\n",
    "# Calculate the dot product to compare their power profiles\n",
    "dot_product = np.dot(hero_1, hero_2)\n",
    "\n",
    "# Calculate the magnitude (norm) of each vector\n",
    "magnitude_hero_1 = np.linalg.norm(hero_1)\n",
    "magnitude_hero_2 = np.linalg.norm(hero_2)\n",
    "\n",
    "# Calculate the cosine similarity\n",
    "cosine_similarity = dot_product / (magnitude_hero_1 * magnitude_hero_2)\n",
    "\n",
    "# Print the power profiles, dot product, and cosine similarity\n",
    "print(\"Hero A Power Profile:\", hero_1)\n",
    "print(\"Hero B Power Profile:\", hero_2)\n",
    "print(\"\\nDot Product (Similarity):\", dot_product)\n",
    "print(\"\\nCosine Similarity (Range: -1 to 1):\", cosine_similarity)\n",
    "\n",
    "# Interpret the cosine similarity\n",
    "if cosine_similarity > 0.9:\n",
    "    similarity_text = \"The heroes have very similar power profiles.\"\n",
    "elif cosine_similarity > 0.7:\n",
    "    similarity_text = \"The heroes have somewhat similar power profiles.\"\n",
    "else:\n",
    "    similarity_text = \"The heroes have quite different power profiles.\"\n",
    "\n",
    "print(\"\\nInterpretation:\", similarity_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PFMHCM5ih5F"
   },
   "source": [
    "### Block 6: Optimizing Attributes with Gradients\n",
    "We use **NumPy** to calculate the gradient between current and target attributes, showing the amount and direction of improvement needed for superheroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MJDvaiQisgD"
   },
   "outputs": [],
   "source": [
    "# Current and target attribute levels (e.g., strength, speed)\n",
    "current_values = np.array([60, 80])\n",
    "target_values = np.array([90, 100])\n",
    "\n",
    "# Calculate the gradient showing required improvement to meet targets\n",
    "gradient = target_values - current_values\n",
    "\n",
    "# Display current levels, targets, and gradient values\n",
    "print('Current Attributes (strength, speed):', current_values)\n",
    "print('Target Attributes (strength, speed):', target_values)\n",
    "print('\\nGradient (Improvement Needed):', gradient)\n",
    "\n",
    "# Explanation of output\n",
    "if np.all(gradient >= 0):\n",
    "    explanation = \"The gradient values indicate how much the superhero needs to improve in each attribute to reach the target levels.\"\n",
    "else:\n",
    "    explanation = \"Some attributes may exceed targets, so not all need adjustment.\"\n",
    "\n",
    "print(\"\\nExplanation:\", explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tam31MdOixTy"
   },
   "source": [
    "### Block 7: Estimating Success Probability\n",
    "We use **NumPy** to compare superhero attributes against thresholds, calculating the probability of success through element-wise operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnlWHsBMi7yB"
   },
   "outputs": [],
   "source": [
    "# Define the actual values (e.g., strength and speed)\n",
    "actual_values = np.array([80, 90])\n",
    "\n",
    "# Define the threshold required for success\n",
    "required_thresholds = np.array([70, 85])\n",
    "\n",
    "# Calculate success probability (actual values divided by required thresholds)\n",
    "probability_of_success = actual_values / required_thresholds\n",
    "\n",
    "# Print the actual values, thresholds, and calculated probabilities\n",
    "print('Actual Values (strength, speed):', actual_values)\n",
    "print('Required Thresholds (strength, speed required):', required_thresholds)\n",
    "print('\\nProbability of Success:', probability_of_success)\n",
    "\n",
    "# Explanation of output\n",
    "if np.all(probability_of_success >= 1):\n",
    "    explanation = \"Values exceed thresholds, showing high success probability.\"\n",
    "else:\n",
    "    explanation = \"Values are below the thresholds, indicating a lower probability of success.\"\n",
    "\n",
    "print(\"\\nExplanation:\", explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yP9EeRnRjFIP"
   },
   "source": [
    "### Block 8: Predicting Revenue Using Linear Regression\n",
    "This block uses machine learning **Scikit-learn** to predict superhero movie box office revenue based on production budgets, demonstrating **linear regression** for simple financial forecasting. We’ll also predict the box office revenue if the budget is $400 million.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izlIleHyjOg5"
   },
   "outputs": [],
   "source": [
    "# Import Linear Regression package from Scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Example data: production budgets (input) and box office revenue (output)\n",
    "budgets = np.array([100, 150, 200, 250, 300]).reshape(-1, 1)  # in millions\n",
    "box_office = np.array([300, 400, 600, 750, 900])  # in millions\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(budgets, box_office)\n",
    "\n",
    "# Predict box office revenue based on production budgets\n",
    "predicted_revenue = model.predict(budgets)\n",
    "\n",
    "# Predict box office revenue for a $400 million budget\n",
    "new_budget = np.array([[400]])  # in millions\n",
    "predicted_new_revenue = model.predict(new_budget)\n",
    "\n",
    "# Print the slope (m), intercept (b), and the prediction for $400 million\n",
    "print(f\"Slope (m): {model.coef_[0]}\")\n",
    "print(f\"Intercept (b): {model.intercept_}\")\n",
    "print(f\"Predicted Box Office Revenue for $400 million budget: {predicted_new_revenue[0]:.2f} million\")\n",
    "\n",
    "# Plot the data points and regression line\n",
    "plt.scatter(budgets, box_office, color='blue', label='Actual Revenue')\n",
    "plt.plot(budgets, predicted_revenue, color='red', label='Predicted Revenue')\n",
    "plt.scatter(new_budget, predicted_new_revenue, color='green', marker='x', s=100, label='Prediction for $400M')\n",
    "plt.xlabel('Production Budget (in millions)')\n",
    "plt.ylabel('Box Office Revenue (in millions)')\n",
    "plt.title('Linear Regression: Box Office Revenue vs Production Budget')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4HkIlTCjUI1"
   },
   "source": [
    "### Block 9: Classifying Emails Using Naive Bayes\n",
    "Trains a model using Scikit-learn's **Naive Bayes** classifier to determine if superhero-themed emails are **spam or not spam**, demonstrating **text classification** and natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMOixXuujdXl"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Sample dataset: 5 spam and 5 non-spam emails\n",
    "emails = [\n",
    "    'Unlock Unlimited Superpowers Today!',\n",
    "    'Claim Your Free Kryptonite—Limited Offer!',\n",
    "    'Win a Year’s Supply of Superhero Gear!',\n",
    "    'Congratulations! You’ve Been Selected as the Next Avenger!',\n",
    "    'Free Batmobile Test Drive!',\n",
    "    'Quarterly sales report is due next Monday',\n",
    "    'Please review the attached budget proposal for next quarter',\n",
    "    'Reminder: Team meeting on Thursday at 9 AM',\n",
    "    'Your invoice for services rendered is attached',\n",
    "    'Client feedback report from last week’s presentation'\n",
    "]\n",
    "\n",
    "# Labels for spam (1) and non-spam (0)\n",
    "labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "# Convert emails to a bag-of-words representation using bi-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "X = vectorizer.fit_transform(emails)\n",
    "\n",
    "# Create and train the Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X, labels)\n",
    "\n",
    "# Test the model with new emails\n",
    "new_emails = [\n",
    "    'Congratulations, You’ve Been Chosen as the Next Avenger!',\n",
    "    'Final review of training schedule',\n",
    "    'Free ticket to the Superhero Conference—Register now!',\n",
    "    'Please review the attached project plan for the upcoming quarter'\n",
    "]\n",
    "X_new = vectorizer.transform(new_emails)\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "# Display predictions for each email\n",
    "for email, prediction in zip(new_emails, predictions):\n",
    "    print(f\"Email: '{email}' is {'spam' if prediction == 1 else 'not spam'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Li1FR4Njjrk"
   },
   "source": [
    "### Block 10: Image Classification with a Pre-Trained Model\n",
    "We'll use a pre-trained model from **Hugging Face** to classify an image, demonstrating **deep learning** for image classification, in this case a superhero comic book image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWcTRE3QkA7E"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load an image for classification from a URL\n",
    "# The image is of a superhero comic book.\n",
    "url = 'https://opensourceai-book.github.io/code/images/C01-Opensource-ComicBook.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Load a pre-trained image classification model from Hugging Face\n",
    "classifier = pipeline('image-classification')\n",
    "\n",
    "# Classify the image and retrieve predictions\n",
    "predictions = classifier(image)\n",
    "\n",
    "# Print the top prediction with its confidence score\n",
    "print(f\"Top Prediction: {predictions[0]['label']} with a score of {predictions[0]['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2VnseplkOQO"
   },
   "source": [
    "### Block 11: Installing Libraries for Image Generation\n",
    "To generate images (GenAI) using **Stable Diffusion**, we first install necessary libraries, including diffusers, transformers, and torch (**pytorch**), which enable model operation and image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auYaHf8PkTAd"
   },
   "outputs": [],
   "source": [
    "# Install required libraries for Stable Diffusion\n",
    "%pip install diffusers transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RN38p5RwkWu5"
   },
   "source": [
    "### Block 12: Generating Images Using Stable Diffusion\n",
    "We will generate a comic-style image using a pre-trained Stable Diffusion model. This exercise involves using **GPU** acceleration for faster image generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxVQLuuZkhIV"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for image generation\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Check if GPU is available and set precision accordingly\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Load the Stable Diffusion model with the appropriate settings\n",
    "pipe = StableDiffusionPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', torch_dtype=torch_dtype).to(device)\n",
    "\n",
    "# Define a text prompt for generating an image\n",
    "prompt = 'a family-friendly comic-style supervillain using colors; primary red, bright blue, sunny yellow, black and white'\n",
    "\n",
    "# Generate the image using the pipeline with reduced inference steps for speed\n",
    "image = pipe(prompt, num_inference_steps=20).images[0]\n",
    "\n",
    "# Display the generated image\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CShNvcHaknYY"
   },
   "source": [
    "### Block 13: Setting Up Hugging Face Access Token\n",
    "We configure our environment with a **Hugging Face** access token. This is necessary for programmatic access to models and datasets available on **Hugging Face Hub**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBUoYzBMk2GF"
   },
   "outputs": [],
   "source": [
    "# Import os library for environment variable manipulation\n",
    "import os\n",
    "\n",
    "# Set your Hugging Face Hub Access token (replace 'your_token_here' with your actual token)\n",
    "os.environ['HUGGINGFACEHUB_ACCESS_TOKEN'] = 'your_token_here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HoDuNiRk7AN"
   },
   "source": [
    "### Block 14: Installing Required Packages for Hugging Face and LangChain\n",
    "In this block, we install the necessary packages for using **Hugging Face Hub and LangChain**, which include community modules and tools for building language model applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAmSCj7TlEOA"
   },
   "outputs": [],
   "source": [
    "# Install required packages for Hugging Face and LangChain usage\n",
    "%pip install huggingface_hub langchain langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C75j_oSZlIlo"
   },
   "source": [
    "### Block 15: Building a Simple Q&A Chatbot Using LangChain\n",
    "We will set up a basic **Q&A chatbot** using **LangChain** and a small language model from Hugging Face. This demonstrates chaining models and using templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqRhAU5ulYHv"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for building a LangChain chatbot\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "# Initialize a small language model from Hugging Face\n",
    "llm = HuggingFaceHub(repo_id='mistralai/Mistral-Small-Instruct-2409', model_kwargs={'temperature': 0.1})\n",
    "\n",
    "# Define a prompt template for the chatbot\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Please respond in {language} in 20 words or less.'),\n",
    "    ('human', '{input}')\n",
    "])\n",
    "\n",
    "# Define the chatbot chain where the prompt is sent to the language model\n",
    "chain = prompt | llm\n",
    "\n",
    "# Invoke the chatbot with a sample input\n",
    "response = chain.invoke({'input': 'Who is the tallest superhero?', 'language': 'English'})\n",
    "\n",
    "# Print the chatbot's response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4u55E1glarq"
   },
   "source": [
    "### Block 16: Dynamic Agent Task Handling with LangChain\n",
    "Create a LangChain agent that dynamically interacts with tools for tasks like web searching and performing calculations. This showcases the flexibility of **LangChain’s agent framework**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_e2j9Xvlkce"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for LangChain agent setup\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain_openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Set OpenAI and SERPAPI API keys securely (replace with your actual keys)\n",
    "os.environ['OPENAI_API_KEY'] = 'your_openai_api_key'\n",
    "os.environ['SERPAPI_API_KEY'] = 'your_serpapi_api_key'\n",
    "\n",
    "# Initialize the OpenAI agent with a temperature setting for control over output randomness\n",
    "llm = OpenAI(temperature=0.1)\n",
    "\n",
    "# Load necessary tools for the agent, including SERPAPI for searches and llm-math for calculations\n",
    "tools = load_tools(['serpapi', 'llm-math'], llm=llm)\n",
    "\n",
    "# Initialize the LangChain agent with the tools loaded and set it for dynamic response handling\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n",
    "\n",
    "# Query the agent to retrieve and calculate data\n",
    "query = 'Add average rating of Spider-Man and Iron-Man movies'\n",
    "agent.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0Vpp7DdlqcY"
   },
   "source": [
    "### Block 17: Installing Fairlearn for Model Bias Evaluation\n",
    "To evaluate and mitigate bias in machine learning models, we install the **Fairlearn** library, which offers tools for assessing fairness in predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HD9tUuyTl2RB"
   },
   "outputs": [],
   "source": [
    "# Install Fairlearn library for evaluating model fairness\n",
    "%pip install fairlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A_ZRNwel7qW"
   },
   "source": [
    "### Block 18: Detecting and Mitigating Bias Using Fairlearn\n",
    "This example uses a simulated dataset and logistic regression to evaluate model bias using **Fairlearn**, highlighting techniques for ensuring ethical AI practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xd5h41a7mHk0"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from fairlearn.metrics import MetricFrame, selection_rate\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate simulated data (100 samples with attributes like age, income, and gender)\n",
    "n_samples = 100\n",
    "ages = np.random.randint(18, 66, size=n_samples)\n",
    "incomes = np.random.randint(20000, 120001, size=n_samples)\n",
    "genders = np.random.randint(0, 2, size=n_samples)  # 0 for Male, 1 for Female\n",
    "\n",
    "# Simulate loan approval status based on income and age\n",
    "loan_approval = (incomes > 50000) & (ages < 50)\n",
    "loan_approval = loan_approval.astype(int)  # Convert boolean to integer\n",
    "\n",
    "# Create a DataFrame for the dataset\n",
    "data = pd.DataFrame({\n",
    "    'age': ages,\n",
    "    'income': incomes,\n",
    "    'gender': genders,\n",
    "    'loan_approval': loan_approval\n",
    "})\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = data[['age', 'income', 'gender']]\n",
    "y = data['loan_approval']\n",
    "\n",
    "# Train-test split with 20% data for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's fairness using Fairlearn\n",
    "predictions = model.predict(X_test)\n",
    "metric_frame = MetricFrame(\n",
    "    metrics=selection_rate,\n",
    "    y_true=y_test,\n",
    "    y_pred=predictions,\n",
    "    sensitive_features=X_test['gender']\n",
    ")\n",
    "\n",
    "# Display bias evaluation results\n",
    "print('\\nSelection rates across gender groups:')\n",
    "print(metric_frame.by_group)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
