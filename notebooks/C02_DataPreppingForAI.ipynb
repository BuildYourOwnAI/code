{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 2 - Prepping Data for AI\n",
        "This notebook contains examples of data preparation strategies for AI, including data cleaning, feature engineering, and handling data sensitivity. Using open-source tools like Pandas, LangChain, and ChromaDB, it explores design patterns for crafting high-quality datasets. It also covers techniques for ensuring data privacy and security, highlighting methods like data masking and synthetic data generation to safeguard sensitive information."
      ],
      "metadata": {
        "id": "znQjgeS3JXyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2-1: Defining Dataset Constants\n",
        "Defining **dataset** URLs as constants for reuse throughout the notebook, enabling easy loading of superhero datasets across multiple cells."
      ],
      "metadata": {
        "id": "Kr_g5Z0z_AmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base GitHub repository URL\n",
        "BASE_URL = \"https://opensourceai-book.github.io/code/datasets/\"\n",
        "\n",
        "# Dataset file names\n",
        "INFO_FILE = \"superheroes_info.csv\"\n",
        "INFO_CLEAN_FILE = \"superheroes_info_cleansed.csv\"\n",
        "POWERS_FILE = \"superheroes_powers.csv\"\n",
        "INFO_POWERS_FILE = \"superheroes_info_powers.csv\"\n",
        "PLOTS_FILE = \"superheroes_story_plots.csv\"\n",
        "\n",
        "# Construct full dataset URLs\n",
        "SUPERHEROES_INFO_URL = f\"{BASE_URL}{INFO_FILE}\"\n",
        "SUPERHEROES_INFO_CLEAN_URL = f\"{BASE_URL}{INFO_CLEAN_FILE}\"\n",
        "SUPERHEROES_POWERS_URL = f\"{BASE_URL}{POWERS_FILE}\"\n",
        "SUPERHEROES_INFO_POWERS_URL = f\"{BASE_URL}{INFO_POWERS_FILE}\"\n",
        "SUPERHEROES_INFO_PLOTS_URL = f\"{BASE_URL}{PLOTS_FILE}\""
      ],
      "metadata": {
        "id": "6gwd5J8n-9RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** If you see errors about unset constants, rerun the above block. This can happen if your Python runtime disconnects in Colab."
      ],
      "metadata": {
        "id": "1Z4hUSWUYylX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2-2: Loading and Displaying Dataset Stats\n",
        "We load our two superhero-related datasets from provided URLs into a Dataframe and displays basic statistics like number of rows, columns, and column names.\n"
      ],
      "metadata": {
        "id": "FMI3FCZVAXiL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAv1HKTkLjx8"
      },
      "outputs": [],
      "source": [
        "# Import our good friend Pandas, home of the Dataframe and Numpy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Using the constants defined earlier in the dictionary for reuse\n",
        "urls = {\n",
        "    \"Superheroes Info Dataset\": SUPERHEROES_INFO_URL,\n",
        "    \"Superheroes Powers Dataset\": SUPERHEROES_POWERS_URL\n",
        "}\n",
        "\n",
        "# Load datasets and display basic stats\n",
        "for name, url in urls.items():\n",
        "    df = pd.read_csv(url)\n",
        "    print(f\"{name}:\\n  - Rows: {df.shape[0]}\\n  - Columns: {df.shape[1]}\"\n",
        "          f\"\\n  - Column names: {', '.join(df.columns[:5])}...\\n\")\n",
        "    print(\"=\"*50 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2-3: Analyzing and Detecting Duplicates and Sparse Fields\n",
        "Identifies duplicate rows by superhero name and assess sparse fields, highlighting missing data percentages across all dataset columns."
      ],
      "metadata": {
        "id": "35g4H9-MRp4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(SUPERHEROES_INFO_URL)\n",
        "\n",
        "# Find duplicate rows based on all columns\n",
        "duplicates = df[df.duplicated(subset=['name'], keep=False)]\n",
        "\n",
        "# List unique superheroes with duplicate rows\n",
        "duplicate_names = duplicates['name'].unique()\n",
        "\n",
        "print(\"Superheroes with duplicate rows:\")\n",
        "print(duplicate_names)\n"
      ],
      "metadata": {
        "id": "i2p7L-bARpYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... **Run this cell next** to analyze the dataset by detecting sparse fields, calculating the percentage of missing data for each column."
      ],
      "metadata": {
        "id": "eY-2IkstYzy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(SUPERHEROES_INFO_URL)\n",
        "\n",
        "# Replace placeholder values '-' and '-99' with NaN to detect missing data\n",
        "df.replace('-', np.nan, inplace=True)\n",
        "df.replace(-99, np.nan, inplace=True)\n",
        "\n",
        "# Remove the counter column (assuming it's the first column)\n",
        "df.drop(df.columns[0], axis=1, inplace=True)\n",
        "\n",
        "# Calculate the percentage of missing data for all columns\n",
        "total_entries = len(df)\n",
        "sparse_fields = {\n",
        "    col: (df[col].isnull().sum() / total_entries) * 100 for col in df.columns\n",
        "}\n",
        "\n",
        "# Display all columns with their percentage of missing data\n",
        "print(\"\\nSparse Fields (Percentage of Missing Data):\")\n",
        "print(\"{:<20} {:>10}\".format(\"Column\", \"Missing %\"))\n",
        "print(\"-\" * 30)\n",
        "for col, percentage in sparse_fields.items():\n",
        "    print(\"{:<20} {:>8.2f}%\".format(col, percentage))"
      ],
      "metadata": {
        "id": "vJdIT3b2Kgce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2-4: Infer Superhero Race with LangChain\n",
        "This function uses LangChain and an open-source model from Hugging Face Hub to infer a superhero's race based on their name and publisher.\n",
        "\n",
        "⚠️ Make sure you've set your `HF_TOKEN` (and optionally,  `OPENAI_API_KEY`)  in Colab secrets.\n",
        "Refer to **Chapter 1** (notebook or book text) for setup instructions.  \n",
        "The next two code cells install the required packages and configure API keys in the environment for use with LangChain.\n"
      ],
      "metadata": {
        "id": "fxz2T_Pgf86c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages for Hugging Face and LangChain usage\n",
        "%pip install -q langchain langchain-community langchain-openai huggingface_hub"
      ],
      "metadata": {
        "id": "A88mOcmVLTsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants and API Key Configuration\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# === Load API keys securely from Google Colab Secrets ===\n",
        "def load_api_keys():\n",
        "    keys = {\n",
        "        \"HF_TOKEN\": userdata.get(\"HF_TOKEN\"),\n",
        "        \"OPENAI_API_KEY\": userdata.get(\"OPENAI_API_KEY\"),\n",
        "    }\n",
        "    for key, value in keys.items():\n",
        "        if not value:\n",
        "            raise ValueError(f\"❌ Missing {key}. Please set this API key in Colab secrets.\")\n",
        "        os.environ[key] = value\n",
        "    print(\"✅ All API keys loaded and configured successfully.\")\n",
        "\n",
        "# Execute API key loading upon running this cell\n",
        "load_api_keys()"
      ],
      "metadata": {
        "id": "FZt4jheKLxBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define the default LLM (Text Generation Model) to use from Hugging Face\n",
        "Run the code cell below to define the DEFAULT_MODEL constant\n",
        "> ⚠️ If you get an error running LangChain code due to a missing model, welcome to open-source AI development. Models are updated or replaced often. Check Hugging Face’s list of supported text generation models here:  \n",
        "> https://huggingface.co/docs/api-inference/en/supported-models\n"
      ],
      "metadata": {
        "id": "jiN5tS-7XJzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_MODEL = \"mistralai/Mistral-Nemo-Instruct-2407\""
      ],
      "metadata": {
        "id": "4tb4CKRLXOPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from huggingface_hub import InferenceClient\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "client = InferenceClient(model=DEFAULT_MODEL)\n",
        "\n",
        "# Define the prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'Provide only the superhero race in one word, surrounded by '\n",
        "               'parentheses (). If you don’t know, respond with \"\".'),\n",
        "    ('human', 'What is the race of {hero_name} from {publisher}?')\n",
        "])\n",
        "\n",
        "def get_race_from_llm(hero_name, publisher):\n",
        "    # Format the prompt\n",
        "    formatted_prompt = prompt_template.format(\n",
        "        hero_name=hero_name,\n",
        "        publisher=publisher\n",
        "    )\n",
        "\n",
        "    # Send to model\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_ID,\n",
        "        messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
        "        max_tokens=100,\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    # Debug: print raw content\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    # Extract race from parentheses\n",
        "    match = re.search(r'\\((\\w+)\\)', content)\n",
        "    return match.group(1) if match else \"\"\n",
        "\n",
        "# Test it\n",
        "hero_name = \"Spider-Man\"\n",
        "publisher = \"Marvel Comics\"\n",
        "race = get_race_from_llm(hero_name, publisher)\n",
        "print(f\"Race of {hero_name}: {race}\")\n"
      ],
      "metadata": {
        "id": "Y0H3GNZB4qVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2-5: Clean and Normalize Dataset\n",
        "Cleans and normalizes the dataset by removing unnecessary columns, filling missing values, and applying race-based averages (using langchain based program defined in previous listing)."
      ],
      "metadata": {
        "id": "hwaPyeUuAk2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv(SUPERHEROES_INFO_URL)\n",
        "\n",
        "# Step 1: Remove unnecessary columns\n",
        "df.drop(columns=['Unnamed: 0', 'Eye color', 'Hair color', 'Skin color'],\n",
        "        inplace=True)\n",
        "\n",
        "# Step 2: Normalize placeholder values\n",
        "df.replace('-', np.nan, inplace=True)\n",
        "df.replace(-99, np.nan, inplace=True)\n",
        "\n",
        "# Step 3: Use LangChain LLM to fill missing race values\n",
        "for idx, row in df[df['Race'].isna()].iterrows():\n",
        "    race = get_race_from_llm(row['name'], row['Publisher'])\n",
        "    if race:\n",
        "        df.at[idx, 'Race'] = race\n",
        "        print(f\"Filled race for {row['name']}: {race}\")\n",
        "    else:\n",
        "        print(f\"Could not determine race for {row['name']}\")\n",
        "\n",
        "# Step 4: Fill missing height/weight using race averages\n",
        "# Round the averages to 1 decimal place\n",
        "race_grouped = df.groupby('Race')[['Height', 'Weight']].mean().round(1)\n",
        "\n",
        "for race in race_grouped.index:\n",
        "    avg_height = race_grouped.loc[race, 'Height']\n",
        "    avg_weight = race_grouped.loc[race, 'Weight']\n",
        "    df.loc[(df['Race'] == race) & (df['Height'].isnull()), 'Height'] = avg_height\n",
        "    df.loc[(df['Race'] == race) & (df['Weight'].isnull()), 'Weight'] = avg_weight\n",
        "\n",
        "# Save cleansed data to CSV\n",
        "df.to_csv('superheroes_info_cleansed.csv', index=False)\n",
        "\n",
        "# Output sample of cleaned dataset\n",
        "print(df[['name', 'Race', 'Height', 'Weight']].sample(10))"
      ],
      "metadata": {
        "id": "N5HVTedsApAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2-6: Calculating Quality with Gini Coefficient\n",
        "We calculate the **Gini coefficient** for the \"Alignment\" column to assess imbalance between categories, helping us evaluate potential skew in model predictions."
      ],
      "metadata": {
        "id": "HS4TDW8bRsKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Superheroes Info dataset\n",
        "df_info = pd.read_csv(SUPERHEROES_INFO_CLEAN_URL)\n",
        "\n",
        "# Function to calculate the Gini coefficient\n",
        "def gini_coefficient(counts):\n",
        "    sorted_counts = np.sort(counts)  # Sort counts\n",
        "    n = len(counts)\n",
        "    cumulative_values = np.cumsum(sorted_counts)  # Cumulative sorted count sum\n",
        "    index = np.arange(1, n + 1)\n",
        "    gini = (np.sum((2 * index - n - 1) * sorted_counts)) / (\n",
        "        n * np.sum(sorted_counts)\n",
        "    )\n",
        "    return gini\n",
        "\n",
        "# Count occurrences of each alignment category (good, bad, neutral)\n",
        "alignment_counts = df_info['Alignment'].value_counts()\n",
        "\n",
        "# Calculate Gini coefficient for the Alignment column\n",
        "gini_score = gini_coefficient(alignment_counts.values)\n",
        "\n",
        "# Display the counts and Gini coefficient\n",
        "print(\"Alignment Counts:\\n\", alignment_counts)\n",
        "print(f\"Gini Coefficient for 'Alignment' categories: {gini_score}\")"
      ],
      "metadata": {
        "id": "1BT8XpY8R7Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2-7: Data Relevance Using EDA\n",
        "Analyzes average height, weight, and moral alignment proportions by gender, then formats and prints an easy-to-read table for data relevance."
      ],
      "metadata": {
        "id": "wh7D4fnBgA-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(SUPERHEROES_INFO_CLEAN_URL)\n",
        "\n",
        "# Filter for missing values in key columns\n",
        "df = df.dropna(subset=['Gender', 'Alignment', 'Race', 'Height', 'Weight'])\n",
        "\n",
        "# Analyze imbalance across categorical columns\n",
        "categories = ['Gender', 'Alignment', 'Race']\n",
        "\n",
        "gini_results = {}\n",
        "for category in categories:\n",
        "    counts = df[category].value_counts()\n",
        "    gini_results[category] = gini_coefficient(counts.values)\n",
        "\n",
        "# Analyze imbalance for discretized height and weight\n",
        "df['Height_bins'] = pd.cut(df['Height'], bins=5)\n",
        "df['Weight_bins'] = pd.cut(df['Weight'], bins=5)\n",
        "\n",
        "gini_results['Height'] = gini_coefficient(df['Height_bins'].value_counts().values)\n",
        "gini_results['Weight'] = gini_coefficient(df['Weight_bins'].value_counts().values)\n",
        "\n",
        "# Print Gini coefficients for each category\n",
        "print(\"Gini Coefficients for Dataset Imbalances:\")\n",
        "for category, gini_score in gini_results.items():\n",
        "    print(f\"{category}: {gini_score:.3f}\")"
      ],
      "metadata": {
        "id": "805d2lNeDi7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2-8: Superhero Dataset Merge Analysis\n",
        "Analyze compatibility of superheroes_info_clean and superheroes_powers by merging on hero_names field and calculating match percentage for **feature integration depth**."
      ],
      "metadata": {
        "id": "Jkf7MbqYtLuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Load the datasets from the URLs\n",
        "info_df = pd.read_csv(SUPERHEROES_INFO_CLEAN_URL)\n",
        "powers_df = pd.read_csv(SUPERHEROES_POWERS_URL)\n",
        "\n",
        "# Rename 'name' in info_df to 'hero_names' for consistent merging\n",
        "info_df.rename(columns={'name': 'hero_names'}, inplace=True)\n",
        "\n",
        "# Merge the datasets on 'hero_names'\n",
        "merged_df = pd.merge(info_df, powers_df, on='hero_names', how='inner')\n",
        "\n",
        "# Calculate and display the total number of matched entries and the percentage match\n",
        "matched_count = merged_df.shape[0]\n",
        "total_info_count = info_df.shape[0]\n",
        "percentage_matched = (matched_count / total_info_count) * 100\n",
        "\n",
        "print(f\"Matched entries: {matched_count}\")\n",
        "print(f\"Total entries in Info dataset: {total_info_count}\")\n",
        "print(f\"Percentage matched: {percentage_matched:.2f}%\")\n"
      ],
      "metadata": {
        "id": "7_8DbU9xIQCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2-9: Hero Power Metrics Enhancement\n",
        "Calculates Offensive Power Rating (OPR) and Strategic Defense Rating (SDR) for each hero by summing relevant power attributes, enriching the dataset for enhanced power analysis."
      ],
      "metadata": {
        "id": "5JyZOQa3uJeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the datasets\n",
        "info_dfs = pd.read_csv(SUPERHEROES_INFO_CLEAN_URL)\n",
        "powers_dfs = pd.read_csv(SUPERHEROES_POWERS_URL)\n",
        "\n",
        "# Ensure consistent naming by renaming 'hero_names' to 'name' in powers_dfs\n",
        "powers_dfs.rename(columns={'hero_names': 'name'}, inplace=True)\n",
        "\n",
        "# Define power attributes for offensive and defensive power calculations\n",
        "OFFENSIVE_POWERS = [\n",
        "    'Super Strength', 'Energy Blasts', 'Weapons Master', 'Marksmanship',\n",
        "    'Magic', 'Telekinesis', 'Cryokinesis', 'Fire Control',\n",
        "    'Power Augmentation', 'Animal Oriented Powers'\n",
        "]\n",
        "\n",
        "DEFENSIVE_POWERS = [\n",
        "    'Durability', 'Invulnerability', 'Force Fields', 'Energy Absorption',\n",
        "    'Regeneration', 'Immortality', 'Camouflage', 'Phasing',\n",
        "    'Enhanced Senses', 'Teleportation'\n",
        "]\n",
        "\n",
        "# Calculate OPR and SDR in the powers dataset\n",
        "powers_dfs['OPR'] = powers_dfs[OFFENSIVE_POWERS].sum(axis=1)\n",
        "powers_dfs['SDR'] = powers_dfs[DEFENSIVE_POWERS].sum(axis=1)\n",
        "\n",
        "# Keep only necessary columns (name, OPR, SDR)\n",
        "powers_ratings = powers_dfs[['name', 'OPR', 'SDR']]\n",
        "\n",
        "# Merge the calculated fields into the info dataset\n",
        "info_with_ratings = pd.merge(info_dfs, powers_ratings, on='name', how='left')\n",
        "\n",
        "# Save the updated info dataset with OPR and SDR\n",
        "info_with_ratings.to_csv(INFO_POWERS_FILE, index=False)\n",
        "print(f\"Updated dataset saved as {INFO_POWERS_FILE} with OPR and SDR.\")"
      ],
      "metadata": {
        "id": "XPi5-a7TwO0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2-10: Generate Story Plot Dataset  \n",
        "This code generates superhero plot summaries using randomized archetypes and a Hugging Face language model, then saves the results to a CSV file for analysis or reuse.\n"
      ],
      "metadata": {
        "id": "h8mc8decK-Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Absolutely! Here's the fully updated and working version of your original code,\n",
        "# now using Hugging Face's `InferenceClient` instead of the deprecated LangChain `HuggingFaceEndpoint`.\n",
        "# The logic, structure, and CSV output are preserved exactly.\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import random\n",
        "from huggingface_hub import InferenceClient\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Configuration\n",
        "TEMP = 0.4\n",
        "NUM_PLOT_SAMPLES = 5\n",
        "PLOTS_FILE = \"superhero_plots.csv\"\n",
        "\n",
        "# Initialize Hugging Face inference client\n",
        "client = InferenceClient(model=DEFAULT_MODEL)\n",
        "\n",
        "# Define the LangChain prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'Generate a concise superhero plot where:'),\n",
        "    ('human', '''\n",
        "    - Hero archetype: {hero}\n",
        "    - Villain archetype: {villain}\n",
        "    - Conflict type: {conflict_type}\n",
        "    - Setting: {setting}\n",
        "    Include a title and three key plot points.\n",
        "    ''')\n",
        "])\n",
        "\n",
        "# Archetype components\n",
        "heroes = [\n",
        "    \"reluctant hero\", \"outcast\", \"chosen one\", \"antihero\", \"reluctant mentor\",\n",
        "    \"alien protector\", \"cyber-enhanced rebel\", \"fallen warrior\", \"mystic sage\",\n",
        "    \"time-traveling guardian\", \"reformed villain\", \"empathic healer\",\n",
        "    \"street-level hero\", \"artificial intelligence (AI)\", \"mystical guardian\",\n",
        "    \"scientific genius\", \"nature warrior\", \"cosmic nomad\", \"avenger of loss\",\n",
        "    \"dimension traveler\", \"energy manipulator\"\n",
        "]\n",
        "\n",
        "villains = [\n",
        "    \"mastermind\", \"alien invader\", \"corrupt hero\", \"rogue AI\", \"dark sorcerer\",\n",
        "    \"doppelgänger\", \"genetic experiment\", \"shadow manipulator\", \"void entity\",\n",
        "    \"cyber overlord\", \"time manipulator\", \"mythical creature\", \"corrupt politician\",\n",
        "    \"revenge-seeking nemesis\", \"intergalactic tyrant\", \"poisonous rival\",\n",
        "    \"mind controller\", \"techno-terrorist\", \"corrupted scientist\", \"necromancer\",\n",
        "    \"cosmic parasite\"\n",
        "]\n",
        "\n",
        "conflicts = [\n",
        "    \"personal vendetta\", \"cosmic invasion\", \"technological catastrophe\", \"race against time\",\n",
        "    \"internal struggle\", \"resource battle\", \"power corruption\", \"revenge plot\",\n",
        "    \"city under siege\", \"reality manipulation\", \"identity crisis\", \"magic vs. science\",\n",
        "    \"clash of ideals\", \"control of the mind\", \"forbidden knowledge\", \"mutant uprising\",\n",
        "    \"alien invasion\", \"technological sabotage\", \"environmental disaster\",\n",
        "    \"lost relic chase\", \"temporal war\"\n",
        "]\n",
        "\n",
        "settings = [\n",
        "    \"futuristic city\", \"ancient ruins\", \"space station\", \"post-apocalyptic wasteland\",\n",
        "    \"underwater fortress\", \"cyberpunk metropolis\", \"forest of lost souls\",\n",
        "    \"deserted asylum\", \"alien planet\", \"hidden temple\", \"high-tech lab\",\n",
        "    \"floating island\", \"deep jungle\", \"mystic caves\", \"parallel universe\",\n",
        "    \"underground society\", \"cloud city\", \"dreamscape\", \"hollow mountain\",\n",
        "    \"time-frozen world\", \"ruins of civilization\"\n",
        "]\n",
        "\n",
        "# Generate plots\n",
        "plot_samples = []\n",
        "for i in range(NUM_PLOT_SAMPLES):\n",
        "    hero = random.choice(heroes)\n",
        "    villain = random.choice(villains)\n",
        "    conflict = random.choice(conflicts)\n",
        "    setting = random.choice(settings)\n",
        "\n",
        "    # Format prompt\n",
        "    formatted_prompt = prompt.format(\n",
        "        hero=hero,\n",
        "        villain=villain,\n",
        "        conflict_type=conflict,\n",
        "        setting=setting\n",
        "    )\n",
        "\n",
        "    # Get response from model\n",
        "    response_obj = client.chat.completions.create(\n",
        "        model=DEFAULT_MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
        "        max_tokens=250,\n",
        "        temperature=TEMP\n",
        "    )\n",
        "\n",
        "    response = response_obj.choices[0].message.content\n",
        "    response_cleaned = response.replace(\"\\n\", \" | \")\n",
        "\n",
        "    plot_samples.append({\n",
        "        \"Hero\": hero,\n",
        "        \"Villain\": villain,\n",
        "        \"Conflict Type\": conflict,\n",
        "        \"Setting\": setting,\n",
        "        \"Plot\": response_cleaned\n",
        "    })\n",
        "\n",
        "    print(f\"\\nPlot {i + 1}:\")\n",
        "    print(f\"Hero: {hero}\")\n",
        "    print(f\"Villain: {villain}\")\n",
        "    print(f\"Conflict Type: {conflict}\")\n",
        "    print(f\"Setting: {setting}\")\n",
        "    print(f\"Plot:\\n{response}\")\n",
        "\n",
        "# Save results\n",
        "df = pd.DataFrame(plot_samples)\n",
        "df.to_csv(PLOTS_FILE, index=False)\n",
        "print(f\"\\nPlot samples saved to {PLOTS_FILE}.\")"
      ],
      "metadata": {
        "id": "ABiFxbVRK9YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2-11 Comic Story Assistant with RAG\n",
        "Showcases a Retrieval-Augmented Generation (RAG) approach that uses ChromaDB with hero plot and power data to dynamically generate tailored superhero story arcs."
      ],
      "metadata": {
        "id": "8ihVdlvDH9iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q chromadb"
      ],
      "metadata": {
        "id": "qyaq2bXmIGce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Load datasets from URLs\n",
        "plot_data = pd.read_csv(SUPERHEROES_INFO_PLOTS_URL)\n",
        "hero_data = pd.read_csv(SUPERHEROES_INFO_POWERS_URL)\n",
        "\n",
        "# Prepare plot data for embeddings\n",
        "plot_texts = plot_data['Plot'].tolist()\n",
        "\n",
        "# Initialize embeddings model and create Chroma vector database for plot data\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = Chroma.from_texts(plot_texts, embeddings)\n",
        "\n",
        "# Define query from the comic writer\n",
        "query_text = (\"Show me a story where a time-traveling hero faces a scientific \"\n",
        "              \"mastermind in a futuristic city.\")\n",
        "\n",
        "# Retrieve the most relevant plot based on similarity\n",
        "results = db.similarity_search(query_text, 1)\n",
        "selected_plot = results[0].page_content\n",
        "\n",
        "# Randomly select hero and villain from hero data (filtered by alignment)\n",
        "hero_row = hero_data[hero_data['Alignment'] == 'good'].sample(1).iloc[0]\n",
        "villain_row = hero_data[hero_data['Alignment'] == 'bad'].sample(1).iloc[0]\n",
        "\n",
        "# Extract hero and villain details for prompt\n",
        "hero_name, hero_opr, hero_sdr = hero_row['name'], hero_row['OPR'], hero_row['SDR']\n",
        "villain_name, villain_opr, villain_sdr = villain_row['name'], villain_row['OPR'], villain_row['SDR']\n",
        "\n",
        "# Define prompt template for story creation\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Generate a superhero story based on the following details:\"),\n",
        "    (\"human\", '''\n",
        "    Plot Outline: {plot}\n",
        "\n",
        "    Hero: {hero} with an offense power of {hero_opr}x and defense of {hero_sdr}x.\n",
        "    Villain: {villain} with an offense power of {villain_opr}x and defense\n",
        "    of {villain_sdr}x.\n",
        "\n",
        "    Rewrite the story with {hero} as the main hero and {villain} as the main villain.\n",
        "    Build tension, introduce challenges, and create a climactic final showdown.\n",
        "    ''')\n",
        "])\n",
        "\n",
        "print(f\"\"\"Hero: {hero_name} with OPR of {hero_opr} and defense of {hero_sdr}x.\n",
        "Villain: {villain_name} with OPR of {villain_opr}x and defense of {villain_sdr}x.\n",
        "\"\"\")\n",
        "\n",
        "# Set up LangChain LLM for generating story arcs\n",
        "llm = ChatOpenAI(temperature=0.2)\n",
        "chain = prompt | llm\n",
        "\n",
        "# Invoke the LLM with structured prompt\n",
        "response = chain.invoke({\n",
        "    \"plot\": selected_plot,\n",
        "    \"hero\": hero_name,\n",
        "    \"villain\": villain_name,\n",
        "    \"hero_opr\": hero_opr,\n",
        "    \"hero_sdr\": hero_sdr,\n",
        "    \"villain_opr\": villain_opr,\n",
        "    \"villain_sdr\": villain_sdr\n",
        "})\n",
        "\n",
        "# Display the generated story arc\n",
        "print(\"Generated Story Arc:\")\n",
        "print(response.content)\n",
        "\n"
      ],
      "metadata": {
        "id": "glItxfhuH9G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2-12: Pseudonymizing Superhero Plots Using SpaCy\n",
        "Demonstrates pseudonymization of entities within superhero plots, using spaCy to replace names, organizations, and locations with generic terms for privacy. **Note:** Be sure to run the *pip* install before running the code snippet below."
      ],
      "metadata": {
        "id": "8n1tjw_Ca6MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q spacy"
      ],
      "metadata": {
        "id": "EqZli752bjYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# Load spaCy's English model for entity recognition\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Example dataset with superhero story plots\n",
        "plot_data = pd.read_csv(SUPERHEROES_INFO_PLOTS_URL)\n",
        "\n",
        "# Function to pseudonymize entity names in the plot text\n",
        "def pseudonymize_entities(text):\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        # Replace detected entities with generic labels\n",
        "        if ent.label_ == \"PERSON\":\n",
        "            text = text.replace(ent.text, 'Hero A')\n",
        "        elif ent.label_ == \"ORG\":\n",
        "            text = text.replace(ent.text, 'Organization X')\n",
        "        elif ent.label_ == \"GPE\":\n",
        "            text = text.replace(ent.text, 'Location Z')\n",
        "    return text\n",
        "\n",
        "# Pseudonymize only the first plot\n",
        "first_plot = plot_data['Plot'].iloc[0]\n",
        "pseudonymized_first_plot = pseudonymize_entities(first_plot)\n",
        "\n",
        "# Display the pseudonymized first plot\n",
        "print(\"Pseudonymized Plot:\")\n",
        "print(pseudonymized_first_plot)"
      ],
      "metadata": {
        "id": "znYuXOTDbDpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2-13: Data Masking And Differential Privacy\n",
        "Demonstrates data masking and differential privacy on health records by masking phone numbers and adding noise to age values."
      ],
      "metadata": {
        "id": "z2nujz0xyZ-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset: health records\n",
        "data = pd.DataFrame({\n",
        "    'patient_id': ['A123', 'B456', 'C789'],\n",
        "    'phone': ['123-456-7890', '987-654-3210', '555-123-4567'],\n",
        "    'age': [29, 47, 35],\n",
        "    'diagnosis': ['Condition A', 'Condition B', 'Condition A']\n",
        "})\n",
        "\n",
        "# Data Masking: Mask all but the last four digits of phone numbers\n",
        "data['masked_phone'] = data['phone'].apply(lambda x: 'XXX-XXX-' + x[-4:])\n",
        "\n",
        "# Differential Privacy: Add noise to age values for anonymization\n",
        "noise_level = 2  # Adjust noise level as needed\n",
        "data['age_noisy'] = data['age'] + np.random.laplace(0, noise_level, len(data))\n",
        "\n",
        "# Display the modified dataset\n",
        "print(\"Anonymized Data:\\n\", data[['patient_id', 'masked_phone',\n",
        "                                 'age_noisy', 'diagnosis']])"
      ],
      "metadata": {
        "id": "w5udkNUxydfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2-14: Encrypting Sensitive Data\n",
        "This code encrypts a sensitive dataset with **Fernet**, allowing secure decryption and access for authorized users only.\n",
        "**Note:** Be sure to run the following *pip install*"
      ],
      "metadata": {
        "id": "qAFVqS5tsRPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install cryptography"
      ],
      "metadata": {
        "id": "ji_uj2aIsZBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Fernet\n",
        "from cryptography.fernet import Fernet\n",
        "import pandas as pd\n",
        "import io  # Import io for StringIO\n",
        "\n",
        "# Generate encryption key and create a cipher suite\n",
        "key = Fernet.generate_key()\n",
        "cipher_suite = Fernet(key)\n",
        "\n",
        "# Sample sensitive data to be encrypted\n",
        "data = pd.DataFrame({\"Patient\": [\"John Doe\", \"Jane Smith\"],\n",
        "                     \"Diagnosis\": [\"Diabetes\", \"Hypertension\"]})\n",
        "data_str = data.to_csv(index=False)\n",
        "encrypted_data = cipher_suite.encrypt(data_str.encode())\n",
        "\n",
        "# Show part of the encrypted string\n",
        "print(\"Encrypted Data (partial):\", encrypted_data[:50], \"...\")\n",
        "\n",
        "# Decrypt the data when access is needed\n",
        "decrypted_data_str = cipher_suite.decrypt(encrypted_data).decode()\n",
        "secure_data = pd.read_csv(io.StringIO(decrypted_data_str))\n",
        "\n",
        "print(\"\\nDecrypted Data Accessible (Only by Authorized Users)\\n\", secure_data)"
      ],
      "metadata": {
        "id": "tnPG9CbjsV8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 2-15: Generating Synthetic Health Records\n",
        "Uses **Faker** to create fictional health records, providing a safe, realistic dataset structure for AI training or testing applications. **Note**: Be sure to run the following pip install."
      ],
      "metadata": {
        "id": "e3aWhUm5iY4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install Faker"
      ],
      "metadata": {
        "id": "p7sk7plLij1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from faker import Faker\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize Faker for synthetic data generation\n",
        "fake = Faker()\n",
        "\n",
        "# Generate a synthetic health records dataset\n",
        "data = pd.DataFrame({\n",
        "    'Patient_ID': [fake.uuid4() for _ in range(5)],\n",
        "    'Name': [fake.name() for _ in range(5)],\n",
        "    'Age': [fake.random_int(min=18, max=90) for _ in range(5)],\n",
        "    'Diagnosis': [fake.random_element(elements=('Condition A',\n",
        "                                               'Condition B',\n",
        "                                               'Condition C'))\n",
        "                   for _ in range(5)],\n",
        "    'Phone': [fake.phone_number() for _ in range(5)],\n",
        "    'Address': [fake.address().replace(\"\\n\", \", \") for _ in range(5)],\n",
        "    'Last_Visit': [fake.date_between(start_date='-2y', end_date='today')\n",
        "                   for _ in range(5)]\n",
        "})\n",
        "\n",
        "# Display the synthetic dataset\n",
        "print(data)"
      ],
      "metadata": {
        "id": "WB18zN0nid42"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}