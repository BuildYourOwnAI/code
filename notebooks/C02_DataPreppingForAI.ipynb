{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znQjgeS3JXyf"
      },
      "source": [
        "## Chapter 2 - Prepping Data for AI\n",
        "This notebook contains examples of data preparation strategies for AI, including data cleaning, feature engineering, and handling data sensitivity. Using open-source tools like Pandas, LangChain, and ChromaDB, it explores design patterns for crafting high-quality datasets. It also covers techniques for ensuring data privacy and security, highlighting methods like data masking and synthetic data generation to safeguard sensitive information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr_g5Z0z_AmW"
      },
      "source": [
        "### Listing 2-1: Defining Dataset Constants\n",
        "The first block defines the **dataset** file names, builds the URLs, and creates a small `load_and_describe` helper. The second block calls that helper twice to load the Superheroes Info and Superheroes Powers CSVs into Pandas DataFrames. For each dataset, you’ll see a quick summary: row count, column count, and the first few column names. This confirms your files loaded correctly before you begin cleaning and analysis. **Run both code blocks below in order.**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filename and URL Constants used throughout this Notebook\n",
        "\n",
        "# Base GitHub URL\n",
        "BASE_URL = \"https://opensourceai-book.github.io/code/datasets/\"\n",
        "\n",
        "# Dataset file names\n",
        "INFO_FILE = \"superheroes_info.csv\"\n",
        "INFO_CLEAN_FILE = \"superheroes_info_cleansed.csv\"\n",
        "POWERS_FILE = \"superheroes_powers.csv\"\n",
        "INFO_POWERS_FILE = \"superheroes_info_powers.csv\"\n",
        "PLOTS_FILE = \"superheroes_story_plots.csv\"\n",
        "\n",
        "# Construct full dataset URLs\n",
        "SUPERHEROES_INFO_URL = f\"{BASE_URL}{INFO_FILE}\"\n",
        "SUPERHEROES_INFO_CLEAN_URL = f\"{BASE_URL}{INFO_CLEAN_FILE}\"\n",
        "SUPERHEROES_POWERS_URL = f\"{BASE_URL}{POWERS_FILE}\"\n",
        "SUPERHEROES_INFO_POWERS_URL = f\"{BASE_URL}{INFO_POWERS_FILE}\"\n",
        "SUPERHEROES_INFO_PLOTS_URL = f\"{BASE_URL}{PLOTS_FILE}\""
      ],
      "metadata": {
        "id": "0jrjhkFken3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gwd5J8n-9RW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_and_describe(name, url):\n",
        "    \"\"\"Load CSV and print a brief summary.\"\"\"\n",
        "    # Read the CSV file into a DataFrame\n",
        "    df = pd.read_csv(url)\n",
        "    # Show the first five column names for a quick preview\n",
        "    cols = \", \".join(df.columns[:5])\n",
        "    print(f\"{name}\\n  Rows: {df.shape[0]}  Cols: {df.shape[1]}\")\n",
        "    print(f\"  First columns: {cols}\\n\" + \"=\" * 75)\n",
        "    return df\n",
        "\n",
        "# Load the two core superhero datasets and display summaries\n",
        "info_df = load_and_describe(\"Superheroes Info\", SUPERHEROES_INFO_URL)\n",
        "powers_df = load_and_describe(\"Superheroes Powers\", SUPERHEROES_POWERS_URL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35g4H9-MRp4j"
      },
      "source": [
        "### Listing 2-2: Analyzing and Detecting Duplicates and Sparse Fields\n",
        "Identifies duplicate rows by superhero name and assess sparse fields, highlighting missing data percentages across all dataset columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2p7L-bARpYb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Find duplicate rows based on all columns\n",
        "duplicates = info_df[info_df.duplicated(subset=['name'], keep=False)]\n",
        "\n",
        "# List unique superheroes with duplicate rows\n",
        "duplicate_names = duplicates['name'].unique()\n",
        "\n",
        "print(\"Superheroes with duplicate rows:\")\n",
        "print(duplicate_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY-2IkstYzy6"
      },
      "source": [
        "... **Run this cell next** to analyze the dataset by detecting sparse fields, calculating the percentage of missing data for each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJdIT3b2Kgce"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def print_sparse_fields(sparse):\n",
        "    \"\"\"Pretty-print percent missing per column.\"\"\"\n",
        "    print(\"\\nSparse Fields (Percentage of Missing Data):\")\n",
        "    print(\"{:<20} {:>10}\".format(\"Column\", \"Missing %\"))\n",
        "    print(\"-\" * 30)\n",
        "    for col, pct in sparse.items():\n",
        "        print(\"{:<20} {:>8.2f}%\".format(col, pct))\n",
        "\n",
        "# Normalize placeholder values to NaN\n",
        "info_df.replace({'-': np.nan, -99: np.nan}, inplace=True)\n",
        "\n",
        "# Remove the counter column (assumed first)\n",
        "info_df.drop(info_df.columns[0], axis=1, inplace=True)\n",
        "\n",
        "# Compute percent missing per column using NumPy on boolean masks\n",
        "sparse_fields = {\n",
        "    col: float(np.mean(info_df[col].isna().to_numpy())) * 100.0\n",
        "    for col in info_df.columns\n",
        "}\n",
        "\n",
        "# Display results\n",
        "print_sparse_fields(sparse_fields)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxz2T_Pgf86c"
      },
      "source": [
        "### Listing 2-3: Infer Superhero Species with LangChain\n",
        "This function uses LangChain and an open-source model from Hugging Face Hub to infer a superhero's species based on their name and publisher.\n",
        "\n",
        "⚠️ Make sure you've set your `HF_TOKEN` (and optionally,  `OPENAI_API_KEY`)  in Colab secrets.\n",
        "Refer to **Chapter 1** (notebook or book text) for setup instructions.  \n",
        "The next two code cells install the required packages and configure API keys in the environment for use with LangChain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A88mOcmVLTsc"
      },
      "outputs": [],
      "source": [
        "# Install required packages for Hugging Face and LangChain usage\n",
        "\n",
        "%pip install -q \"langchain>=0.2\" \"langchain-huggingface>=0.0.3\" \\\n",
        "                 \"huggingface_hub>=0.23\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZt4jheKLxBM"
      },
      "outputs": [],
      "source": [
        "# Constants and API Key Configuration\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# === Load API keys securely from Google Colab Secrets ===\n",
        "def load_api_keys():\n",
        "    keys = {\n",
        "        \"HF_TOKEN\": userdata.get(\"HF_TOKEN\"),\n",
        "        \"OPENAI_API_KEY\": userdata.get(\"OPENAI_API_KEY\"),\n",
        "    }\n",
        "    for key, value in keys.items():\n",
        "        if not value:\n",
        "            raise ValueError(f\"❌ Missing {key}. Please set this API key in Colab secrets.\")\n",
        "        os.environ[key] = value\n",
        "    print(\"✅ All API keys loaded and configured successfully.\")\n",
        "\n",
        "# Execute API key loading upon running this cell\n",
        "load_api_keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiN5tS-7XJzG"
      },
      "source": [
        "#### Define the default LLM (Text Generation Model) to use from Hugging Face\n",
        "Run the code cell below to define the DEFAULT_MODEL constant\n",
        "> ⚠️ If you get an error running LangChain code due to a missing model, welcome to open-source AI development. Models are updated or replaced often. Check Hugging Face’s list of supported text generation models here:  \n",
        "> https://huggingface.co/docs/api-inference/en/supported-models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tb4CKRLXOPK"
      },
      "outputs": [],
      "source": [
        "# Candidate Models\n",
        "\n",
        "#DEFAULT_MODEL = \"openai/gpt-oss-20b\"\n",
        "#DEFAULT_MODEL = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "#DEFAULT_MODEL = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "DEFAULT_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0H3GNZB4qVP"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Build chat LLM on Hugging Face serverless\n",
        "chat_llm = ChatHuggingFace(\n",
        "    llm=HuggingFaceEndpoint(\n",
        "        repo_id=DEFAULT_MODEL, task=\"conversational\",\n",
        "        temperature=0.1, max_new_tokens=24, return_full_text=False\n",
        "    )\n",
        ")\n",
        "\n",
        "# Prompt that constrains the output format\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'Provide only the superhero race in one word, surrounded by '\n",
        "               'parentheses (). If you don’t know, respond with \"\".'),\n",
        "    ('human', 'What is the race of {hero_name} from {publisher}?')\n",
        "])\n",
        "\n",
        "# Chain: Prompt → LLM → plain text\n",
        "chain = prompt | chat_llm | StrOutputParser()\n",
        "\n",
        "# Extract the one word in parentheses\n",
        "def get_species(hero_name, publisher):\n",
        "    txt = chain.invoke({\"hero_name\": hero_name, \"publisher\": publisher})\n",
        "    m = re.search(r\"\\(([A-Za-z\\-']+)\\)\", txt)\n",
        "    return m.group(1) if m else \"\"\n",
        "\n",
        "# Example call\n",
        "print(\"Species of Spider-Man: \", get_species(\"Spider-Man\", \"Marvel Comics\"))\n",
        "print(\"Batman →\", get_species(\"Batman\", \"DC Comics\"))\n",
        "print(\"Vision →\", get_species(\"Vision\", \"Marvel Comics\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwaPyeUuAk2r"
      },
      "source": [
        "### Listing 2-4: Clean and Normalize Dataset\n",
        "Cleans and normalizes the dataset by removing unnecessary columns, filling missing values, and applying race-based averages (using langchain based program defined in previous listing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5HVTedsApAv"
      },
      "outputs": [],
      "source": [
        "# Start from info_df (loaded earlier) and work on a copy\n",
        "df = info_df.copy()\n",
        "\n",
        "# Drop sparse or unused columns\n",
        "df.drop(columns=['Unnamed: 0', 'Eye color', 'Hair color', 'Skin color'],\n",
        "        errors='ignore', inplace=True)\n",
        "\n",
        "# Normalize placeholders to NaN for consistent missing-value handling\n",
        "df.replace({'-': np.nan, -99: np.nan}, inplace=True)\n",
        "\n",
        "# Fill Species via LLM (see Listing 2-4)\n",
        "miss = df['Species'].isna()\n",
        "filled = 0\n",
        "for i, row in df.loc[miss, ['name', 'Publisher']].iterrows():\n",
        "    sp = get_species(row['name'], row['Publisher'])\n",
        "    if sp:\n",
        "        df.at[i, 'Species'] = sp\n",
        "        filled += 1\n",
        "print(f\"Species filled: {filled}, still missing: {df['Species'].isna().sum()}\")\n",
        "\n",
        "# Impute Height and Weight by Species mean (rounded to 1 decimal)\n",
        "grp = (df.groupby('Species')[['Height', 'Weight']]\n",
        "         .transform('mean')\n",
        "         .round(1))\n",
        "df['Height'] = df['Height'].fillna(grp['Height'])\n",
        "df['Weight'] = df['Weight'].fillna(grp['Weight'])\n",
        "\n",
        "# Save and preview a sample of the cleaned result\n",
        "df.to_csv('superheroes_info_cleansed.csv', index=False)\n",
        "print(df[['name', 'Species', 'Height', 'Weight']].sample(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS4TDW8bRsKF"
      },
      "source": [
        "### Listing 2-5: Calculating Quality with Gini Coefficient\n",
        "We calculate the **Gini coefficient** for the \"Alignment\" column to assess imbalance between categories, helping us evaluate potential skew in model predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BT8XpY8R7Tz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Superheroes Info dataset\n",
        "df_info = pd.read_csv(SUPERHEROES_INFO_CLEAN_URL)\n",
        "\n",
        "# Function to calculate the Gini coefficient\n",
        "def gini_coefficient(counts):\n",
        "    sorted_counts = np.sort(counts)  # Sort counts\n",
        "    n = len(counts)\n",
        "    cumulative_values = np.cumsum(sorted_counts)  # Cumulative sorted count sum\n",
        "    index = np.arange(1, n + 1)\n",
        "    gini = (np.sum((2 * index - n - 1) * sorted_counts)) / (\n",
        "        n * np.sum(sorted_counts)\n",
        "    )\n",
        "    return gini\n",
        "\n",
        "# Count occurrences of each alignment category (good, bad, neutral)\n",
        "alignment_counts = df_info['Alignment'].value_counts()\n",
        "\n",
        "# Calculate Gini coefficient for the Alignment column\n",
        "gini_score = gini_coefficient(alignment_counts.values)\n",
        "\n",
        "# Display the counts and Gini coefficient\n",
        "print(\"Alignment Counts:\\n\", alignment_counts)\n",
        "print(f\"Gini Coefficient for 'Alignment' categories: {gini_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh7D4fnBgA-o"
      },
      "source": [
        "### Listing 2-6: Data Relevance Using EDA\n",
        "Analyzes average height, weight, and moral alignment proportions by gender, then formats and prints an easy-to-read table for data relevance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "805d2lNeDi7u"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(SUPERHEROES_INFO_CLEAN_URL)\n",
        "\n",
        "# Filter for missing values in key columns\n",
        "df = df.dropna(subset=['Gender', 'Alignment', 'Species', 'Height', 'Weight'])\n",
        "\n",
        "# Analyze imbalance across categorical columns\n",
        "categories = ['Gender', 'Alignment', 'Species']\n",
        "\n",
        "gini_results = {}\n",
        "for category in categories:\n",
        "    counts = df[category].value_counts()\n",
        "    gini_results[category] = gini_coefficient(counts.values)\n",
        "\n",
        "# Analyze imbalance for discretized height and weight\n",
        "df['Height_bins'] = pd.cut(df['Height'], bins=5)\n",
        "df['Weight_bins'] = pd.cut(df['Weight'], bins=5)\n",
        "\n",
        "gini_results['Height'] = gini_coefficient(df['Height_bins'].value_counts().values)\n",
        "gini_results['Weight'] = gini_coefficient(df['Weight_bins'].value_counts().values)\n",
        "\n",
        "# Print Gini coefficients for each category\n",
        "print(\"Gini Coefficients for Dataset Imbalances:\")\n",
        "for category, gini_score in gini_results.items():\n",
        "    print(f\"{category}: {gini_score:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkf7MbqYtLuA"
      },
      "source": [
        "### Listing 2-7: Superhero Dataset Merge Analysis\n",
        "Analyze compatibility of superheroes_info_clean and superheroes_powers by merging on hero_names field and calculating match percentage for **feature integration depth**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_8DbU9xIQCD"
      },
      "outputs": [],
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Load the datasets from the URLs\n",
        "info_df = pd.read_csv(SUPERHEROES_INFO_CLEAN_URL)\n",
        "powers_df = pd.read_csv(SUPERHEROES_POWERS_URL)\n",
        "\n",
        "# Rename 'name' in info_df to 'hero_names' for consistent merging\n",
        "info_df.rename(columns={'name': 'hero_names'}, inplace=True)\n",
        "\n",
        "# Merge the datasets on 'hero_names'\n",
        "merged_df = pd.merge(info_df, powers_df, on='hero_names', how='inner')\n",
        "\n",
        "# Calculate and display the total number of matched entries and the percentage match\n",
        "matched_count = merged_df.shape[0]\n",
        "total_info_count = info_df.shape[0]\n",
        "percentage_matched = (matched_count / total_info_count) * 100\n",
        "\n",
        "print(f\"Matched entries: {matched_count}\")\n",
        "print(f\"Total entries in Info dataset: {total_info_count}\")\n",
        "print(f\"Percentage matched: {percentage_matched:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JyZOQa3uJeD"
      },
      "source": [
        "### Listing 2-8: Compute OPR/SDR and Merge into Hero Info\n",
        "This program builds two composite signals for each hero: Offensive Power Rating\n",
        "(OPR) and Strategic Defense Rating (SDR). Magic and Super Speed are treated as\n",
        "dual-use with a small defensive weight, and the code avoids double counting while\n",
        "using only columns that exist. Duplicate power rows are aggregated by hero name,\n",
        "and a has_powers_source flag is added (1=yes, 0=no) to mark whether a powers\n",
        "row exists so missing rows are not mistaken for true zeros. The scores and flag are\n",
        "then merged into the info table and written to INFO_POWERS_FILE."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Helper functions\n",
        "\n",
        "def normalize_join_key(df):\n",
        "    if 'hero_names' in df.columns and 'name' not in df.columns:\n",
        "        return df.rename(columns={'hero_names': 'name'})\n",
        "    return df\n",
        "\n",
        "def dedupe_info(df):\n",
        "    if 'Publisher' in df.columns:\n",
        "        return (df.sort_values(['name','Publisher'])\n",
        "                .drop_duplicates(subset=['name','Publisher'], keep='first'))\n",
        "    return (df.sort_values('name')\n",
        "            .drop_duplicates(subset=['name'], keep='first'))\n",
        "\n",
        "def aggregate_powers(df):\n",
        "    cols = [c for c in df.columns if c != 'name']\n",
        "    return df.groupby('name', as_index=False)[cols].max()\n",
        "\n",
        "def prune_present(df, off, deff, dual):\n",
        "    dual_present = [p for p in dual if p in df.columns]\n",
        "    off_base = [p for p in off if p in df.columns and p not in dual_present]\n",
        "    def_base = [p for p in deff if p in df.columns and p not in dual_present]\n",
        "    pcols = sorted(set(off_base + def_base + dual_present))\n",
        "    return off_base, def_base, dual_present, pcols\n",
        "\n",
        "def compute_opr_sdr(df, off_base, def_base, dual_w, pcols):\n",
        "    out = df[['name']].copy()\n",
        "    opr = df[off_base].fillna(0).sum(axis=1) if off_base else 0.0\n",
        "    sdr = df[def_base].fillna(0).sum(axis=1) if def_base else 0.0\n",
        "    for col in [c for c in dual_w if c in df.columns]:\n",
        "        opr = opr + dual_w[col]['OPR'] * df[col].fillna(0)\n",
        "        sdr = sdr + dual_w[col]['SDR'] * df[col].fillna(0)\n",
        "    out['OPR'] = opr\n",
        "    out['SDR'] = sdr\n",
        "    out['has_powers_source'] = False\n",
        "    if pcols:\n",
        "        out['has_powers_source'] = ~df[pcols].isna().all(axis=1)\n",
        "    return out\n",
        "\n",
        "def merge_and_save(info_df, ratings, out_file):\n",
        "    res = info_df.merge(ratings, on='name', how='left')\n",
        "    res['has_powers_source'] = (res['has_powers_source']\n",
        "                                .fillna(False).astype('int8'))\n",
        "    res.to_csv(out_file, index=False)\n",
        "    print(f\"Saved {out_file} (has_powers_source: 1=yes, 0=no).\")\n",
        "    return res\n"
      ],
      "metadata": {
        "id": "q7zCbsNk7luH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPi5-a7TwO0S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Opt in to future behavior to avoid downcasting warnings\n",
        "pd.set_option('future.no_silent_downcasting', True)\n",
        "\n",
        "# Load\n",
        "info_df = pd.read_csv(SUPERHEROES_INFO_CLEAN_URL)\n",
        "powers_df = pd.read_csv(SUPERHEROES_POWERS_URL)\n",
        "\n",
        "# Prep\n",
        "powers_df = normalize_join_key(powers_df)\n",
        "info_df = dedupe_info(info_df)\n",
        "powers_df = aggregate_powers(powers_df)\n",
        "\n",
        "# Power spec\n",
        "OFFENSIVE_POWERS = [\n",
        "    'Super Strength','Energy Blasts','Weapons Master','Marksmanship',\n",
        "    'Telekinesis','Cryokinesis','Fire Control','Power Augmentation',\n",
        "    'Animal Oriented Powers','Super Speed'\n",
        "]\n",
        "DEFENSIVE_POWERS = [\n",
        "    'Durability','Invulnerability','Force Fields','Energy Absorption',\n",
        "    'Regeneration','Immortality','Camouflage','Phasing',\n",
        "    'Enhanced Senses','Teleportation'\n",
        "]\n",
        "DUAL_WEIGHTS = {\n",
        "    'Magic': {'OPR': 0.7, 'SDR': 0.3},\n",
        "    'Super Speed': {'OPR': 0.7, 'SDR': 0.3},\n",
        "}\n",
        "\n",
        "# Build features\n",
        "off_base, def_base, dual_present, pcols = prune_present(\n",
        "    powers_df, OFFENSIVE_POWERS, DEFENSIVE_POWERS, DUAL_WEIGHTS\n",
        ")\n",
        "ratings = compute_opr_sdr(\n",
        "    powers_df, off_base, def_base, DUAL_WEIGHTS, pcols\n",
        ")\n",
        "\n",
        "# Merge and save (writes has_powers_source as 1/0)\n",
        "info_with_ratings = merge_and_save(info_df, ratings, INFO_POWERS_FILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhkq2nAVLDMp"
      },
      "source": [
        "### Listing 2-8A: OPR/SDR Sanity Test Suite\n",
        "Validates OPR/SDR by checking coverage, face validity, consistency vs.\n",
        "source powers, redundancy, sensitivity to small changes, outliers, and\n",
        "simple leakage. Assumes constants from Listing 2-9 are in memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-0UPSnXLHvK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Require constants from Listing 2-9\n",
        "# Load enriched info+powers produced by Listing 2-9\n",
        "#try:\n",
        "#    df = pd.read_csv(INFO_POWERS_FILE)\n",
        "#except Exception:\n",
        "df = pd.read_csv('superheroes_info_powers.csv')\n",
        "\n",
        "# Load raw powers for cross-checks\n",
        "pow_df = pd.read_csv(SUPERHEROES_POWERS_URL)\n",
        "if 'hero_names' in pow_df.columns and 'name' not in pow_df.columns:\n",
        "    pow_df = pow_df.rename(columns={'hero_names': 'name'})\n",
        "\n",
        "# Buckets present in this file\n",
        "present_off = [p for p in OFFENSIVE_POWERS if p in pow_df.columns]\n",
        "present_def = [p for p in DEFENSIVE_POWERS if p in pow_df.columns]\n",
        "present_dual = [p for p in DUAL_WEIGHTS if p in pow_df.columns]\n",
        "off_base = [p for p in present_off if p not in present_dual]\n",
        "def_base = [p for p in present_def if p not in present_dual]\n",
        "\n",
        "# 1) Coverage and descriptives\n",
        "print(\"OPR/SDR summary:\")\n",
        "print(df[['OPR','SDR']].describe().T[['count','mean','std','min','max']], \"\\n\")\n",
        "miss = df[['OPR','SDR']].isna().mean().mul(100).rename('missing_%')\n",
        "print(\"Missing values (%):\\n\", miss.to_frame(), \"\\n\")\n",
        "\n",
        "# 2) Face validity (top/bottom)\n",
        "def peek(dfin, col, k=10):\n",
        "    cols = [c for c in ['name','Publisher','Alignment','Species','OPR','SDR']\n",
        "            if c in dfin.columns]\n",
        "    print(f\"Top {k} by {col}:\")\n",
        "    print(dfin[cols].sort_values(col, ascending=False).head(k)\n",
        "          .to_string(index=False), \"\\n\")\n",
        "    print(f\"Bottom {k} by {col}:\")\n",
        "    print(dfin[cols].sort_values(col, ascending=True).head(k)\n",
        "          .to_string(index=False), \"\\n\")\n",
        "\n",
        "peek(df, 'OPR', 10)\n",
        "peek(df, 'SDR', 10)\n",
        "\n",
        "# 3) Consistency vs. source flags\n",
        "src = pow_df.set_index('name')\n",
        "have_off = (src[off_base + present_dual].fillna(0).sum(axis=1) > 0\n",
        "            if (off_base or present_dual) else pd.Series(False, index=src.index))\n",
        "have_def = (src[def_base + present_dual].fillna(0).sum(axis=1) > 0\n",
        "            if (def_base or present_dual) else pd.Series(False, index=src.index))\n",
        "\n",
        "dfi = df.set_index('name')\n",
        "\n",
        "only_dual_off = ((src[present_dual].fillna(0).sum(axis=1) > 0) &\n",
        "                 (src[off_base].fillna(0).sum(axis=1) == 0))\n",
        "only_dual_def = ((src[present_dual].fillna(0).sum(axis=1) > 0) &\n",
        "                 (src[def_base].fillna(0).sum(axis=1) == 0))\n",
        "\n",
        "viol_opr_strict = dfi.loc[have_off & ~only_dual_off & (dfi['OPR'] < 1)]\n",
        "viol_opr_dual   = dfi.loc[only_dual_off & (dfi['OPR'] < 0.3)]\n",
        "viol_sdr_strict = dfi.loc[have_def & ~only_dual_def & (dfi['SDR'] < 1)]\n",
        "viol_sdr_dual   = dfi.loc[only_dual_def & (dfi['SDR'] < 0.3)]\n",
        "\n",
        "print(f\"OPR violations (strict): {len(viol_opr_strict)}\")\n",
        "print(f\"OPR violations (dual):   {len(viol_opr_dual)}\")\n",
        "print(f\"SDR violations (strict): {len(viol_sdr_strict)}\")\n",
        "print(f\"SDR violations (dual):   {len(viol_sdr_dual)}\\n\")\n",
        "\n",
        "# 4) Redundancy and alignment signal\n",
        "corr = df[['OPR','SDR']].corr(method='pearson').iloc[0,1]\n",
        "print(f\"Correlation(OPR, SDR): {corr:.3f}\")\n",
        "if 'Alignment' in df.columns:\n",
        "    means = df.groupby('Alignment')[['OPR','SDR']].mean().round(2)\n",
        "    counts = df['Alignment'].value_counts()\n",
        "    print(\"\\nMeans by Alignment:\\n\", means)\n",
        "    print(\"\\nCounts by Alignment:\\n\", counts, \"\\n\")\n",
        "\n",
        "# 5) Sensitivity: move one offense power to defense, then re-rank\n",
        "cand = [c for c in off_base if c in pow_df.columns]\n",
        "tweak = cand[0] if cand else None\n",
        "if tweak:\n",
        "    t = pow_df.copy()\n",
        "    cols = [*off_base, *def_base, *present_dual]\n",
        "    t[cols] = t[cols].fillna(0)\n",
        "    off_t = [c for c in off_base if c != tweak]\n",
        "    def_t = def_base + [tweak]\n",
        "    t['OPR_tmp'] = t[off_t].sum(axis=1)\n",
        "    t['SDR_tmp'] = t[def_t].sum(axis=1)\n",
        "    for col in present_dual:\n",
        "        w = DUAL_WEIGHTS[col]\n",
        "        if col in t.columns:\n",
        "            t['OPR_tmp'] += w['OPR'] * t[col]\n",
        "            t['SDR_tmp'] += w['SDR'] * t[col]\n",
        "    tmp = dfi[['OPR','SDR']].merge(t[['OPR_tmp','SDR_tmp']],\n",
        "                                   left_index=True, right_index=True,\n",
        "                                   how='inner')\n",
        "    if tmp['OPR'].nunique() > 1 and tmp['OPR_tmp'].nunique() > 1:\n",
        "        rho_opr = tmp['OPR'].corr(tmp['OPR_tmp'], method='spearman')\n",
        "        print(f\"Sensitivity Spearman(OPR vs OPR_tmp): {rho_opr:.3f}\")\n",
        "    if tmp['SDR'].nunique() > 1 and tmp['SDR_tmp'].nunique() > 1:\n",
        "        rho_sdr = tmp['SDR'].corr(tmp['SDR_tmp'], method='spearman')\n",
        "        print(f\"Sensitivity Spearman(SDR vs SDR_tmp): {rho_sdr:.3f}\\n\")\n",
        "else:\n",
        "    print(\"Sensitivity test skipped (no suitable offensive power).\\n\")\n",
        "\n",
        "# 6) Outliers and a robust cosmic flag\n",
        "q99_opr = df['OPR'].quantile(0.99)\n",
        "q99_sdr = df['SDR'].quantile(0.99)\n",
        "df['cosmic_flag'] = (df['OPR'] >= q99_opr) | (df['SDR'] >= q99_sdr)\n",
        "print(\"Cosmic-flagged heroes:\", int(df['cosmic_flag'].sum()))\n",
        "if df['cosmic_flag'].any():\n",
        "    print(df.loc[df['cosmic_flag'], ['name','OPR','SDR']]\n",
        "          .head(15).to_string(index=False), \"\\n\")\n",
        "\n",
        "# 7) Simple leakage heuristic\n",
        "maybe = [c for c in ['Good','Evil','Omniscient','Omnipotent']\n",
        "         if c in pow_df.columns]\n",
        "print(\"Potentially leaky power columns:\", maybe if maybe else \"None\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8mc8decK-Fx"
      },
      "source": [
        "### Listing 2-9: Generate Story Plot Dataset  \n",
        "This code generates superhero plot summaries using randomized archetypes and a Hugging Face language model, then saves the results to a CSV file for analysis or reuse.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABiFxbVRK9YL"
      },
      "outputs": [],
      "source": [
        "import os, re, random, pandas as pd\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Config\n",
        "NUM_PLOTS = 5\n",
        "TEMP = 0.4\n",
        "MODEL = \"gpt-4o-mini\"  # fast, capable default\n",
        "\n",
        "# LLM and prompt\n",
        "llm = ChatOpenAI(model=MODEL, temperature=TEMP)\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Write a concise superhero plot. Include a title and three beats.\"),\n",
        "    (\"user\",\n",
        "     \"Hero: {hero}\\nVillain: {villain}\\nConflict: {conflict}\\nSetting: {setting}\")\n",
        "])\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Archetype pools\n",
        "heroes = [\"reluctant hero\",\"outcast\",\"chosen one\",\"antihero\",\"AI guardian\",\n",
        "          \"alien protector\",\"mystic sage\",\"time traveler\",\"reformed villain\"]\n",
        "villains = [\"mastermind\",\"rogue AI\",\"dark sorcerer\",\"doppelganger\",\"void entity\",\n",
        "            \"time manipulator\",\"cosmic tyrant\",\"corrupt politician\"]\n",
        "conflicts = [\"personal vendetta\",\"cosmic invasion\",\"identity crisis\",\n",
        "             \"magic vs science\",\"city under siege\",\"revenge plot\"]\n",
        "settings = [\"futuristic city\",\"space station\",\"hidden temple\",\"cyberpunk sprawl\",\n",
        "            \"parallel universe\",\"post-apocalyptic wasteland\"]\n",
        "\n",
        "# Generate\n",
        "rows = []\n",
        "for i in range(NUM_PLOTS):\n",
        "    args = dict(hero=random.choice(heroes),\n",
        "                villain=random.choice(villains),\n",
        "                conflict=random.choice(conflicts),\n",
        "                setting=random.choice(settings))\n",
        "    text = chain.invoke(args)\n",
        "    text = re.sub(r\"\\s*\\n\\s*\", \" | \", text.strip())\n",
        "    rows.append({**{k.title(): v for k, v in args.items()}, \"Plot\": text})\n",
        "    print(f\"Plot {i+1}: {text[:120]}...\")\n",
        "\n",
        "# Save\n",
        "pd.DataFrame(rows).to_csv(PLOTS_FILE, index=False)\n",
        "print(f\"Saved {PLOTS_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ihVdlvDH9iv"
      },
      "source": [
        "### Listing 2-10 Comic Story Assistant with RAG\n",
        "Showcases a Retrieval-Augmented Generation (RAG) approach that uses ChromaDB with hero plot and power data to dynamically generate tailored superhero story arcs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyaq2bXmIGce"
      },
      "outputs": [],
      "source": [
        "%pip install -q chromadb langchain_chroma langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glItxfhuH9G6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, random\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Load data\n",
        "plots = pd.read_csv(SUPERHEROES_INFO_PLOTS_URL)\n",
        "heroes = pd.read_csv(SUPERHEROES_INFO_POWERS_URL)\n",
        "\n",
        "# Vector store from plot texts\n",
        "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "db = Chroma.from_texts(plots['Plot'].astype(str).tolist(), emb)\n",
        "\n",
        "# Writer intent and retrieval\n",
        "query = (\"Time-traveling hero vs scientific mastermind in a futuristic city.\")\n",
        "doc = db.similarity_search(query, k=1)[0].page_content\n",
        "\n",
        "# Pick hero/villain with valid power scores\n",
        "good = heroes.query(\"Alignment == 'good'\").dropna(subset=['OPR','SDR'])\n",
        "bad  = heroes.query(\"Alignment == 'bad'\").dropna(subset=['OPR','SDR'])\n",
        "h, v = good.sample(1).iloc[0], bad.sample(1).iloc[0]\n",
        "\n",
        "# Prompt and model\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Write a clear superhero story in 8–12 sentences.\"),\n",
        "    (\"user\",\n",
        "     \"Plot outline:\\n{plot}\\n\\n\"\n",
        "     \"Hero: {h} (OPR {ho:.1f}, SDR {hs:.1f}).\\n\"\n",
        "     \"Villain: {v} (OPR {vo:.1f}, SDR {vs:.1f}).\\n\"\n",
        "     \"Rewrite the story with these roles. Build tension and a final showdown.\")\n",
        "])\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "chain = prompt | llm\n",
        "\n",
        "# Generate\n",
        "resp = chain.invoke({\n",
        "    \"plot\": doc, \"h\": h['name'], \"ho\": h['OPR'], \"hs\": h['SDR'],\n",
        "    \"v\": v['name'], \"vo\": v['OPR'], \"vs\": v['SDR']\n",
        "})\n",
        "print(resp.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n1tjw_Ca6MB"
      },
      "source": [
        "### Listing 2-11: Pseudonymizing Superhero Plots Using SpaCy\n",
        "Demonstrates pseudonymization of entities within superhero plots, using spaCy to replace names, organizations, and locations with generic terms for privacy. **Note:** Be sure to run the *pip* install before running the code snippet below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqZli752bjYa"
      },
      "outputs": [],
      "source": [
        "%pip install -q spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znYuXOTDbDpA"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy's English model for entity recognition\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to pseudonymize entity names in the plot text\n",
        "def pseudonymize_entities(text):\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        # Replace detected entities with generic labels\n",
        "        if ent.label_ == \"PERSON\":\n",
        "            text = text.replace(ent.text, 'Hero A')\n",
        "        elif ent.label_ == \"ORG\":\n",
        "            text = text.replace(ent.text, 'Organization X')\n",
        "        elif ent.label_ == \"GPE\":\n",
        "            text = text.replace(ent.text, 'Location Z')\n",
        "    return text\n",
        "\n",
        "# Pseudonymize only the first plot\n",
        "first_plot = plots['Plot'].iloc[0]\n",
        "pseudonymized_first_plot = pseudonymize_entities(first_plot)\n",
        "\n",
        "# Display the pseudonymized first plot\n",
        "print(\"Pseudonymized Plot:\")\n",
        "print(pseudonymized_first_plot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2nujz0xyZ-V"
      },
      "source": [
        "### Listing 2-12: Data Masking And Differential Privacy\n",
        "Demonstrates data masking and differential privacy on health records by masking phone numbers and adding noise to age values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5udkNUxydfP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset: health records\n",
        "data = pd.DataFrame({\n",
        "    'patient_id': ['A123', 'B456', 'C789'],\n",
        "    'phone': ['123-456-7890', '987-654-3210', '555-123-4567'],\n",
        "    'age': [29, 47, 35],\n",
        "    'diagnosis': ['Condition A', 'Condition B', 'Condition A']\n",
        "})\n",
        "\n",
        "# Data Masking: Mask all but the last four digits of phone numbers\n",
        "data['masked_phone'] = data['phone'].apply(lambda x: 'XXX-XXX-' + x[-4:])\n",
        "\n",
        "# Differential Privacy: Add noise to age values for anonymization\n",
        "noise_level = 2  # Adjust noise level as needed\n",
        "data['age_noisy'] = data['age'] + np.random.laplace(0, noise_level, len(data))\n",
        "\n",
        "# Display the modified dataset\n",
        "print(\"Anonymized Data:\\n\", data[['patient_id', 'masked_phone',\n",
        "                                 'age_noisy', 'diagnosis']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAFVqS5tsRPT"
      },
      "source": [
        "### Listing 2-13: Encrypting Sensitive Data\n",
        "This code encrypts a sensitive dataset with **Fernet**, allowing secure decryption and access for authorized users only.\n",
        "**Note:** Be sure to run the following *pip install*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji_uj2aIsZBC"
      },
      "outputs": [],
      "source": [
        "%pip -q install cryptography"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnPG9CbjsV8V"
      },
      "outputs": [],
      "source": [
        "#Import Fernet\n",
        "from cryptography.fernet import Fernet\n",
        "import pandas as pd\n",
        "import io  # Import io for StringIO\n",
        "\n",
        "# Generate encryption key and create a cipher suite\n",
        "key = Fernet.generate_key()\n",
        "cipher_suite = Fernet(key)\n",
        "\n",
        "# Sample sensitive data to be encrypted\n",
        "data = pd.DataFrame({\"Patient\": [\"John Doe\", \"Jane Smith\"],\n",
        "                     \"Diagnosis\": [\"Diabetes\", \"Hypertension\"]})\n",
        "data_str = data.to_csv(index=False)\n",
        "encrypted_data = cipher_suite.encrypt(data_str.encode())\n",
        "\n",
        "# Show part of the encrypted string\n",
        "print(\"Encrypted Data (partial):\", encrypted_data[:50], \"...\")\n",
        "\n",
        "# Decrypt the data when access is needed\n",
        "decrypted_data_str = cipher_suite.decrypt(encrypted_data).decode()\n",
        "secure_data = pd.read_csv(io.StringIO(decrypted_data_str))\n",
        "\n",
        "print(\"\\nDecrypted Data Accessible (Only by Authorized Users)\\n\", secure_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3aWhUm5iY4x"
      },
      "source": [
        "### Listing 2-14: Generating Synthetic Health Records\n",
        "Uses **Faker** to create fictional health records, providing a safe, realistic dataset structure for AI training or testing applications. **Note**: Be sure to run the following pip install."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7sk7plLij1Y"
      },
      "outputs": [],
      "source": [
        "%pip -q install Faker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB18zN0nid42"
      },
      "outputs": [],
      "source": [
        "from faker import Faker\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize Faker for synthetic data generation\n",
        "fake = Faker()\n",
        "\n",
        "# Generate a synthetic health records dataset\n",
        "data = pd.DataFrame({\n",
        "    'Patient_ID': [fake.uuid4() for _ in range(5)],\n",
        "    'Name': [fake.name() for _ in range(5)],\n",
        "    'Age': [fake.random_int(min=18, max=90) for _ in range(5)],\n",
        "    'Diagnosis': [fake.random_element(elements=('Condition A',\n",
        "                                               'Condition B',\n",
        "                                               'Condition C'))\n",
        "                   for _ in range(5)],\n",
        "    'Phone': [fake.phone_number() for _ in range(5)],\n",
        "    'Address': [fake.address().replace(\"\\n\", \", \") for _ in range(5)],\n",
        "    'Last_Visit': [fake.date_between(start_date='-2y', end_date='today')\n",
        "                   for _ in range(5)]\n",
        "})\n",
        "\n",
        "# Display the synthetic dataset\n",
        "print(data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}