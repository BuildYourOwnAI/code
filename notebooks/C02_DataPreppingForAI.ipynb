{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znQjgeS3JXyf"
   },
   "source": [
    "## Chapter 2 - Prepping Data for AI\n",
    "This notebook contains examples of data preparation strategies for AI, including data cleaning, feature engineering, and handling data sensitivity. Using open-source tools like Pandas, LangChain, and ChromaDB, it explores design patterns for crafting high-quality datasets. It also covers techniques for ensuring data privacy and security, highlighting methods like data masking and synthetic data generation to safeguard sensitive information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr_g5Z0z_AmW"
   },
   "source": [
    "### Listing 2-1: Defining Dataset Constants\n",
    "Defining **dataset** URLs as constants for reuse throughout the notebook, enabling easy loading of superhero datasets across multiple cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gwd5J8n-9RW"
   },
   "outputs": [],
   "source": [
    "# Base GitHub repository URL\n",
    "BASE_URL = \"https://opensourceai-book.github.io/code/datasets/\"\n",
    "\n",
    "# Dataset file names\n",
    "INFO_FILE = \"superheroes_info.csv\"\n",
    "INFO_CLEAN_FILE = \"superheroes_info_cleansed.csv\"\n",
    "POWERS_FILE = \"superheroes_powers.csv\"\n",
    "INFO_POWERS_FILE = \"superheroes_info_powers.csv\"\n",
    "PLOTS_FILE = \"superheroes_story_plots.csv\"\n",
    "\n",
    "# Construct full dataset URLs\n",
    "SUPERHEROES_INFO_URL = f\"{BASE_URL}{INFO_FILE}\"\n",
    "SUPERHEROES_INFO_CLEAN_URL = f\"{BASE_URL}{INFO_CLEAN_FILE}\"\n",
    "SUPERHEROES_POWERS_URL = f\"{BASE_URL}{POWERS_FILE}\"\n",
    "SUPERHEROES_INFO_POWERS_URL = f\"{BASE_URL}{INFO_POWERS_FILE}\"\n",
    "SUPERHEROES_INFO_PLOTS_URL = f\"{BASE_URL}{PLOTS_FILE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Z4hUSWUYylX"
   },
   "source": [
    "**Note:** If you see errors about unset constants, rerun the above block. This can happen if your Python runtime disconnects in Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMI3FCZVAXiL"
   },
   "source": [
    "### Listing 2-2: Loading and Displaying Dataset Stats\n",
    "We load our two superhero-related datasets from provided URLs into a Dataframe and displays basic statistics like number of rows, columns, and column names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAv1HKTkLjx8"
   },
   "outputs": [],
   "source": [
    "# Import our good friend Pandas, home of the Dataframe and Numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Using the constants defined earlier in the dictionary for reuse\n",
    "urls = {\n",
    "    \"Superheroes Info Dataset\": SUPERHEROES_INFO_URL,\n",
    "    \"Superheroes Powers Dataset\": SUPERHEROES_POWERS_URL\n",
    "}\n",
    "\n",
    "# Load datasets and display basic stats\n",
    "for name, url in urls.items():\n",
    "    df = pd.read_csv(url)\n",
    "    print(f\"{name}:\\n  - Rows: {df.shape[0]}\\n  - Columns: {df.shape[1]}\"\n",
    "          f\"\\n  - Column names: {', '.join(df.columns[:5])}...\\n\")\n",
    "    print(\"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY-2IkstYzy6"
   },
   "source": [
    "### Listing 2-3: Analyzing and Detecting Sparse Fields\n",
    "Analyzes the dataset by detecting sparse fields, calculating the percentage of missing data for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJdIT3b2Kgce"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(SUPERHEROES_INFO_URL)\n",
    "\n",
    "# Replace placeholder values '-' and '-99' with NaN for better detection of missing data\n",
    "df.replace('-', np.nan, inplace=True)\n",
    "df.replace(-99, np.nan, inplace=True)\n",
    "\n",
    "# Remove the extraneous counter column (assuming it's the first column)\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Sparse fields assessment: Calculate percentage of missing data for all columns\n",
    "total_entries = len(df)\n",
    "sparse_fields = {col: (df[col].isnull().sum() / total_entries) * 100 for col in df.columns}\n",
    "\n",
    "# Output the necessary information, showing all columns, even with 0% missing data\n",
    "print(\"\\nSparse Fields (Percentage of Missing Data):\")\n",
    "print(\"{:<20} {:>10}\".format(\"Column\", \"Missing %\"))\n",
    "print(\"-\" * 30)\n",
    "for col, percentage in sparse_fields.items():\n",
    "    print(\"{:<20} {:>8.2f}%\".format(col, percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxz2T_Pgf86c"
   },
   "source": [
    "### Listing 2-4: Lanchain Function for Inferring Race\n",
    "Defines a function that use Langchain and an open source model on Hugging Face Hub that queries the LLM for a superhero's introduction year using their name and publisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y0H3GNZB4qVP",
    "outputId": "ce45f399-34db-468c-8058-2512584e354b"
   },
   "outputs": [],
   "source": [
    "# Install required packages for Hugging Face and LangChain usage\n",
    "%pip install -q langchain langchain-community langchain-huggingface\n",
    "\n",
    "# Import os library for environment variable manipulation\n",
    "import os\n",
    "\n",
    "# Set your Hugging Face Hub Access token (replace 'your_token_here'\n",
    "# with your actual token)\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'Your_Token_Here'\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "import re\n",
    "\n",
    "# Initialize the Mistral language model from HuggingFace\n",
    "llm = HuggingFaceEndpoint(repo_id='mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
    "                          temperature=0.2)\n",
    "\n",
    "# Define a prompt template for querying the race via the LLM\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Provide only the superhero race in one word, surrounded by '\n",
    "               'parentheses (). If you donâ€™t know, respond with \"\".'),\n",
    "    ('human', 'What is the race of {hero_name} from {publisher}?')\n",
    "])\n",
    "\n",
    "# Define the chatbot chain where the prompt is sent to the language model\n",
    "chain = prompt | llm\n",
    "\n",
    "# Function to query the LLM for the race and extract the response within\n",
    "# parentheses\n",
    "def get_race_from_llm(hero_name, publisher):\n",
    "    response = chain.invoke({'hero_name': hero_name, 'publisher': publisher})\n",
    "    match = re.search(r'\\((\\w+)\\)', response)  # Extract race from parentheses\n",
    "    return match.group(1) if match else \"\"\n",
    "\n",
    "# Example usage\n",
    "hero_name = \"Spider-Man\"\n",
    "publisher = \"Marvel Comics\"\n",
    "race = get_race_from_llm(hero_name, publisher)\n",
    "print(f\"Race of {hero_name}: {race}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwaPyeUuAk2r"
   },
   "source": [
    "### Listing 2-5: Clean and Normalize Dataset\n",
    "Cleans and normalizes the dataset by removing unnecessary columns, filling missing values, and applying race-based averages (using langchain based program defined in previous listing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5HVTedsApAv"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(SUPERHEROES_INFO_URL)\n",
    "\n",
    "# Step 1: Remove unnecessary columns\n",
    "df.drop(columns=['Unnamed: 0', 'Eye color', 'Hair color', 'Skin color'],\n",
    "        inplace=True)\n",
    "\n",
    "# Step 2: Normalize placeholder values\n",
    "df.replace('-', np.nan, inplace=True)\n",
    "df.replace(-99, np.nan, inplace=True)\n",
    "\n",
    "# Step 3: Use LangChain LLM to fill missing race values\n",
    "for idx, row in df[df['Race'].isna()].iterrows():\n",
    "    race = get_race_from_llm(row['name'], row['Publisher'])\n",
    "    if race:\n",
    "        df.at[idx, 'Race'] = race\n",
    "        print(f\"Filled race for {row['name']}: {race}\")\n",
    "    else:\n",
    "        print(f\"Could not determine race for {row['name']}\")\n",
    "\n",
    "# Step 4: Fill missing height/weight using race averages\n",
    "race_grouped = df.groupby('Race')[['Height', 'Weight']].mean()\n",
    "\n",
    "for race in race_grouped.index:\n",
    "    avg_height = race_grouped.loc[race, 'Height']\n",
    "    avg_weight = race_grouped.loc[race, 'Weight']\n",
    "    df.loc[(df['Race'] == race) & (df['Height'].isnull()), 'Height'] = avg_height\n",
    "    df.loc[(df['Race'] == race) & (df['Weight'].isnull()), 'Weight'] = avg_weight\n",
    "\n",
    "# Save cleansed data to CSV\n",
    "df.to_csv('superheroes_info_cleansed.csv', index=False)\n",
    "\n",
    "# Output sample of cleaned dataset\n",
    "print(df[['name', 'Race', 'Height', 'Weight']].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS4TDW8bRsKF"
   },
   "source": [
    "### Listing 2-6: Calculating Quality with Gini Coefficient\n",
    "We calculate the **Gini coefficient** for the \"Alignment\" column to assess imbalance between categories, helping us evaluate potential skew in model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BT8XpY8R7Tz"
   },
   "outputs": [],
   "source": [
    "# Import our favorite data minipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Superheroes Info dataset\n",
    "df_info = pd.read_csv(SUPERHEROES_INFO_URL)\n",
    "\n",
    "# Function to calculate the Gini coefficient for a given distribution of counts\n",
    "def gini_coefficient(counts):\n",
    "    sorted_counts = np.sort(counts)  # Sort counts in ascending order\n",
    "    n = len(counts)  # Number of counts\n",
    "    cumulative_values = np.cumsum(sorted_counts)  # Cumulative sum of sorted counts\n",
    "    index = np.arange(1, n + 1)  # Indices for Gini calculation\n",
    "    # Calculate Gini coefficient using the formula\n",
    "    gini = (np.sum((2 * index - n - 1) * sorted_counts)) / (\n",
    "        n * np.sum(sorted_counts)\n",
    "    )\n",
    "    return gini\n",
    "\n",
    "# Count occurrences of each alignment category (good, bad, neutral) after removing '-'\n",
    "alignment_counts = df_info_filtered['Alignment'].value_counts()\n",
    "\n",
    "# Calculate Gini coefficient for the Alignment column\n",
    "gini_score = gini_coefficient(alignment_counts.values)\n",
    "\n",
    "# Display the counts and Gini coefficient\n",
    "print(\"Alignment Counts (excluding blanks):\\n\", alignment_counts)\n",
    "print(f\"Gini Coefficient for 'Alignment' categories: {gini_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wh7D4fnBgA-o"
   },
   "source": [
    "### Listing 2-7: Data Relevance Using EDA\n",
    "Analyzes average height, weight, and moral alignment proportions by gender, then formats and prints an easy-to-read table for data relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "805d2lNeDi7u"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(SUPERHEROES_INFO_CLEAN_URL)\n",
    "\n",
    "# Filter for missing values in key columns\n",
    "df = df.dropna(subset=['Gender', 'Alignment', 'Race', 'Height', 'Weight'])\n",
    "\n",
    "# Analyze imbalance across categorical columns\n",
    "categories = ['Gender', 'Alignment', 'Race']\n",
    "\n",
    "gini_results = {}\n",
    "for category in categories:\n",
    "    counts = df[category].value_counts()\n",
    "    gini_results[category] = gini_coefficient(counts.values)\n",
    "\n",
    "# Analyze imbalance for discretized height and weight\n",
    "df['Height_bins'] = pd.cut(df['Height'], bins=5)\n",
    "df['Weight_bins'] = pd.cut(df['Weight'], bins=5)\n",
    "\n",
    "gini_results['Height'] = gini_coefficient(df['Height_bins'].value_counts().values)\n",
    "gini_results['Weight'] = gini_coefficient(df['Weight_bins'].value_counts().values)\n",
    "\n",
    "# Print Gini coefficients for each category\n",
    "print(\"Gini Coefficients for Dataset Imbalances:\")\n",
    "for category, gini_score in gini_results.items():\n",
    "    print(f\"{category}: {gini_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jkf7MbqYtLuA"
   },
   "source": [
    "### Listing 2-8: Superhero Dataset Merge Analysis\n",
    "Analyze compatibility of superheroes_info_clean and superheroes_powers by merging on hero_names field and calculating match percentage for **feature integration depth**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_8DbU9xIQCD"
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets from the URLs\n",
    "info_df = pd.read_csv(SUPERHEROES_INFO_CLEAN_URL)\n",
    "powers_df = pd.read_csv(SUPERHEROES_POWERS_URL)\n",
    "\n",
    "# Rename 'name' in info_df to 'hero_names' for consistent merging\n",
    "info_df.rename(columns={'name': 'hero_names'}, inplace=True)\n",
    "\n",
    "# Merge the datasets on 'hero_names'\n",
    "merged_df = pd.merge(info_df, powers_df, on='hero_names', how='inner')\n",
    "\n",
    "# Calculate and display the total number of matched entries and the percentage match\n",
    "matched_count = merged_df.shape[0]\n",
    "total_info_count = info_df.shape[0]\n",
    "percentage_matched = (matched_count / total_info_count) * 100\n",
    "\n",
    "print(f\"Matched entries: {matched_count}\")\n",
    "print(f\"Total entries in Info dataset: {total_info_count}\")\n",
    "print(f\"Percentage matched: {percentage_matched:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JyZOQa3uJeD"
   },
   "source": [
    "### Listing 2-9: Hero Power Metrics Enhancement\n",
    "Calculates Offensive Power Rating (OPR) and Strategic Defense Rating (SDR) for each hero by summing relevant power attributes, enriching the dataset for enhanced power analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPi5-a7TwO0S"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "info_dfs = pd.read_csv(SUPERHEROES_INFO_CLEAN_URL)\n",
    "powers_dfs = pd.read_csv(SUPERHEROES_POWERS_URL)\n",
    "\n",
    "# Ensure consistent naming by renaming 'hero_names' to 'name' in powers_dfs\n",
    "powers_dfs.rename(columns={'hero_names': 'name'}, inplace=True)\n",
    "\n",
    "# Define power attributes for OPR and SDR calculations\n",
    "OFFENSIVE_POWERS = [\n",
    "    'Super Strength', 'Energy Blasts', 'Weapons Master', 'Telekinesis'\n",
    "]\n",
    "DEFENSIVE_POWERS = [\n",
    "    'Invulnerability', 'Energy Absorption', 'Teleportation', 'Force Fields'\n",
    "]\n",
    "\n",
    "# Calculate OPR and SDR in the powers dataset\n",
    "powers_dfs['OPR'] = powers_dfs[OFFENSIVE_POWERS].sum(axis=1)\n",
    "powers_dfs['SDR'] = powers_dfs[DEFENSIVE_POWERS].sum(axis=1)\n",
    "\n",
    "# Keep only necessary columns (name, OPR, SDR)\n",
    "powers_ratings = powers_dfs[['name', 'OPR', 'SDR']]\n",
    "\n",
    "# Merge the calculated fields into the info dataset\n",
    "info_with_ratings = pd.merge(info_dfs, powers_ratings, on='name', how='left')\n",
    "\n",
    "# Save the updated info dataset with OPR and SDR\n",
    "info_with_ratings.to_csv(INFO_POWERS_FILE, index=False)\n",
    "print(f\"Updated dataset saved as {INFO_POWERS_FILE} with OPR and SDR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8mc8decK-Fx"
   },
   "source": [
    "### Listing 2-10: Generate Story Plot Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABiFxbVRK9YL"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for building a LangChain chatbot\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "import pandas as pd\n",
    "import random  # For random selection\n",
    "\n",
    "# Configuration\n",
    "LANGUAGE_MODEL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"  # Hugging Face model\n",
    "TEMP = 0.4  # Temperature for varied responses\n",
    "NUM_PLOT_SAMPLES = 5  # Number of plot samples to generate for testing\n",
    "\n",
    "# Initialize the language model\n",
    "llm = HuggingFaceEndpoint(repo_id=LANGUAGE_MODEL, temperature=TEMP)\n",
    "\n",
    "# Define a prompt template for generating superhero plots\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Generate a concise superhero plot where:'),\n",
    "    ('human', '''\n",
    "    - Hero archetype: {hero}\n",
    "    - Villain archetype: {villain}\n",
    "    - Conflict type: {conflict_type}\n",
    "    - Setting: {setting}\n",
    "    Include a title and three key plot points.\n",
    "    ''')\n",
    "])\n",
    "\n",
    "# Define the plot generation chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Expanded components for plot generation\n",
    "heroes = [\n",
    "    \"reluctant hero\", \"outcast\", \"chosen one\", \"antihero\", \"reluctant mentor\",\n",
    "    \"alien protector\", \"cyber-enhanced rebel\", \"fallen warrior\", \"mystic sage\",\n",
    "    \"time-traveling guardian\", \"reformed villain\", \"empathic healer\",\n",
    "    \"street-level hero\", \"artificial intelligence (AI)\", \"mystical guardian\",\n",
    "    \"scientific genius\", \"nature warrior\", \"cosmic nomad\", \"avenger of loss\",\n",
    "    \"dimension traveler\", \"energy manipulator\"\n",
    "]\n",
    "\n",
    "villains = [\n",
    "    \"mastermind\", \"alien invader\", \"corrupt hero\", \"rogue AI\", \"dark sorcerer\",\n",
    "    \"doppelgÃ¤nger\", \"genetic experiment\", \"shadow manipulator\", \"void entity\",\n",
    "    \"cyber overlord\", \"time manipulator\", \"mythical creature\", \"corrupt politician\",\n",
    "    \"revenge-seeking nemesis\", \"intergalactic tyrant\", \"poisonous rival\",\n",
    "    \"mind controller\", \"techno-terrorist\", \"corrupted scientist\", \"necromancer\",\n",
    "    \"cosmic parasite\"\n",
    "]\n",
    "\n",
    "conflicts = [\n",
    "    \"personal vendetta\", \"cosmic invasion\", \"technological catastrophe\", \"race against time\",\n",
    "    \"internal struggle\", \"resource battle\", \"power corruption\", \"revenge plot\",\n",
    "    \"city under siege\", \"reality manipulation\", \"identity crisis\", \"magic vs. science\",\n",
    "    \"clash of ideals\", \"control of the mind\", \"forbidden knowledge\", \"mutant uprising\",\n",
    "    \"alien invasion\", \"technological sabotage\", \"environmental disaster\",\n",
    "    \"lost relic chase\", \"temporal war\"\n",
    "]\n",
    "\n",
    "settings = [\n",
    "    \"futuristic city\", \"ancient ruins\", \"space station\", \"post-apocalyptic wasteland\",\n",
    "    \"underwater fortress\", \"cyberpunk metropolis\", \"forest of lost souls\",\n",
    "    \"deserted asylum\", \"alien planet\", \"hidden temple\", \"high-tech lab\",\n",
    "    \"floating island\", \"deep jungle\", \"mystic caves\", \"parallel universe\",\n",
    "    \"underground society\", \"cloud city\", \"dreamscape\", \"hollow mountain\",\n",
    "    \"time-frozen world\", \"ruins of civilization\"\n",
    "]\n",
    "\n",
    "# Generate a set number of plot samples for testing with random selection\n",
    "plot_samples = []\n",
    "for i in range(NUM_PLOT_SAMPLES):\n",
    "    # Randomly select a hero, villain, conflict, and setting for each plot\n",
    "    hero = random.choice(heroes)\n",
    "    villain = random.choice(villains)\n",
    "    conflict = random.choice(conflicts)\n",
    "    setting = random.choice(settings)\n",
    "\n",
    "    # Generate a plot with the chosen elements\n",
    "    response = chain.invoke({\n",
    "        'hero': hero,\n",
    "        'villain': villain,\n",
    "        'conflict_type': conflict,\n",
    "        'setting': setting\n",
    "    })\n",
    "\n",
    "    # Replace newline characters in the plot response with a separator (|) for CSV compatibility\n",
    "    response_cleaned = response.replace(\"\\n\", \" | \")\n",
    "\n",
    "    # Store the generated plot in a dictionary\n",
    "    plot_samples.append({\n",
    "        \"Hero\": hero,\n",
    "        \"Villain\": villain,\n",
    "        \"Conflict Type\": conflict,\n",
    "        \"Setting\": setting,\n",
    "        \"Plot\": response_cleaned\n",
    "    })\n",
    "\n",
    "    # Print the generated plot\n",
    "    print(f\"\\nPlot {i + 1}:\")\n",
    "    print(f\"Hero: {hero}\")\n",
    "    print(f\"Villain: {villain}\")\n",
    "    print(f\"Conflict Type: {conflict}\")\n",
    "    print(f\"Setting: {setting}\")\n",
    "    print(f\"Plot:\\n{response}\")\n",
    "\n",
    "# Save the plot samples to a CSV\n",
    "df = pd.DataFrame(plot_samples)\n",
    "df.to_csv(PLOTS_FILE, index=False)\n",
    "print(f\"\\nPlot samples saved to {PLOTS_FILE}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ihVdlvDH9iv"
   },
   "source": [
    "### Listing 2-11 Comic Story Assistant with RAG\n",
    "Showcases a Retrieval-Augmented Generation (RAG) approach that uses ChromaDB with hero plot and power data to dynamically generate tailored superhero story arcs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyaq2bXmIGce"
   },
   "outputs": [],
   "source": [
    "%pip install -q langchain langchain-community langchain-huggingface langchain-openai chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSqaLSBuKPY3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Set OpenAI key for accessing GPT-3 and GPT-3 models (paid account)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Your_Token_Here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glItxfhuH9G6"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load datasets from URLs\n",
    "plot_data = pd.read_csv(SUPERHEROES_INFO_PLOTS_URL)\n",
    "hero_data = pd.read_csv(SUPERHEROES_INFO_POWERS_URL)\n",
    "\n",
    "# Prepare plot data for embeddings\n",
    "plot_texts = plot_data['Plot'].tolist()\n",
    "\n",
    "# Initialize embeddings model and create Chroma vector database for plot data\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma.from_texts(plot_texts, embeddings)\n",
    "\n",
    "# Define query from the comic writer\n",
    "query_text = (\"Show me a story where a time-traveling hero faces a scientific \"\n",
    "              \"mastermind in a futuristic city.\")\n",
    "\n",
    "# Retrieve the most relevant plot based on similarity\n",
    "results = db.similarity_search(query_text, 1)\n",
    "selected_plot = results[0].page_content\n",
    "\n",
    "# Randomly select hero and villain from hero data (filtered by alignment)\n",
    "hero_row = hero_data[hero_data['Alignment'] == 'good'].sample(1).iloc[0]\n",
    "villain_row = hero_data[hero_data['Alignment'] == 'bad'].sample(1).iloc[0]\n",
    "\n",
    "# Extract hero and villain details for prompt\n",
    "hero_name, hero_opr, hero_sdr = hero_row['name'], hero_row['OPR'], hero_row['SDR']\n",
    "villain_name, villain_opr, villain_sdr = villain_row['name'], villain_row['OPR'], villain_row['SDR']\n",
    "\n",
    "# Define prompt template for story creation\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Generate a superhero story based on the following details:\"),\n",
    "    (\"human\", '''\n",
    "    Plot Outline: {plot}\n",
    "\n",
    "    Hero: {hero} with an offense power of {hero_opr}x and defense of {hero_sdr}x.\n",
    "    Villain: {villain} with an offense power of {villain_opr}x and defense\n",
    "    of {villain_sdr}x.\n",
    "\n",
    "    Rewrite the story with {hero} as the main hero and {villain} as the main villain.\n",
    "    Build tension, introduce challenges, and create a climactic final showdown.\n",
    "    ''')\n",
    "])\n",
    "\n",
    "print(f\"\"\"Hero: {hero_name} with OPR of {hero_opr} and defense of {hero_sdr}x.\n",
    "Villain: {villain_name} with OPR of {villain_opr}x and defense of {villain_sdr}x.\n",
    "\"\"\")\n",
    "\n",
    "# Set up LangChain LLM for generating story arcs\n",
    "llm = ChatOpenAI(temperature=0.2)\n",
    "chain = prompt | llm\n",
    "\n",
    "# Invoke the LLM with structured prompt\n",
    "response = chain.invoke({\n",
    "    \"plot\": selected_plot,\n",
    "    \"hero\": hero_name,\n",
    "    \"villain\": villain_name,\n",
    "    \"hero_opr\": hero_opr,\n",
    "    \"hero_sdr\": hero_sdr,\n",
    "    \"villain_opr\": villain_opr,\n",
    "    \"villain_sdr\": villain_sdr\n",
    "})\n",
    "\n",
    "# Display the generated story arc\n",
    "print(\"Generated Story Arc:\")\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8n1tjw_Ca6MB"
   },
   "source": [
    "### Listing 2-12: Pseudonymizing Superhero Plots Using SpaCy\n",
    "Demonstrates pseudonymization of entities within superhero plots, using spaCy to replace names, organizations, and locations with generic terms for privacy. **Note:** Be sure to run the *pip* install before running the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqZli752bjYa"
   },
   "outputs": [],
   "source": [
    "%pip install -q spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znYuXOTDbDpA",
    "outputId": "ae9a71a2-7d4a-42dc-93ca-90a4b5c62f70"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load spaCy's English model for entity recognition\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Example dataset with superhero story plots\n",
    "plot_data = pd.read_csv(SUPERHEROES_INFO_PLOTS_URL)\n",
    "\n",
    "# Function to pseudonymize entity names in the plot text\n",
    "def pseudonymize_entities(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        # Replace detected entities with generic labels\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            text = text.replace(ent.text, 'Hero A')\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            text = text.replace(ent.text, 'Organization X')\n",
    "        elif ent.label_ == \"GPE\":\n",
    "            text = text.replace(ent.text, 'Location Z')\n",
    "    return text\n",
    "\n",
    "# Pseudonymize only the first plot\n",
    "first_plot = plot_data['Plot'].iloc[0]\n",
    "pseudonymized_first_plot = pseudonymize_entities(first_plot)\n",
    "\n",
    "# Display the pseudonymized first plot\n",
    "print(\"Pseudonymized Plot:\")\n",
    "print(pseudonymized_first_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2nujz0xyZ-V"
   },
   "source": [
    "### Listing 2-13: Data Masking And Differential Privacy\n",
    "Demonstrates data masking and differential privacy on health records by masking phone numbers and adding noise to age values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w5udkNUxydfP",
    "outputId": "ddb2247a-8596-45f5-b738-6a805dea61e7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset: health records\n",
    "data = pd.DataFrame({\n",
    "    'patient_id': ['A123', 'B456', 'C789'],\n",
    "    'phone': ['123-456-7890', '987-654-3210', '555-123-4567'],\n",
    "    'age': [29, 47, 35],\n",
    "    'diagnosis': ['Condition A', 'Condition B', 'Condition A']\n",
    "})\n",
    "\n",
    "# Data Masking: Mask all but the last four digits of phone numbers\n",
    "data['masked_phone'] = data['phone'].apply(lambda x: 'XXX-XXX-' + x[-4:])\n",
    "\n",
    "# Differential Privacy: Add noise to age values for anonymization\n",
    "noise_level = 2  # Adjust noise level as needed\n",
    "data['age_noisy'] = data['age'] + np.random.laplace(0, noise_level, len(data))\n",
    "\n",
    "# Display the modified dataset\n",
    "print(\"Anonymized Data:\\n\", data[['patient_id', 'masked_phone',\n",
    "                                 'age_noisy', 'diagnosis']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAFVqS5tsRPT"
   },
   "source": [
    "### Listing 2-14: Encrypting Sensitive Data\n",
    "This code encrypts a sensitive dataset with **Fernet**, allowing secure decryption and access for authorized users only.\n",
    "**Note:** Be sure to run the following *pip install*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ji_uj2aIsZBC"
   },
   "outputs": [],
   "source": [
    "%pip -q install cryptography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tnPG9CbjsV8V",
    "outputId": "2f436f60-fedb-4d39-b755-2f87685c97b5"
   },
   "outputs": [],
   "source": [
    "#Import Fernet\n",
    "from cryptography.fernet import Fernet\n",
    "import pandas as pd\n",
    "import io  # Import io for StringIO\n",
    "\n",
    "# Generate encryption key and create a cipher suite\n",
    "key = Fernet.generate_key()\n",
    "cipher_suite = Fernet(key)\n",
    "\n",
    "# Sample sensitive data to be encrypted\n",
    "data = pd.DataFrame({\"Patient\": [\"John Doe\", \"Jane Smith\"],\n",
    "                     \"Diagnosis\": [\"Diabetes\", \"Hypertension\"]})\n",
    "data_str = data.to_csv(index=False)\n",
    "encrypted_data = cipher_suite.encrypt(data_str.encode())\n",
    "\n",
    "# Show part of the encrypted string\n",
    "print(\"Encrypted Data (partial):\", encrypted_data[:50], \"...\")\n",
    "\n",
    "# Decrypt the data when access is needed\n",
    "decrypted_data_str = cipher_suite.decrypt(encrypted_data).decode()\n",
    "secure_data = pd.read_csv(io.StringIO(decrypted_data_str))\n",
    "\n",
    "print(\"\\nDecrypted Data Accessible (Only by Authorized Users)\\n\", secure_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3aWhUm5iY4x"
   },
   "source": [
    "### Listing 2-15: Generating Synthetic Health Records\n",
    "Uses **Faker** to create fictional health records, providing a safe, realistic dataset structure for AI training or testing applications. **Note**: Be sure to run the following pip install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7sk7plLij1Y"
   },
   "outputs": [],
   "source": [
    "%pip -q install Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WB18zN0nid42"
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Faker for synthetic data generation\n",
    "fake = Faker()\n",
    "\n",
    "# Generate a synthetic health records dataset\n",
    "data = pd.DataFrame({\n",
    "    'Patient_ID': [fake.uuid4() for _ in range(5)],\n",
    "    'Name': [fake.name() for _ in range(5)],\n",
    "    'Age': [fake.random_int(min=18, max=90) for _ in range(5)],\n",
    "    'Diagnosis': [fake.random_element(elements=('Condition A',\n",
    "                                               'Condition B',\n",
    "                                               'Condition C'))\n",
    "                   for _ in range(5)],\n",
    "    'Phone': [fake.phone_number() for _ in range(5)],\n",
    "    'Address': [fake.address().replace(\"\\n\", \", \") for _ in range(5)],\n",
    "    'Last_Visit': [fake.date_between(start_date='-2y', end_date='today')\n",
    "                   for _ in range(5)]\n",
    "})\n",
    "\n",
    "# Display the synthetic dataset\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iKWZVsVgP6s"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain import HuggingFaceHub\n",
    "import re\n",
    "\n",
    "# Initialize the Mistral language model from HuggingFace\n",
    "llm = HuggingFaceHub(repo_id='mistralai/Mistral-Small-Instruct-2409', model_kwargs={'temperature': 0.1})\n",
    "\n",
    "# Define a prompt template for querying the introduction year via the LLM\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Provide only the superhero introduction year in 4 characters. If you donâ€™t know, respond with \"\".'),\n",
    "    ('human', 'When was {hero_name} introduced by {publisher}?')\n",
    "])\n",
    "\n",
    "# Define the chatbot chain where the prompt is sent to the language model\n",
    "chain = prompt | llm\n",
    "\n",
    "# Function to query the LLM for the introduction year and extract just the year\n",
    "def get_intro_year_from_llm(hero_name, publisher):\n",
    "    # Invoke the chatbot chain with hero name and publisher\n",
    "    response = chain.invoke({'hero_name': hero_name, 'publisher': publisher})\n",
    "    # Use a regex to extract the year from the response (e.g., 1963 or 2015)\n",
    "    match = re.search(r'\\b\\d{4}\\b', response)\n",
    "    return match.group(0) if match else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKEE78rdgcGV"
   },
   "source": [
    "### Block - 2: Enrich Superhero Dataset with Years\n",
    "This block loads a dataset, uses the function to get superhero intro years, updates the dataset, and saves it as a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJnD5g9hgqgd"
   },
   "outputs": [],
   "source": [
    "# Load the superheroes dataset from the provided URL\n",
    "df_info = pd.read_csv(SUPERHEROES_INFO_URL)\n",
    "\n",
    "# Option to process the first 10 rows or all rows\n",
    "# df_info_sample = df_info.copy()  # Uncomment the line below to process all rows\n",
    "df_info_sample = df_info.head(10).copy()  # Process only the first 10 rows\n",
    "\n",
    "# Iterate through each row, passing both name and publisher for the search\n",
    "for idx in df_info_sample.index:\n",
    "    hero_name = df_info_sample.loc[idx, 'name']  # Extract superhero's name\n",
    "    publisher = df_info_sample.loc[idx, 'Publisher']  # Extract publisher's name\n",
    "    # Use LLM for intro year and update the dataframe\n",
    "    intro_year = get_intro_year_from_llm(hero_name, publisher)\n",
    "    df_info_sample.loc[idx, 'Intro Year'] = intro_year  # Safely update the dataframe\n",
    "    print(f\"Processed {hero_name}, Intro Year: {intro_year}\")  # Display progress\n",
    "\n",
    "    # Delay between requests to avoid overwhelming servers (1-second delay)\n",
    "    time.sleep(1)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "# df_info_sample.to_csv('superheroes_info_with_intro_year_sample.csv', index=False)\n",
    "# print(\"Dataset updated and saved to 'superheroes_info_with_intro_year_sample.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
