{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "546fabb6"
      },
      "source": [
        "## Chapter 3 - Exploring Classical Machine Learning\n",
        "\n",
        "This chapter dives into classical machine learning techniques using Scikit-learn, focusing on regression, classification, dimensionality reduction, and clustering. We begin with a linear regression example to predict superhero height from weight, followed by classification models (Logistic Regression and Decision Trees) to categorize superheroes by race based on their powers. The notebook then explores dimensionality reduction using PCA to create a \"Power Score\" and identifies optimal components for capturing variance. Clustering is applied to group superheroes based on their PCA scores and alignment, followed by an analysis of cluster characteristics and cosine similarity to find similar and dissimilar pairs. Finally, the chapter prepares for supervised modeling by screening potential targets, building features, and establishing a baseline with a Gradient Boosting classifier. It then fine-tunes the model, visualizes the results with a confusion matrix, and uses SHAP values to explain model predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Run the following cell to define constants related to datasets"
      ],
      "metadata": {
        "id": "RxgGdMvRCQg2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHihzm4BJZfF"
      },
      "outputs": [],
      "source": [
        "# Base GitHub repository URL\n",
        "BASE_URL = \"https://buildyourownai.github.io/code/datasets/\"\n",
        "\n",
        "# Dataset file names\n",
        "POWERS_FILE       = \"superheroes_powers.csv\"\n",
        "INFO_POWERS_FILE  = \"superheroes_info_powers.csv\"\n",
        "INFO_POWERS2_FILE = \"superheroes_info_powers2.csv\"\n",
        "INFO_POWERS3_FILE = \"superheroes_info_powers3.csv\"\n",
        "PLOTS_FILE        = \"superheroes_story_plots.csv\"\n",
        "\n",
        "# Construct full dataset URLs\n",
        "SUPERHEROES_POWERS_URL = f\"{BASE_URL}{POWERS_FILE}\"\n",
        "SUPERHEROES_INFO_POWERS_URL = f\"{BASE_URL}{INFO_POWERS_FILE}\"\n",
        "SUPERHEROES_INFO_POWERS2_URL = f\"{BASE_URL}{INFO_POWERS2_FILE}\"\n",
        "SUPERHEROES_INFO_POWERS3_URL = f\"{BASE_URL}{INFO_POWERS2_FILE}\"\n",
        "SUPERHEROES_INFO_PLOTS_URL = f\"{BASE_URL}{PLOTS_FILE}\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 3-1: Linear Regression of Superhero Height and Weight\n",
        "Performs linear regression to predict height from weight, using filtered superhero data and evaluating with the coefficient, intercept, and mean squared error. Then Plots the actual versus predicted heights, visualizing the linear regression model’s performance and highlighting how well the model fits the superhero data."
      ],
      "metadata": {
        "id": "9jN9bxgnEbJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Helper (Colab only): tidy regression report\n",
        "def show_regression_report(model, mse, r2, feature_names=None):\n",
        "    coefs = getattr(model, \"coef_\", [])\n",
        "    coefs = coefs.ravel() if hasattr(coefs, \"ravel\") else coefs\n",
        "    if feature_names is None:\n",
        "        feature_names = [f\"x{i}\" for i in range(len(coefs))]\n",
        "    print(\"Regression Report\")\n",
        "    print(\"-----------------\")\n",
        "    for name, c in zip(feature_names, coefs):\n",
        "        print(f\"  coef[{name:<8}] : {c:>10.4f}\")\n",
        "    if hasattr(model, \"intercept_\"):\n",
        "        print(f\"  intercept   : {model.intercept_:>10.2f}\")\n",
        "    print(f\"  MSE         : {mse:>10.2f}\")\n",
        "    print(f\"  R2          : {r2:>10.3f}\")\n",
        "\n",
        "def plot_regression(X_test, y_test, y_pred, title=None,\n",
        "                    xlab=\"Weight\", ylab=\"Height\"):\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Sort by X so the prediction line draws correctly\n",
        "    order = np.argsort(X_test.ravel())\n",
        "    Xs = X_test.ravel()[order]\n",
        "    ys = y_test.ravel()[order]\n",
        "    yhat = y_pred.ravel()[order]\n",
        "\n",
        "    plt.scatter(Xs, ys, color=\"blue\", label=\"Actual Heights\")\n",
        "    plt.plot(Xs, yhat, color=\"red\", label=\"Predicted Heights\")\n",
        "    plt.xlabel(xlab)\n",
        "    plt.ylabel(ylab)\n",
        "    plt.title(title or \"Linear Regression: Height vs. Weight (Human & Cyborg)\")\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.show()\n",
        "\n",
        "# --------- Main Control Flow\n",
        "\n",
        "# Step 0: Load\n",
        "df = pd.read_csv(SUPERHEROES_INFO_POWERS_URL)\n",
        "\n",
        "# Step 1: Prep — filter, drop NAs, trim extremes\n",
        "df = df[df['Species'].isin(['Human', 'Cyborg'])].dropna(\n",
        "    subset=['Height', 'Weight']\n",
        ")\n",
        "df = df[df['Weight'].between(30, 400) & df['Height'].between(100, 350)]\n",
        "\n",
        "# Step 2: Split independent features (X) and dependent target (y)\n",
        "X = df[['Weight']].values\n",
        "y = df['Height'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train\n",
        "model = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = model.score(X_test, y_test)\n",
        "\n",
        "# Step 5: Create regression report and plot results\n",
        "show_regression_report(model, mse, r2, feature_names=[\"Weight\"])\n",
        "plot_regression(\n",
        "    X_test, y_test, y_pred,\n",
        "    title=\"Linear Regression: Height vs. Weight (Human & Cyborg)\"\n",
        ")"
      ],
      "metadata": {
        "id": "qvSmxWWdJ3P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 3-2: Training Classification Models for Superhero Races\n",
        "Trains logistic regression and decision tree models to classify superheroes into races (Human, Mutant, Cyborg) using powers data, then outputs accuracy and performance metrics. Then Visualizes the decision tree using a simplified, color-coded representation to help understand how the model makes predictions based on superhero attributes."
      ],
      "metadata": {
        "id": "KzzgENH-FZCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: SUPERHEROES_POWERS_URL, SUPERHEROES_INFO_POWERS_URL\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Helpers for Colab only; omit from book listing\n",
        "def load_and_prepare(powers_url, info_url):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      X: all power flags as numeric features (independent vars)\n",
        "      y: Species labels mapped to integers (dependent var)\n",
        "    \"\"\"\n",
        "    df_p = pd.read_csv(powers_url)\n",
        "    df_i = pd.read_csv(info_url)[['name', 'Species']]\n",
        "    df = df_p.merge(df_i, left_on='hero_names', right_on='name')\n",
        "    df = df[df['Species'].isin(['Human', 'Mutant', 'Cyborg'])].dropna()\n",
        "    # Labels y: Species -> int\n",
        "    y = df['Species'].map({'Human': 0, 'Mutant': 1, 'Cyborg': 2}).values\n",
        "    # Features X: all power columns only\n",
        "    X = df.drop(columns=['hero_names', 'name', 'Species']).astype(int).values\n",
        "    return X, y\n",
        "\n",
        "def train_and_eval(model, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Fit on training data, predict on test data, return accuracy + report.\n",
        "    \"\"\"\n",
        "    model.fit(X_train, y_train)\n",
        "    y_hat = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_hat)\n",
        "    rep = classification_report(y_test, y_hat, zero_division=0)\n",
        "    return acc, rep\n",
        "\n",
        "def print_report(title, acc, rep):\n",
        "    print(f\"{title} Accuracy: {acc:.3f}\")\n",
        "    print(f\"{title} Report:\\n{rep}\")\n",
        "\n",
        "# Step 0: Load and prepare data  (X = powers, y = Species)\n",
        "X, y = load_and_prepare(SUPERHEROES_POWERS_URL,\n",
        "                        SUPERHEROES_INFO_POWERS_URL)\n",
        "\n",
        "# Step 1: Train/test split (hold out 20% for testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Define models\n",
        "#   LogisticRegression: fast baseline\n",
        "#   DecisionTree: interpretable, non-linear splits\n",
        "log_model = LogisticRegression(max_iter=1000)\n",
        "tree_model = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
        "\n",
        "# Step 3: Train and evaluate\n",
        "log_acc, log_rep = train_and_eval(log_model, X_train, X_test,\n",
        "                                  y_train, y_test)\n",
        "tree_acc, tree_rep = train_and_eval(tree_model, X_train, X_test,\n",
        "                                    y_train, y_test)\n",
        "\n",
        "# Step 4: Show results\n",
        "print_report(\"Logistic Regression\", log_acc, log_rep)\n",
        "print_report(\"Decision Tree\", tree_acc, tree_rep)"
      ],
      "metadata": {
        "id": "G_EiS-uBuCVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following code cell to plot the decision tree"
      ],
      "metadata": {
        "id": "bXGx7M4YFsTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES before running this cell:\n",
        "#   - Packages: pandas, scikit-learn, matplotlib\n",
        "#   - Data paths/URLs defined: SUPERHEROES_POWERS_URL, SUPERHEROES_INFO_POWERS_URL\n",
        "\n",
        "# =========================\n",
        "# ------ CONSTANTS --------\n",
        "# =========================\n",
        "FIG_W, FIG_H        = 42, 22        # big canvas; extra vertical room for lower leaves\n",
        "TITLE_SIZE          = 48            # headline-large for slides\n",
        "LABEL_SIZE          = 42            # large but avoids label collisions\n",
        "LEGEND_SIZE         = 42\n",
        "\n",
        "LEGEND_SIZE         = 44\n",
        "LEGEND_BOX_W, LEGEND_BOX_H = 44, 18  # points-ish; tune to taste\n",
        "\n",
        "EDGE_WIDTH          = 3.0           # darker, thicker edges pop on projectors\n",
        "EDGE_COLOR          = \"k\"\n",
        "NODE_MARKERSIZE     = 9\n",
        "LEAF_SIZE           = 0.20          # slightly larger class blocks\n",
        "LEAF_EDGE_COLOR     = \"k\"\n",
        "LEAF_EDGE_WIDTH     = 0.8\n",
        "\n",
        "LABEL_BOX_ALPHA     = 0.90          # more opaque; text stays crisp over lines\n",
        "LABEL_BOX_EDGE      = \"0.5\"\n",
        "LABEL_Y_OFFSET      = 0.045         # a bit more clearance above node\n",
        "NAME_TRUNC          = 28            # keeps labels short enough across the width\n",
        "MAX_RULES           = 12            # shows top splits without crowding\n",
        "\n",
        "CLASSES             = [\"Human\", \"Mutant\", \"Cyborg\"]\n",
        "COLORS              = [\"blue\", \"red\", \"yellow\"]\n",
        "\n",
        "# =========================\n",
        "# ------ HELPERS ----------\n",
        "# =========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def keyify(s: pd.Series) -> pd.Series:\n",
        "    return s.astype(str).str.strip().str.lower()\n",
        "\n",
        "def to01(s: pd.Series) -> pd.Series:\n",
        "    if s.dtype == \"bool\":\n",
        "        return s.astype(int)\n",
        "    if s.dtype == \"O\":\n",
        "        return s.astype(str).str.lower().isin([\"true\",\"1\",\"yes\"]).astype(int)\n",
        "    return s.fillna(0).astype(int)\n",
        "\n",
        "# Minimal tree viz with human-readable labels (keeps your original draw order)\n",
        "def plot_annotated_tree(tree_model, feature_names=None,\n",
        "                        title=\"Decision Tree\", annotate=True, max_rules=MAX_RULES):\n",
        "    # prefer names learned by sklearn; else use provided list\n",
        "    if hasattr(tree_model, \"feature_names_in_\"):\n",
        "        names = list(tree_model.feature_names_in_)\n",
        "    else:\n",
        "        names = list(feature_names) if feature_names is not None else None\n",
        "\n",
        "    def short_label(idx: int) -> str:\n",
        "        if names and 0 <= idx < len(names):\n",
        "            n = str(names[idx]).strip()\n",
        "            return (n[:NAME_TRUNC] + (\"…\" if len(n) > NAME_TRUNC else \"\")) + \"?\"\n",
        "        return f\"feat_{idx}?\"\n",
        "\n",
        "    tree = tree_model.tree_\n",
        "    fig, ax = plt.subplots(figsize=(FIG_W, FIG_H))\n",
        "    rules, rid = [], 0\n",
        "\n",
        "    def plot_node(node, depth, x, width):\n",
        "        nonlocal rid\n",
        "        left, right = tree.children_left[node], tree.children_right[node]\n",
        "        if left != right:  # internal\n",
        "            lx, rx = x - width/2, x + width/2\n",
        "            nxt = width/2\n",
        "            ax.plot([x, lx], [depth, depth + 1], color=EDGE_COLOR, linewidth=EDGE_WIDTH)\n",
        "            ax.plot([x, rx], [depth, depth + 1], color=EDGE_COLOR, linewidth=EDGE_WIDTH)\n",
        "            if annotate and rid < max_rules:\n",
        "                feat = int(tree.feature[node])\n",
        "                rid += 1\n",
        "                lab = f\"{rid}: {short_label(feat)}\"\n",
        "                ax.text(x, depth - LABEL_Y_OFFSET, lab, ha=\"center\", va=\"top\",\n",
        "                        fontsize=LABEL_SIZE,\n",
        "                        bbox=dict(facecolor=\"white\", alpha=LABEL_BOX_ALPHA,\n",
        "                                  edgecolor=LABEL_BOX_EDGE))\n",
        "                rules.append(lab)\n",
        "            ax.plot(x, depth, \"ko\", markersize=NODE_MARKERSIZE)\n",
        "            plot_node(left, depth + 1, lx, nxt)\n",
        "            plot_node(right, depth + 1, rx, nxt)\n",
        "        else:  # leaf\n",
        "            vals = tree.value[node].ravel()\n",
        "            pred = int(np.argmax(vals))\n",
        "            half = LEAF_SIZE / 2.0\n",
        "            ax.add_patch(plt.Rectangle((x - half, depth - half),\n",
        "                                       LEAF_SIZE, LEAF_SIZE,\n",
        "                                       facecolor=COLORS[pred],\n",
        "                                       edgecolor=LEAF_EDGE_COLOR,\n",
        "                                       linewidth=LEAF_EDGE_WIDTH))\n",
        "\n",
        "    plot_node(0, 0, 0.5, 1.0)\n",
        "    ax.axis(\"off\")\n",
        "    patches = [mpatches.Patch(color=c, label=l) for c, l in zip(COLORS, CLASSES)]\n",
        "\n",
        "    leg = ax.legend(\n",
        "        handles=patches,\n",
        "        loc=\"upper right\",\n",
        "        framealpha=0.95,\n",
        "        fontsize=LEGEND_SIZE,   # legend text size\n",
        "        handlelength=2.2,       # widens the color swatch\n",
        "        borderpad=0.6,\n",
        "        labelspacing=0.6,\n",
        "        handletextpad=0.6,\n",
        "    )\n",
        "\n",
        "    # enlarge the color boxes (works across Matplotlib versions)\n",
        "    for p in leg.get_patches():\n",
        "        p.set_width(LEGEND_BOX_W)\n",
        "        p.set_height(LEGEND_BOX_H)\n",
        "\n",
        "    ax.set_title(title, fontsize=TITLE_SIZE)\n",
        "    ax.invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if annotate and rules:\n",
        "        print(\"Annotated split questions (top of tree):\")\n",
        "        for r in rules:\n",
        "            print(f\"  {r}\")\n",
        "\n",
        "# =========================\n",
        "# ---- MAIN CONTROL -------\n",
        "# =========================\n",
        "# Load data\n",
        "pow_df  = pd.read_csv(SUPERHEROES_POWERS_URL)       # 160+ flags\n",
        "info_df = pd.read_csv(SUPERHEROES_INFO_POWERS_URL)  # names + Species\n",
        "\n",
        "# Join on common key\n",
        "pow_key = \"hero_names\" if \"hero_names\" in pow_df.columns else \"name\"\n",
        "inf_key = \"name\" if \"name\" in info_df.columns else \"hero_names\"\n",
        "pow_df[\"name_key\"]  = keyify(pow_df[pow_key])\n",
        "info_df[\"name_key\"] = keyify(info_df[inf_key])\n",
        "dfm = pow_df.merge(info_df[[\"name_key\", \"Species\"]], on=\"name_key\", how=\"inner\")\n",
        "\n",
        "# Keep target classes\n",
        "valid = [\"Human\", \"Mutant\", \"Cyborg\"]\n",
        "dfm = dfm[dfm[\"Species\"].isin(valid)].copy()\n",
        "\n",
        "# Select power columns (exclude metadata)\n",
        "exclude = {\"name_key\", \"Species\", \"name\", \"hero_names\",\n",
        "           \"Height\", \"Weight\", \"has_powers_source\"}\n",
        "feat_cols = [c for c in dfm.columns if c not in exclude]\n",
        "\n",
        "# Features/labels\n",
        "X_df = dfm[feat_cols].apply(to01)\n",
        "y_cls = dfm[\"Species\"].map({\"Human\":0, \"Mutant\":1, \"Cyborg\":2}).astype(int)\n",
        "\n",
        "print(f\"features={X_df.shape[1]}  class_counts={y_cls.value_counts().to_dict()}\")\n",
        "\n",
        "# Train/test and fit\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X_df, y_cls, test_size=0.2, random_state=42)\n",
        "tree = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
        "tree.fit(X_tr, y_tr)\n",
        "\n",
        "# Plot\n",
        "plot_annotated_tree(\n",
        "    tree,\n",
        "    feature_names=X_df.columns.tolist(),\n",
        "    title=\"Decision Tree: Rule Paths for Species Prediction\",\n",
        "    annotate=True,\n",
        "    max_rules=MAX_RULES\n",
        ")"
      ],
      "metadata": {
        "id": "kzNCETb-V9pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 3-3: Dimensionality Reduction with PCA Power Score\n",
        "This code uses PCA for dimensionality reduction, enhancing the superheroes_info_powers dataset with a more nuanced Power Score, offering a comprehensive measure of superheroes' abilities."
      ],
      "metadata": {
        "id": "FhQO7QQsx3gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: SUPERHEROES_INFO_POWERS_URL, SUPERHEROES_POWERS_URL\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "MIN_POWER_FLAGS = 2  # require ≥2 powers to count as a valid profile\n",
        "\n",
        "# ----------------------- Helper: validity mask -----------------------\n",
        "def valid_power_mask(df: pd.DataFrame, X: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"True for rows with a source and ≥ MIN_POWER_FLAGS set.\n",
        "       Also require OPR+SDR > 0 when present.\"\"\"\n",
        "    has_src = df['has_powers_source'] if 'has_powers_source' in df.columns \\\n",
        "              else pd.Series(1, index=df.index)\n",
        "    power_count = X.fillna(0).astype(float).sum(axis=1)\n",
        "    flags_ok = power_count >= MIN_POWER_FLAGS          # exclude 0–1 flags\n",
        "    if {'OPR','SDR'}.issubset(df.columns):\n",
        "        opr_sdr_ok = (df['OPR'].fillna(0) + df['SDR'].fillna(0)) > 0\n",
        "    else:\n",
        "        opr_sdr_ok = pd.Series(True, index=df.index)\n",
        "    return (has_src == 1) & flags_ok & opr_sdr_ok\n",
        "\n",
        "# ----------------------------- Control ------------------------------\n",
        "\n",
        "# Load data\n",
        "info = pd.read_csv(SUPERHEROES_INFO_POWERS_URL)\n",
        "powers = pd.read_csv(SUPERHEROES_POWERS_URL)\n",
        "\n",
        "# Join on hero name\n",
        "df = info.merge(powers, left_on='name', right_on='hero_names')\n",
        "\n",
        "# Build power matrix\n",
        "non_power = [\n",
        "    'name','Gender','Species','Height','Publisher','Alignment',\n",
        "    'Weight','OPR','SDR','hero_names','has_powers_source'\n",
        "]\n",
        "X = df.drop(columns=[c for c in non_power if c in df.columns], errors='ignore')\n",
        "\n",
        "# Mask rows with real evidence (source present, ≥2 powers, OPR+SDR>0)\n",
        "mask = valid_power_mask(df, X)                          # boolean per row\n",
        "\n",
        "# Fit scaler on valid rows only (avoid bias from placeholders)\n",
        "X_fit = X.loc[mask].fillna(0).astype(float)             # numeric, no NaNs\n",
        "scaler = StandardScaler()                               # create scaler\n",
        "Xz_fit = scaler.fit_transform(X_fit)                    # standardize valid set\n",
        "\n",
        "# Fit PCA on valid rows only (teach with 1D PC1; expand later if needed)\n",
        "pca = PCA(n_components=1)                               # first component only\n",
        "pc1_fit = pca.fit_transform(Xz_fit).ravel()             # per-hero PC1 (valid)\n",
        "evr = float(pca.explained_variance_ratio_[0])           # variance share of PC1\n",
        "\n",
        "# Transform all rows consistently, assign scores only to valid rows\n",
        "X_all = X.fillna(0).astype(float)                       # same prep for all\n",
        "pc1_all = pca.transform(scaler.transform(X_all)).ravel()# project everyone\n",
        "\n",
        "scores = pd.Series(np.nan, index=df.index)              # start as missing\n",
        "scores.loc[mask] = np.round(pc1_all[mask], 1)           # write valid scores\n",
        "df['PCA_Power_Score'] = scores                          # keep NaN for 0–1 flags\n",
        "\n",
        "# Map back to original info table\n",
        "df_info_powers = info.merge(\n",
        "    df[['name','PCA_Power_Score']], on='name', how='left'\n",
        ")\n",
        "\n",
        "# Quick preview\n",
        "print('Explained variance (PC1):', round(evr, 3))        # e.g., 0.056\n",
        "print('Skipped as invalid (<=1 power or OPR+SDR==0):',\n",
        "      int((~mask).sum()))\n",
        "print(df_info_powers.head())"
      ],
      "metadata": {
        "id": "uAgr3Prkx6-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05802efa"
      },
      "source": [
        "### Listing 3-3A: Finding the Optimal Number of PCA Components\n",
        "This section refines the PCA process by exploring how many components are needed to capture a desired amount of variance in the power data. It allows experimentation with the `N_COMPONENTS` or `VAR_TARGET` constants to determine the best dimensionality reduction for representing the superhero powers effectively."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PCA reference runner with adjustable components.\n",
        "- MODE: \"k\" (keep first N_COMPONENTS) or \"var\" (hit VAR_TARGET).\n",
        "- Validity mask excludes rows with weak/incomplete power info.\n",
        "- Fits scaler+PCA on valid rows, then transforms all rows.\n",
        "- Reattaches PCs; invalid rows get NaN for all PCA outputs.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# ------------------------------- CONFIG --------------------------------\n",
        "MODE = \"k\"          # \"k\" for fixed number; \"var\" to target variance\n",
        "N_COMPONENTS = 80   # used if MODE == \"k\"\n",
        "VAR_TARGET  = 0.80  # used if MODE == \"var\" (e.g., 0.80 for 80%)\n",
        "MIN_POWER_FLAGS = 2 # require ≥2 power flags to consider a row \"valid\"\n",
        "\n",
        "# --------------------------- HELPERS -----------------------------------\n",
        "\n",
        "def join_on_name(info_url: str, powers_url: str) -> pd.DataFrame:\n",
        "    info = pd.read_csv(info_url)\n",
        "    powers = pd.read_csv(powers_url)\n",
        "    return info.merge(powers, left_on='name', right_on='hero_names')\n",
        "\n",
        "def build_power_matrix(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    non_power = [\n",
        "        'name','Gender','Species','Height','Publisher','Alignment',\n",
        "        'Weight','OPR','SDR','hero_names','has_powers_source'\n",
        "    ]\n",
        "    return df.drop(columns=[c for c in non_power if c in df.columns],\n",
        "                   errors='ignore')\n",
        "\n",
        "def valid_power_mask(df: pd.DataFrame, X: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"True for rows with a source and ≥ MIN_POWER_FLAGS set.\n",
        "       Also require OPR+SDR > 0 when present.\"\"\"\n",
        "    has_src = df['has_powers_source'] if 'has_powers_source' in df.columns \\\n",
        "              else pd.Series(1, index=df.index)\n",
        "    power_count = X.fillna(0).astype(float).sum(axis=1)\n",
        "    flags_ok = power_count >= MIN_POWER_FLAGS\n",
        "    if {'OPR','SDR'}.issubset(df.columns):\n",
        "        opr_sdr_ok = (df['OPR'].fillna(0) + df['SDR'].fillna(0)) > 0\n",
        "    else:\n",
        "        opr_sdr_ok = pd.Series(True, index=df.index)\n",
        "    return (has_src == 1) & flags_ok & opr_sdr_ok\n",
        "\n",
        "def fit_scaler_pca(X_fit: pd.DataFrame, mode: str,\n",
        "                   n_components: int, var_target: float):\n",
        "    \"\"\"Fit StandardScaler and PCA on valid rows.\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    Xz = scaler.fit_transform(X_fit)\n",
        "\n",
        "    # Fit full PCA first to read explained variance profile\n",
        "    pca_full = PCA().fit(Xz)\n",
        "    evr = pca_full.explained_variance_ratio_\n",
        "    cum = np.cumsum(evr)\n",
        "\n",
        "    if mode == \"var\":\n",
        "        k = int(np.searchsorted(cum, var_target) + 1)\n",
        "    else:\n",
        "        k = n_components\n",
        "\n",
        "    # Refit PCA for just the chosen number of components\n",
        "    pca = PCA(n_components=k).fit(Xz)\n",
        "    return scaler, pca, evr, cum, k\n",
        "\n",
        "def transform_all_rows(scaler: StandardScaler, pca: PCA,\n",
        "                       X_all: pd.DataFrame) -> np.ndarray:\n",
        "    Xz_all = scaler.transform(X_all)\n",
        "    pcs_all = pca.transform(Xz_all)  # shape: (n_samples, k)\n",
        "    return pcs_all\n",
        "\n",
        "# --------------------------- CONTROL FLOW ------------------------------\n",
        "\n",
        "# 1) Load and join\n",
        "df = join_on_name(SUPERHEROES_INFO_POWERS_URL, SUPERHEROES_POWERS_URL)\n",
        "\n",
        "# --- Tidy: drop redundant/merge-helper columns and dupes early ---\n",
        "drop_exact = [c for c in ['hero_names', 'Unnamed: 0'] if c in df.columns]\n",
        "df = df.drop(columns=drop_exact, errors='ignore')\n",
        "\n",
        "# If merge created *_x/*_y duplicates, prefer the left side (_x) and drop _y\n",
        "dupe_pairs = [(c, c.replace('_x', '_y')) for c in df.columns if c.endswith('_x')]\n",
        "to_drop = [b for a, b in dupe_pairs if b in df.columns]\n",
        "df = df.rename(columns={a: a[:-2] for a, b in dupe_pairs})  # strip _x\n",
        "df = df.drop(columns=to_drop, errors='ignore')\n",
        "\n",
        "# 2) Build power matrix X\n",
        "X = build_power_matrix(df)\n",
        "\n",
        "# 3) Compute validity mask (keep rows with real evidence)\n",
        "mask = valid_power_mask(df, X)\n",
        "\n",
        "# 4) Fit scaler + PCA on valid rows\n",
        "X_fit = X.loc[mask].fillna(0).astype(float)\n",
        "scaler, pca, evr, cum, k = fit_scaler_pca(\n",
        "    X_fit, MODE, N_COMPONENTS, VAR_TARGET\n",
        ")\n",
        "\n",
        "print(\"Explained variance ratios (first 10):\",\n",
        "      [round(v, 3) for v in evr[:10]])\n",
        "print(\"Cumulative (first 10):\",\n",
        "      [round(c, 3) for c in cum[:10]])\n",
        "print(f\"Chosen k={k}  (cumulative variance @k={cum[k-1]:.3f})\")\n",
        "\n",
        "# 5) Transform all rows, attach PCA only for valid ones\n",
        "X_all = X.fillna(0).astype(float)\n",
        "pcs_all = transform_all_rows(scaler, pca, X_all)  # (n_rows, k)\n",
        "\n",
        "# Prepare PCA columns; invalid rows get NaN\n",
        "for i in range(k):\n",
        "    col = f'PCA_PC{i+1}'\n",
        "    vals = pd.Series(np.nan, index=df.index)\n",
        "    vals.loc[mask] = np.round(pcs_all[mask.values, i], 2)\n",
        "    df[col] = vals\n",
        "\n",
        "# Validity flag for downstream steps\n",
        "df['PCA_valid'] = mask\n",
        "\n",
        "# 6) Reorder columns so PCA block is on the right (no PCA_Power_Score)\n",
        "pca_cols = [f'PCA_PC{i+1}' for i in range(k)]\n",
        "tail_cols = ['PCA_valid'] + pca_cols\n",
        "base_cols = [c for c in df.columns if c not in tail_cols]\n",
        "df = df[base_cols + tail_cols]\n",
        "\n",
        "# 7) Preview and report\n",
        "print(\"\\n=== PCA Summary Report ===\")\n",
        "print(f\"Rows used for PCA fit: {int(mask.sum())}\")\n",
        "print(f\"Rows excluded (insufficient power data): {int((~mask).sum())}\")\n",
        "print(f\"Number of components retained: {k}\")\n",
        "print(f\"Cumulative variance explained: {cum[k-1]:.3f} ({cum[k-1]*100:.1f}%)\\n\")\n",
        "\n",
        "# Show first few principal component columns\n",
        "preview_cols = ['name', 'Species', 'Alignment', 'PCA_valid'] + \\\n",
        "               [f'PCA_PC{i+1}' for i in range(min(5, k))]\n",
        "print(\"Sample of first few principal components per hero (rows truncated):\")\n",
        "print(df[preview_cols].head())\n",
        "\n",
        "# 8) Export version with PCA block at the end\n",
        "print(\"\\nBuilding 'superheroes_info_powers2.csv' with PCA columns appended...\")\n",
        "keep_info = ['name','Species','Gender','Alignment','Publisher',\n",
        "             'Height','Weight','OPR','SDR','has_powers_source']\n",
        "keep_info = [c for c in keep_info if c in df.columns]\n",
        "\n",
        "# Save new INFO_POWERS dataset with PCA values\n",
        "df_info_powers2 = df[keep_info + ['PCA_valid'] + pca_cols].copy()\n",
        "df_info_powers2.to_csv(INFO_POWERS2_FILE, index=False)\n",
        "print(\"File saved successfully.\")"
      ],
      "metadata": {
        "id": "3RzzbISFKG1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71205ccd"
      },
      "source": [
        "### Listing 3-4: K-Means Clustering on PCA Features\n",
        "This code performs K-Means clustering on the PCA-transformed superhero power data. It uses a specified number of principal components to group superheroes into clusters. The code then reports the cluster sizes, silhouette score, and provides a basic analysis of cluster characteristics based on attributes like Species and Alignment. Finally, it visualizes the clusters using a scatter plot of the first two principal components."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Means clustering on PCA features with a basic report and plot.\n",
        "# Set SUPERHEROES_INFO_POWERS2_URL to your CSV location before running.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# =========================\n",
        "# Constants\n",
        "# =========================\n",
        "N_PCS_DEFAULT = 60   # number of PCA components to use by default\n",
        "K_CLUSTERS    = 3    # number of clusters for K-Means\n",
        "RAND_SEED     = 42\n",
        "\n",
        "# =========================\n",
        "# Helpers\n",
        "# =========================\n",
        "def _to_bool(x):\n",
        "    if isinstance(x, str):\n",
        "        return x.strip().lower() in (\"true\", \"t\", \"1\", \"yes\")\n",
        "    try:\n",
        "        return bool(int(x))\n",
        "    except Exception:\n",
        "        return bool(x)\n",
        "\n",
        "def build_features(df, n_pcs=N_PCS_DEFAULT):\n",
        "    \"\"\"Return X and df_feat filtered for clustering on the first n_pcs PCs.\"\"\"\n",
        "    mask = pd.Series(True, index=df.index)\n",
        "    if \"PCA_valid\" in df.columns:\n",
        "        mask &= df[\"PCA_valid\"].apply(_to_bool)\n",
        "    if \"has_powers_source\" in df.columns:\n",
        "        mask &= df[\"has_powers_source\"].apply(_to_bool)\n",
        "    df = df[mask].copy()\n",
        "\n",
        "    # Allow up to the first 60 components if present, then slice to n_pcs\n",
        "    pc_cols_all = [f\"PCA_PC{i}\" for i in range(1, 61)]\n",
        "    pc_cols = [c for c in pc_cols_all if c in df.columns][:n_pcs]\n",
        "    if not pc_cols:\n",
        "        raise ValueError(\"No PCA_PC columns found in the dataset.\")\n",
        "\n",
        "    feats = df[pc_cols].astype(float).dropna()\n",
        "    df_feat = df.loc[feats.index].copy()\n",
        "    return feats.to_numpy(), df_feat, pc_cols\n",
        "\n",
        "def basic_report(df_feat, labels, sil):\n",
        "    df_feat = df_feat.copy()\n",
        "    df_feat[\"Cluster\"] = labels\n",
        "    print(f\"Chosen K: {len(np.unique(labels))}\")\n",
        "    print(f\"Silhouette score: {sil:.3f}\\n\")\n",
        "\n",
        "    if \"Alignment\" in df_feat.columns:\n",
        "        print(\"Cluster vs Alignment (counts):\")\n",
        "        print(pd.crosstab(df_feat[\"Cluster\"], df_feat[\"Alignment\"]))\n",
        "        print()\n",
        "\n",
        "    if \"Species\" in df_feat.columns:\n",
        "        print(\"Cluster vs Species (top 10):\")\n",
        "        species_ct = pd.crosstab(df_feat[\"Cluster\"], df_feat[\"Species\"])\n",
        "        top = species_ct.sum().sort_values(ascending=False).head(10).index\n",
        "        print(species_ct[top])\n",
        "        print()\n",
        "\n",
        "    print(\"Sample heroes per cluster:\")\n",
        "    if \"name\" in df_feat.columns:\n",
        "        for c in sorted(df_feat[\"Cluster\"].unique()):\n",
        "            names = df_feat.loc[df_feat[\"Cluster\"] == c, \"name\"].head(10)\n",
        "            print(f\"Cluster {c}: {', '.join(names)}\")\n",
        "    print()\n",
        "\n",
        "def plot_pc_scatter(df_feat, labels, title):\n",
        "    dfp = df_feat.copy()\n",
        "    dfp[\"Cluster\"] = labels\n",
        "    colors = ListedColormap([\"black\", \"red\", \"blue\"])\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sc = plt.scatter(\n",
        "        dfp[\"PCA_PC1\"], dfp[\"PCA_PC2\"],\n",
        "        c=dfp[\"Cluster\"], cmap=colors, s=40, alpha=0.6, edgecolor=\"none\"\n",
        "    )\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.title(title)\n",
        "    cbar = plt.colorbar(sc)\n",
        "    cbar.set_label(\"Cluster\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def attach_kmeans_clusters(df: pd.DataFrame,\n",
        "                           n_pcs: int = N_PCS_DEFAULT,\n",
        "                           k: int = K_CLUSTERS,\n",
        "                           label_col: str = \"Cluster\",\n",
        "                           save_path: str | None = None,\n",
        "                           random_state: int = RAND_SEED):\n",
        "    \"\"\"\n",
        "    Fit K-Means on the first n_pcs PCA columns and add a 'Cluster' column.\n",
        "\n",
        "    Returns (df_with_labels, kmeans_model, used_pc_columns, silhouette_score).\n",
        "\n",
        "    Tip: If you later train a supervised model to predict these clusters,\n",
        "    use a different feature set than these PCs to avoid leakage.\n",
        "    \"\"\"\n",
        "    mask = pd.Series(True, index=df.index)\n",
        "    if \"PCA_valid\" in df.columns:\n",
        "        mask &= df[\"PCA_valid\"].astype(str).str.lower().isin([\"true\", \"t\", \"1\", \"yes\"])\n",
        "    if \"has_powers_source\" in df.columns:\n",
        "        mask &= df[\"has_powers_source\"].astype(str).str.lower().isin([\"true\", \"t\", \"1\", \"yes\"])\n",
        "\n",
        "    pc_cols = [f\"PCA_PC{i}\" for i in range(1, 61)]\n",
        "    pc_cols = [c for c in pc_cols if c in df.columns][:n_pcs]\n",
        "    if not pc_cols:\n",
        "        raise ValueError(\"No PCA_PC* columns found.\")\n",
        "\n",
        "    feats = df.loc[mask, pc_cols].apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
        "    X = feats.to_numpy()\n",
        "\n",
        "    km = KMeans(n_clusters=k, n_init=20, max_iter=300, random_state=random_state)\n",
        "    labels = km.fit_predict(X)\n",
        "    sil = silhouette_score(X, labels) if len(np.unique(labels)) > 1 else float(\"nan\")\n",
        "\n",
        "    out = df.copy()\n",
        "    out[label_col] = np.nan\n",
        "    out.loc[feats.index, label_col] = labels\n",
        "\n",
        "    if save_path:\n",
        "        out.to_csv(save_path, index=False)\n",
        "        print(f\"Wrote: {save_path}\")\n",
        "\n",
        "    print(f\"Rows labeled: {len(feats)} / {len(df)} | PCs used: {len(pc_cols)}\")\n",
        "    print(f\"Chosen K: {k} | Silhouette: {sil:.3f}\")\n",
        "    return out, km, pc_cols, sil\n",
        "\n",
        "# --------------------------- CONTROL FLOW ------------------------------\n",
        "# Step 1 - Load the dataset from a CSV URL or local path.\n",
        "df = pd.read_csv(SUPERHEROES_INFO_POWERS2_URL)\n",
        "\n",
        "# Step 2 - Build the feature matrix from the first N_PCS_DEFAULT components.\n",
        "#          PCA compresses 160+ power columns into fewer, denoised axes.\n",
        "X, df_feat, feature_cols = build_features(df, n_pcs=N_PCS_DEFAULT)\n",
        "print(f\"Rows used: {X.shape[0]}   Features used: {X.shape[1]}\")\n",
        "\n",
        "# Step 3 - Fit K-Means with K=K_CLUSTERS.\n",
        "#          Silhouette will help us judge separation quality later.\n",
        "km = KMeans(\n",
        "    n_clusters=K_CLUSTERS,   # number of clusters to find\n",
        "    random_state=RAND_SEED,  # fixed seed for repeatable results\n",
        "    n_init=20,               # try 20 centroid seeds, keep the best\n",
        "    max_iter=300             # cap refinement steps\n",
        ")\n",
        "labels = km.fit_predict(X)   # run K-Means clustering\n",
        "\n",
        "# Step 4 - Compute a quick quality score and print a basic report.\n",
        "#          Silhouette ranges from -1 to 1. Higher means cleaner separation.\n",
        "sil = silhouette_score(X, labels) if len(np.unique(labels)) > 1 else float(\"nan\")\n",
        "basic_report(df_feat, labels, sil)\n",
        "\n",
        "# Step 5 - Plot clusters on PC1 vs PC2 for a visual read of the grouping.\n",
        "plot_pc_scatter(\n",
        "    df_feat,\n",
        "    labels,\n",
        "    f\"K-Means on {N_PCS_DEFAULT} PCs (K={K_CLUSTERS})\"\n",
        ")\n",
        "\n",
        "# Step 6 - (Optional) Write a new dataset with the added 'Cluster' column.\n",
        "df3, km_model, used_pcs, sil = attach_kmeans_clusters(\n",
        "    df,\n",
        "    n_pcs=N_PCS_DEFAULT,     # keep in sync with the features above\n",
        "    k=K_CLUSTERS,            # number of clusters\n",
        "    label_col=\"Cluster\",\n",
        "    save_path=INFO_POWERS3_FILE,\n",
        "    random_state=RAND_SEED\n",
        ")"
      ],
      "metadata": {
        "id": "g5JG9zg3uOTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e79dd3cb"
      },
      "source": [
        "### Listing 3-4A: Analyzing K-Means Cluster Characteristics\n",
        "This section examines the characteristics of the superhero clusters identified by K-Means in Listing 3-4. By running statistics across the groups, the code reveals notable similarities and trends within each cluster. This helps to understand the composition of the clusters based on attributes like Species, Alignment, and power levels, providing insights into the meaningfulness of the discovered groupings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: df_feat, labels\n",
        "# Cluster trend profiler: counts, top categories, numeric summaries.\n",
        "# Call: profile_cluster_trends(df_feat, labels)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def _pct_table(s, top_n=8):\n",
        "    ct = s.value_counts(dropna=False)\n",
        "    pct = (ct / ct.sum()).round(3)\n",
        "    out = pd.concat([ct.rename(\"count\"), pct.rename(\"pct\")], axis=1)\n",
        "    return out.head(top_n)\n",
        "\n",
        "def profile_cluster_trends(df_feat, labels,\n",
        "                           cat_cols=(\"Species\", \"Alignment\", \"Gender\"),\n",
        "                           num_cols=(\"OPR\", \"SDR\", \"Height\", \"Weight\"),\n",
        "                           top_n=8):\n",
        "    fr = df_feat.copy()\n",
        "    fr[\"Cluster\"] = labels\n",
        "    print(\"Cluster sizes:\")\n",
        "    print(fr[\"Cluster\"].value_counts().sort_index(), \"\\n\")\n",
        "\n",
        "    # Categorical snapshots\n",
        "    for col in cat_cols:\n",
        "        if col not in fr.columns:\n",
        "            continue\n",
        "        print(f\"Top {top_n} {col} values by cluster:\")\n",
        "        for c in sorted(fr[\"Cluster\"].unique()):\n",
        "            tab = _pct_table(fr.loc[fr[\"Cluster\"] == c, col], top_n=top_n)\n",
        "            print(f\"\\nCluster {c} — {col}\")\n",
        "            print(tab)\n",
        "        print()\n",
        "\n",
        "    # Numeric summaries\n",
        "    keep = [c for c in num_cols if c in fr.columns]\n",
        "    if keep:\n",
        "        print(\"Numeric summaries by cluster:\")\n",
        "        summ = (fr.groupby(\"Cluster\")[keep]\n",
        "                  .agg([\"mean\", \"median\", \"std\", \"min\", \"max\", \"count\"]))\n",
        "        print(summ.round(2), \"\\n\")\n",
        "\n",
        "    # Simple power tier from OPR and SDR if present\n",
        "    if {\"OPR\", \"SDR\"}.issubset(fr.columns):\n",
        "        z = fr[[\"OPR\", \"SDR\"]].apply(\n",
        "            lambda c: (c - c.mean()) / c.std(ddof=0)\n",
        "        )\n",
        "        fr[\"power_index\"] = z.sum(axis=1)\n",
        "        tier_mean = (fr.groupby(\"Cluster\")[\"power_index\"]\n",
        "                       .mean().sort_values())\n",
        "        order = tier_mean.index.tolist()\n",
        "        names = {order[0]: \"Low\", order[-1]: \"High\"}\n",
        "        if len(order) >= 3:\n",
        "            names[order[len(order)//2]] = \"Medium\"\n",
        "        print(\"Average power_index by cluster:\")\n",
        "        print(tier_mean.round(3))\n",
        "        print(\"Assigned tiers:\", names, \"\\n\")\n",
        "\n",
        "# Example call right after clustering:\n",
        "profile_cluster_trends(df_feat, labels)"
      ],
      "metadata": {
        "id": "Tgav8Sz951Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 3-5: Cosine Similarity Calculation for Superheroes Relationships\n",
        "Calculates the cosine similarity between superheroes, identifying the most and least similar pairs based on their powers. The results highlight relationships between characters."
      ],
      "metadata": {
        "id": "FjqcOuvvWXQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: Assumes df_feat (with PCA_PC* columns) and labels are in memory.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# =========================\n",
        "# Helpers (keep in Colab)\n",
        "# =========================\n",
        "def _pca_feature_matrix(df_feat, n_pcs=None):\n",
        "    \"\"\"Build a numeric feature matrix from PCA_PC* columns.\"\"\"\n",
        "    pc_cols = [c for c in df_feat.columns if c.startswith(\"PCA_PC\")]\n",
        "    pc_cols = sorted(pc_cols, key=lambda c: int(c.replace(\"PCA_PC\", \"\")))\n",
        "    if n_pcs is not None:\n",
        "        pc_cols = pc_cols[:n_pcs]\n",
        "    if not pc_cols:\n",
        "        raise ValueError(\"No PCA_PC* columns found in df_feat.\")\n",
        "    X = df_feat[pc_cols].astype(float).to_numpy()\n",
        "    return X, pc_cols\n",
        "\n",
        "def _upper_triangle_pairs(n):\n",
        "    \"\"\"Indices for unique unordered pairs (i < j).\"\"\"\n",
        "    return np.triu_indices(n, k=1)\n",
        "\n",
        "def _per_cluster_within_means(S, labels):\n",
        "    \"\"\"Mean within-cluster cosine per cluster.\"\"\"\n",
        "    out = []\n",
        "    labs = np.asarray(labels)\n",
        "    for c in sorted(np.unique(labs)):\n",
        "        idx = np.where(labs == c)[0]\n",
        "        if len(idx) < 2:\n",
        "            out.append((c, np.nan, 0))\n",
        "            continue\n",
        "        Sc = S[np.ix_(idx, idx)]\n",
        "        iu = np.triu_indices(len(idx), 1)\n",
        "        vals = Sc[iu]\n",
        "        out.append((c, float(np.mean(vals)), int(vals.size)))\n",
        "    return out\n",
        "\n",
        "def _top_within_pairs(S, labels, names=None, top_k=5):\n",
        "    \"\"\"Top similar pairs inside each cluster.\"\"\"\n",
        "    labs = np.asarray(labels)\n",
        "    rows = []\n",
        "    for c in sorted(np.unique(labs)):\n",
        "        idx = np.where(labs == c)[0]\n",
        "        if len(idx) < 2:\n",
        "            rows.append((c, []))\n",
        "            continue\n",
        "        Sc = S[np.ix_(idx, idx)]\n",
        "        iu = np.triu_indices(len(idx), 1)\n",
        "        vals = Sc[iu]\n",
        "        order = np.argsort(-vals)[:top_k]\n",
        "        pairs = []\n",
        "        for r in order:\n",
        "            i = idx[iu[0][r]]\n",
        "            j = idx[iu[1][r]]\n",
        "            a = names[i] if names is not None else str(i)\n",
        "            b = names[j] if names is not None else str(j)\n",
        "            pairs.append((a, b, float(vals[r])))\n",
        "        rows.append((c, pairs))\n",
        "    return rows\n",
        "\n",
        "# =========================\n",
        "# Main flow (show in book)\n",
        "# =========================\n",
        "\n",
        "# Step 1 - Build the feature matrix used for cosine.\n",
        "#          We use the same PCA axes we used for clustering.\n",
        "X_cos, used_cols = _pca_feature_matrix(df_feat, n_pcs=20)\n",
        "print(f\"Cosine features: {len(used_cols)} columns \"\n",
        "      f\"({used_cols[0]}, {used_cols[1]}, ...)\")\n",
        "\n",
        "# Step 2 - Compute cosine similarity between all heroes.\n",
        "#          Values range from -1 (opposites) to 1 (identical direction).\n",
        "S = cosine_similarity(X_cos)\n",
        "\n",
        "# Step 3 - Compare similarity within clusters vs between clusters.\n",
        "labs = np.asarray(labels)          # cluster label for each hero as a NumPy array\n",
        "n = len(labs)                      # number of heroes\n",
        "iu = _upper_triangle_pairs(n)      # all unique hero pairs (i < j)\n",
        "same = labs[iu[0]] == labs[iu[1]]  # True if a pair is in the same cluster\n",
        "within_vals = S[iu][same]          # cosine scores for same-cluster pairs\n",
        "between_vals = S[iu][~same]        # cosine scores for cross-cluster pairs\n",
        "\n",
        "print(\"Cosine similarity (overall):\")\n",
        "print(f\"  Within clusters:  mean={within_vals.mean():.3f}  \"\n",
        "      f\"pairs={within_vals.size}\")\n",
        "print(f\"  Between clusters: mean={between_vals.mean():.3f} \"\n",
        "      f\"pairs={between_vals.size}\\n\")\n",
        "\n",
        "# Step 4 - Show which clusters are tight vs diffuse.\n",
        "print(\"Per-cluster within means:\")\n",
        "for c, m, cnt in _per_cluster_within_means(S, labs):\n",
        "    print(f\"  Cluster {c}: mean={m:.3f} pairs={cnt}\")\n",
        "print()\n",
        "\n",
        "# Step 5 - List a few most-similar hero pairs inside each cluster.\n",
        "names = df_feat[\"name\"].tolist() if \"name\" in df_feat.columns else None\n",
        "print(\"Top within-cluster pairs:\")\n",
        "for c, pairs in _top_within_pairs(S, labs, names=names, top_k=5):\n",
        "    if not pairs:\n",
        "        print(f\"  Cluster {c}: not enough members\")\n",
        "        continue\n",
        "    print(f\"  Cluster {c}:\")\n",
        "    for a, b, v in pairs:\n",
        "        print(f\"    {a}  ~  {b}   cos={v:.3f}\")"
      ],
      "metadata": {
        "id": "-EDCOneqTekE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 3-6A: Target screening for fine-tuning candidates"
      ],
      "metadata": {
        "id": "YF_POPSiN2y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: SUPERHEROES_INFO_POWERS2_URL\n",
        "# Target screening for fine-tuning candidates\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "\n",
        "def make_power_tier(df: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"Low/Medium/High from z(OPR)+z(SDR); preserves NaNs.\"\"\"\n",
        "    if {\"OPR\", \"SDR\"}.issubset(df.columns):\n",
        "        s = df[[\"OPR\", \"SDR\"]].apply(pd.to_numeric, errors=\"coerce\")\n",
        "        z = (s - s.mean()) / s.std(ddof=0)\n",
        "        idx = z.sum(axis=1)\n",
        "        q1, q2 = idx.quantile([1/3, 2/3])\n",
        "        bins = [-np.inf, q1, q2, np.inf]\n",
        "        out = pd.cut(idx, bins=bins, labels=[\"Low\", \"Medium\", \"High\"])\n",
        "        out.name = \"Power_Tier\"\n",
        "        return out\n",
        "    return pd.Series(pd.NA, index=df.index, name=\"Power_Tier\", dtype=\"object\")\n",
        "\n",
        "def make_publisher3(df: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"Collapse Publisher → Marvel / DC / Other.\"\"\"\n",
        "    pub = df.get(\"Publisher\")\n",
        "    if pub is None:\n",
        "        return pd.Series(pd.NA, index=df.index, name=\"Publisher3\", dtype=\"object\")\n",
        "    s = pub.astype(str)\n",
        "    is_marvel = s.str.contains(\"marvel\", case=False, na=False)\n",
        "    is_dc = s.str.contains(\"dc\", case=False, na=False)\n",
        "    out = np.where(is_marvel, \"Marvel\", np.where(is_dc, \"DC\", \"Other\"))\n",
        "    return pd.Series(out, index=df.index, name=\"Publisher3\")\n",
        "\n",
        "def target_health(df: pd.DataFrame, col: str) -> dict:\n",
        "    \"\"\"Quick diagnostics for a classification target.\"\"\"\n",
        "    s = df[col]\n",
        "    miss = float(s.isna().mean())\n",
        "    k = int(s.dropna().nunique())\n",
        "    vc_all = s.value_counts(dropna=False).sort_values(ascending=False)\n",
        "    vc_nomiss = s.dropna().value_counts()\n",
        "    maj = float(vc_nomiss.iloc[0] / vc_nomiss.sum()) if not vc_nomiss.empty else np.nan\n",
        "    if np.isnan(maj):\n",
        "        verdict = \"unusable (no data)\"\n",
        "    elif maj >= 0.80:\n",
        "        verdict = \"very imbalanced\"\n",
        "    elif maj >= 0.60:\n",
        "        verdict = \"imbalanced but workable\"\n",
        "    else:\n",
        "        verdict = \"reasonably balanced\"\n",
        "    return {\n",
        "        \"target\": col,\n",
        "        \"classes\": k,\n",
        "        \"missing_pct\": round(miss, 3),\n",
        "        \"majority_baseline\": round(maj, 3) if not np.isnan(maj) else np.nan,\n",
        "        \"verdict\": verdict,\n",
        "        \"counts\": vc_all.to_dict(),\n",
        "    }\n",
        "\n",
        "def print_health_summary(results: list):\n",
        "    \"\"\"Pretty print a compact summary for each target.\"\"\"\n",
        "    for r in results:\n",
        "        print(f\"\\n=== {r['target']} ===\")\n",
        "        print(f\"classes: {r['classes']} | missing: {r['missing_pct']}\")\n",
        "        print(f\"majority baseline: {r['majority_baseline']} | {r['verdict']}\")\n",
        "        counts = pd.Series(r[\"counts\"]).sort_values(ascending=False).head(8)\n",
        "        print(counts.to_string())\n",
        "\n",
        "# ---------- Main ----------\n",
        "\n",
        "df = pd.read_csv(SUPERHEROES_INFO_POWERS2_URL).copy()\n",
        "\n",
        "# Derived targets\n",
        "df[\"Power_Tier\"] = make_power_tier(df)\n",
        "df[\"Publisher3\"] = make_publisher3(df)\n",
        "\n",
        "# Choose targets present in the file\n",
        "candidates = [\n",
        "    c for c in [\"Alignment\", \"Species\", \"Gender\",\n",
        "                \"has_powers_source\", \"Power_Tier\", \"Publisher3\"]\n",
        "    if c in df.columns\n",
        "]\n",
        "\n",
        "results = [target_health(df, c) for c in candidates]\n",
        "print_health_summary(results)"
      ],
      "metadata": {
        "id": "ZgOEMSmKN7q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 3-6: (Tuning Step 1) Build Target, Select Features and Encode\n",
        "This listing prepares the dataset for supervised modeling. It starts by loading the updated superheroes_info_powers2 file and creating a clean binary target (“Marvel” vs “DC”) with make_target. Next, it organizes the feature set in one place using select_columns, keeping 40 PCA components, a handful of numeric fields (Height, Weight, OPR, SDR), and text fields (Alignment, Species, Gender)—intentionally excluding name. With the features set, we encode the categorical variables and target using scikit-learn’s LabelEncoder, producing numeric inputs ready for training. Finally, we print quick stats so you can check class balance and feature shapes before moving ahead."
      ],
      "metadata": {
        "id": "ERcXuY8iiEeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: SUPERHEROES_INFO_POWERS2_URL\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# Helpers (omitted from the printed book)\n",
        "\n",
        "def make_target(df, source_col, keep_values, mapping):\n",
        "    \"\"\"\n",
        "    Filter to rows whose publisher matches any value in keep_values (case-insens).\n",
        "    Then map those rows to a clean target label using 'mapping'.\n",
        "    \"\"\"\n",
        "    s = df[source_col].astype(str)\n",
        "\n",
        "    # Build a safe OR-pattern (escape in case values contain special chars)\n",
        "    or_pat = \"|\".join(re.escape(v) for v in keep_values)\n",
        "\n",
        "    mask = s.str.contains(or_pat, case=False, na=False)\n",
        "    out = df[mask].copy()\n",
        "\n",
        "    # Row-wise mapping to \"Marvel\"/\"DC\" from the raw publisher string\n",
        "    out[\"target\"] = np.select(\n",
        "        [out[source_col].str.contains(k, case=False, na=False) for k in mapping],\n",
        "        [mapping[k] for k in mapping],\n",
        "        default=\"Other\"  # should not occur because of mask, but kept for safety\n",
        "    )\n",
        "    return out\n",
        "\n",
        "\n",
        "def select_columns(df, base_num_cols, text_cols, n_pcs=40):\n",
        "    \"\"\"\n",
        "    Pick features:\n",
        "      - numeric: PCA_PC1..PCA_PC{n_pcs} + base numeric columns\n",
        "      - text: the provided text columns\n",
        "    Require full PCA block; coerce text to strings to avoid NaNs.\n",
        "    \"\"\"\n",
        "    pc_cols = [f\"PCA_PC{i}\" for i in range(1, n_pcs + 1)]\n",
        "    df = df.dropna(subset=pc_cols).copy()\n",
        "\n",
        "    num_cols = pc_cols + [c for c in base_num_cols if c in df.columns]\n",
        "    txt_cols = [c for c in text_cols if c in df.columns]\n",
        "\n",
        "    for c in txt_cols:\n",
        "        df[c] = df[c].astype(str).fillna(\"NA\")\n",
        "\n",
        "    return df, num_cols, txt_cols\n",
        "\n",
        "# ---------- control flow ----------\n",
        "\n",
        "# Load latest version of the superheroes dataset\n",
        "df = pd.read_csv(SUPERHEROES_INFO_POWERS2_URL).copy()\n",
        "\n",
        "target_feature = \"Publisher\"              # column predict\n",
        "\n",
        "# Target: Marvel vs DC (concert from full strings e.g., 'Marvel Comics')\n",
        "df = make_target(\n",
        "    df,\n",
        "    source_col=target_feature,\n",
        "    keep_values=[\"marvel comics\", \"dc comics\"],\n",
        "    mapping={\"marvel comics\": \"Marvel\", \"dc comics\": \"DC\"}\n",
        ")\n",
        "\n",
        "# Column picks include both Numeric and Textual Fields\n",
        "base_numeric = [\"Height\", \"Weight\", \"OPR\", \"SDR\"]\n",
        "text_fields  = [\"Alignment\", \"Species\", \"Gender\"]  # do NOT include 'name'\n",
        "\n",
        "# Select features in one place (adds PCA_PC1..PCA_PC40 to numeric_cols)\n",
        "df, numeric_cols, text_cols = select_columns(\n",
        "    df, base_num_cols=base_numeric, text_cols=text_fields, n_pcs=40\n",
        ")\n",
        "\n",
        "# Encode Features and Target\n",
        "X = df[text_cols + numeric_cols]          # feature frame\n",
        "le = LabelEncoder()                       # map labels to 0..K-1\n",
        "y = le.fit_transform(df[\"target\"])        # encodes \"Marvel\"/\"DC\" -> ints\n",
        "\n",
        "# Print Statistics\n",
        "print(f\"Rows with full PC block: {len(df)}\")\n",
        "print(\"Target counts:\", df[\"target\"].value_counts().to_dict())\n",
        "print(f\"Text cols ({len(text_cols)}): {text_cols}\")\n",
        "print(f\"Numeric cols ({len(numeric_cols)}): {numeric_cols}...\")"
      ],
      "metadata": {
        "id": "dy4fdE_Qfcja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 3-7: (Tuning Step 2) Preparing Train/Test and Establishing a Baseline\n",
        "This code sets up the superhero dataset for training and evaluation. It splits the data into train and test sets while preserving the Marvel/DC class balance, then applies preprocessing: one-hot encoding for text fields (alignment, species, gender) and median imputation for numeric fields (Height, Weight, OPR, SDR, plus PCA features). With the features transformed, it establishes a baseline by always predicting the majority class from the training labels, giving a simple reference point for accuracy and macro-F1 before moving on to more advanced models."
      ],
      "metadata": {
        "id": "qOMMz9WnWF4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: X, y, text_cols, numeric_cols\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Helpers (omitted from the printed book)\n",
        "def build_preprocessor(text_cols, num_cols):\n",
        "    \"\"\"One-hot for text; median impute for numeric.\"\"\"\n",
        "    return ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"text\",\n",
        "             OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True),\n",
        "             text_cols),\n",
        "            (\"num\",\n",
        "             Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))]),\n",
        "             num_cols),\n",
        "        ],\n",
        "        remainder=\"drop\",\n",
        "    )\n",
        "\n",
        "def report(tag, y_true, y_pred):\n",
        "    \"\"\"Print accuracy and macro-F1 in one line.\"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    print(f\"{tag:24} acc: {acc:.3f} | macro-F1: {f1m:.3f}\")\n",
        "    return acc, f1m\n",
        "\n",
        "# ----------- CONTROL FLOW -----------\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Train/test split (80/20) with stratify to preserve class ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 2. Preprocessing: One-hot encode text, median impute numeric\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"text\",\n",
        "         OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True),\n",
        "         text_cols),\n",
        "        (\"num\",\n",
        "         Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))]),\n",
        "         numeric_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        ")\n",
        "\n",
        "# 3. Fit on training data only (avoid leakage)\n",
        "X_train_encoded = preprocessor.fit_transform(X_train)\n",
        "X_test_encoded  = preprocessor.transform(X_test)\n",
        "\n",
        "# 4. Baseline: always predict the majority class from training labels\n",
        "majority_class = np.bincount(y_train).argmax()\n",
        "y_base = np.full_like(y_test, majority_class)\n",
        "\n",
        "# 5. Calculate and print base line accuracy score\n",
        "acc = accuracy_score(y_test, y_base)\n",
        "f1m = f1_score(y_test, y_base, average=\"macro\")\n",
        "\n",
        "print(f\"Train size: {X_train.shape[0]} | Test size: {X_test.shape[0]}\")\n",
        "print(\"X_train_encoded:\", X_train_encoded.shape,\n",
        "      \"| X_test_encoded:\", X_test_encoded.shape)\n",
        "print(\"y_train:\", y_train.size, \"| y_test:\", y_test.size)\n",
        "print(f\"Baseline (majority) acc: {acc:.3f} | macro-F1: {f1m:.3f}\")"
      ],
      "metadata": {
        "id": "GLwkreBppHVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ac84548"
      },
      "source": [
        "### Listing 3-8: (Tuning Step 3) Pre-Fine-Tune GradientBoosting Classifier\n",
        "\n",
        "This snippet trains a vanilla HistGradientBoostingClassifier on the already-encoded features to establish a baseline we’ll try to beat in the next step. It converts the sparse design matrices to dense (HGB requires dense), fits the model, and reports accuracy and macro-F1 plus a class-wise report.\n",
        "Prereqs from the previous cell: X_train_encoded, X_test_encoded, y_train, y_test, and le (a LabelEncoder fitted on the target) are already defined."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: X_train_encoded, X_test_encoded, y_train, y_test, le\n",
        "# Pre-fine-tune model: HistGradientBoosting (simple baseline)\n",
        "# Goal: a clean starting point we can beat in the next step\n",
        "\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# HGB needs dense arrays (not sparse CSR). Convert once.\n",
        "X_train_dense = (X_train_encoded.toarray()\n",
        "                 if hasattr(X_train_encoded, \"toarray\")\n",
        "                 else X_train_encoded)\n",
        "X_test_dense = (X_test_encoded.toarray()\n",
        "                if hasattr(X_test_encoded, \"toarray\")\n",
        "                else X_test_encoded)\n",
        "\n",
        "# Barebones classifier\n",
        "hgb = HistGradientBoostingClassifier()\n",
        "\n",
        "# Fit → Predict → Score\n",
        "hgb.fit(X_train_dense, y_train)   # train the model\n",
        "y_pred = hgb.predict(X_test_dense)\n",
        "\n",
        "# Prepare and Print Classification Report\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "f1m = f1_score(y_test, y_pred, average=\"macro\")\n",
        "\n",
        "print(f\"HGB (baseline)   acc: {acc:.3f} | macro-F1: {f1m:.3f}\")\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))"
      ],
      "metadata": {
        "id": "RCef-qS5pUB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 3-9: (Tuning Step 4) Fine Tuning of GradientBoosting Classifier\n",
        "We now apply the **HistGradientBoostingClassifier** with parameters tuned through earlier experiments. The learning rate, tree depth, and L2 regularization were adjusted to strike a balance between model flexibility and generalization. This “best pass” run shows how testing a range of settings lets us converge on a strong configuration. The final model combines encoded text and numeric features, demonstrating how systematic fine-tuning can push accuracy and macro-F1 well above  \n",
        "baseline performance."
      ],
      "metadata": {
        "id": "-IMUG9aO4WGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: X_train_dense, X_test_dense, y_test, y_train, le\n",
        "# Fine-tuned HistGradientBoosting\n",
        "\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "clf = HistGradientBoostingClassifier(\n",
        "    learning_rate=0.3,        # step size for boosting updates\n",
        "    max_depth=5,              # depth of each tree (controls complexity)\n",
        "    l2_regularization=0.1,    # weight decay to reduce overfitting\n",
        "    max_iter=100,             # enough trees to converge\n",
        "    random_state=42           # reproducible results\n",
        ")\n",
        "\n",
        "# Fit the model to the training data\n",
        "clf.fit(X_train_dense, y_train)   # train with encoded features\n",
        "pred = clf.predict(X_test_dense)  # predict on held-out test set\n",
        "\n",
        "# Prepare and Print Classification Report\n",
        "# Evaluate results with accuracy and macro-F1\n",
        "acc = accuracy_score(y_test, pred)\n",
        "f1m = f1_score(y_test, pred, average=\"macro\")\n",
        "\n",
        "print(f\"HGB (best params)  acc: {acc:.3f} | macro-F1: {f1m:.3f}\\n\")\n",
        "print(\"Detailed report:\")\n",
        "print(classification_report(y_test, pred, target_names=le.classes_))"
      ],
      "metadata": {
        "id": "_Usdqxp-JM-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot history of tunning accuracy"
      ],
      "metadata": {
        "id": "A--w8RupmLEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: labels, values\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = [\"3-class\\nbaseline\", \"Binary\\nbaseline\", \"Untuned\\nHGB\", \"Tuned\\nHGB\"]\n",
        "values = [52, 65, 77, 84]\n",
        "colors = [\"yellow\", \"gray\", \"red\", \"blue\"]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "bars = plt.bar(labels, values, color=colors)\n",
        "plt.title(\"Model Progress: Accuracy (%)\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.ylim(0, 100)\n",
        "\n",
        "for bar, val in zip(bars, values):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, height + 1, f\"{val}%\",\n",
        "             ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XK0vsCTFmQ6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "699f4662"
      },
      "source": [
        "### Listing 3-9A: Model Selection and Fine-Tuning Analysis Sweep\n",
        "\n",
        "This program runs three strong classifiers side by side: LinearSVC (linear support vector machine), Logistic Regression, and HistGradientBoosting. Each is trained on the same encoded dataset, with small sweeps over their key parameters (regularization strength for linear models, learning rate and tree depth for boosting). The results show not only which model performs best, but also which parameter settings give the highest accuracy and macro-F1, guiding us toward a tuned final model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: X_train_encoded, X_test_encoded, X_train_dense, X_test_dense,\n",
        "#          y_train, y_test\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def evaluate(tag, y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    print(f\"{tag:28} acc: {acc:.3f} | macro-F1: {f1m:.3f}\")\n",
        "    return acc, f1m, y_pred\n",
        "\n",
        "def best_of(rows):\n",
        "    # rows: [(acc, f1m, pred, params_dict), ...]\n",
        "    return max(rows, key=lambda t: (t[0], t[1]))\n",
        "\n",
        "# ---------- LinearSVC sweep (sparse OK) ----------\n",
        "svc_trials = []\n",
        "for C in [0.5, 1.0, 1.5, 2.0]:\n",
        "    for balanced in (True, False):\n",
        "        clf = LinearSVC(\n",
        "            C=C,\n",
        "            class_weight=(\"balanced\" if balanced else None),\n",
        "            random_state=42\n",
        "        )\n",
        "        clf.fit(X_train_encoded, y_train)\n",
        "        pred = clf.predict(X_test_encoded)\n",
        "        svc_trials.append(\n",
        "            evaluate(f\"SVC C={C:<3} bal={balanced}\", y_test, pred) +\n",
        "            ({\"C\": C, \"balanced\": balanced},)\n",
        "        )\n",
        "\n",
        "svc_best = best_of(svc_trials)\n",
        "print(\"\\nBest LinearSVC params:\", svc_best[3])\n",
        "print(classification_report(y_test, svc_best[2]))\n",
        "\n",
        "# ---------- Logistic Regression sweep (sparse OK) ----------\n",
        "lr_trials = []\n",
        "for C in [0.5, 1.0, 2.0, 4.0]:\n",
        "    for balanced in (True, False):\n",
        "        clf = LogisticRegression(\n",
        "            C=C,\n",
        "            penalty=\"l2\",\n",
        "            solver=\"liblinear\",          # solid with sparse & binary\n",
        "            class_weight=(\"balanced\" if balanced else None),\n",
        "            max_iter=2000,\n",
        "            n_jobs=-1,\n",
        "            random_state=42\n",
        "        )\n",
        "        clf.fit(X_train_encoded, y_train)\n",
        "        pred = clf.predict(X_test_encoded)\n",
        "        lr_trials.append(\n",
        "            evaluate(f\"LogReg C={C:<3} bal={balanced}\", y_test, pred) +\n",
        "            ({\"C\": C, \"balanced\": balanced},)\n",
        "        )\n",
        "\n",
        "lr_best = best_of(lr_trials)\n",
        "print(\"\\nBest LogisticRegression params:\", lr_best[3])\n",
        "print(classification_report(y_test, lr_best[2]))\n",
        "\n",
        "# ---------- HistGradientBoosting sweep (dense required) ----------\n",
        "# Include the known strong combo: lr=0.3, depth=5, l2=0.1\n",
        "hgb_trials = []\n",
        "for lr in [0.10, 0.20, 0.30, 0.40]:\n",
        "    for depth in [5]:\n",
        "        for l2 in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]:\n",
        "            clf = HistGradientBoostingClassifier(\n",
        "                learning_rate=lr,\n",
        "                max_depth=depth,\n",
        "                l2_regularization=l2,\n",
        "                random_state=42\n",
        "            )\n",
        "            clf.fit(X_train_dense, y_train)\n",
        "            pred = clf.predict(X_test_dense)\n",
        "            hgb_trials.append(\n",
        "                evaluate(f\"HGB lr={lr:<3} d={depth} l2={l2:<3}\", y_test, pred) +\n",
        "                ({\"lr\": lr, \"depth\": depth, \"l2\": l2},)\n",
        "            )\n",
        "\n",
        "hgb_best = best_of(hgb_trials)\n",
        "print(\"\\nBest HistGB params:\", hgb_best[3])\n",
        "print(classification_report(y_test, hgb_best[2]))\n",
        "\n",
        "# ---------- Overall winner ----------\n",
        "winners = [\n",
        "    (\"LinearSVC\",) + svc_best,   # (name, acc, f1, pred, params)\n",
        "    (\"LogReg\",)    + lr_best,\n",
        "    (\"HistGB\",)    + hgb_best,\n",
        "]\n",
        "overall = max(winners, key=lambda t: (t[1], t[2]))\n",
        "\n",
        "print(\"\\n=== Overall best ===\")\n",
        "print(\"Model:\", overall[0], \"| params:\", overall[4])\n",
        "print(f\"acc: {overall[1]:.3f} | macro-F1: {overall[2]:.3f}\\n\")\n",
        "print(\"Detailed report (overall best):\")\n",
        "print(classification_report(y_test, overall[3]))"
      ],
      "metadata": {
        "id": "ZUiJdJsUx6-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 3-9B - SMOTE sniff test"
      ],
      "metadata": {
        "id": "-nyylREb8JBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: X_train_encoded, X_test_encoded, y_train, y_test from earlier cells\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def to_dense(X):\n",
        "    return X.toarray() if hasattr(X, \"toarray\") else X\n",
        "\n",
        "def score(tag, clf, Xtr, ytr, Xte, yte):\n",
        "    clf.fit(Xtr, ytr)\n",
        "    pred = clf.predict(Xte)\n",
        "    acc = accuracy_score(yte, pred)\n",
        "    f1m = f1_score(yte, pred, average=\"macro\")\n",
        "    print(f\"{tag:22} acc: {acc:.3f} | macro-F1: {f1m:.3f}\")\n",
        "    return acc, f1m\n",
        "\n",
        "# Convert to dense (HGB + SMOTE need dense arrays)\n",
        "Xtr = to_dense(X_train_encoded)\n",
        "Xte = to_dense(X_test_encoded)\n",
        "\n",
        "# Tuned HGB settings from our fine-tuning\n",
        "hgb = HistGradientBoostingClassifier(\n",
        "    learning_rate=0.3, max_depth=5, l2_regularization=0.1,\n",
        "    max_iter=100, random_state=42\n",
        ")\n",
        "\n",
        "# 1) No SMOTE\n",
        "score(\"HGB (no SMOTE)\", hgb, Xtr, y_train, Xte, y_test)\n",
        "\n",
        "# 2) With SMOTE on the training set only\n",
        "sm = SMOTE(random_state=42)\n",
        "Xtr_sm, ytr_sm = sm.fit_resample(Xtr, y_train)\n",
        "\n",
        "print(\"Train balance before:\", Counter(y_train))\n",
        "print(\"Train balance after :\", Counter(ytr_sm))\n",
        "\n",
        "score(\"HGB (with SMOTE)\", hgb, Xtr_sm, ytr_sm, Xte, y_test)"
      ],
      "metadata": {
        "id": "FYEDtmte8N3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 3-10: (Tuning Step 5) Visualizing a Confusion Matrix for Model Evaluation\n",
        "Generates and visualizes the confusion matrix, showing the breakdown of correct and incorrect predictions for each class in the dataset."
      ],
      "metadata": {
        "id": "8AizO7lS1yDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix for HistGradientBoosting predictions\n",
        "# REQUIRES: y_test (true), pred (preds), and `le` from the target encoding step\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# 1) Build the 2x2 confusion matrix in the encoder's class order\n",
        "class_indices = np.arange(len(le.classes_))   # e.g., [0, 1]\n",
        "cm = confusion_matrix(y_test, pred, labels=class_indices)\n",
        "tick_names = le.classes_                      # e.g., [\"DC Comics\", \"Marvel Comics\"]\n",
        "\n",
        "# 2) Create a cell-by-cell color grid:\n",
        "#    [ [UL (TP class 0)=RED,  UR (FN class 0)=YELLOW],\n",
        "#      [LL (FP class 0)=YELLOW, LR (TP class 1)=BLUE] ]\n",
        "colors = np.array([\n",
        "    [(1.0, 0.0, 0.0, 0.70),  (1.0, 1.0, 0.0, 0.70)],\n",
        "    [(1.0, 1.0, 0.0, 0.70),  (0.0, 0.0, 1.0, 0.70)],\n",
        "])\n",
        "\n",
        "# 3) Plot the colored grid and overlay the counts\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "ax.imshow(colors, interpolation='none', aspect='equal')\n",
        "\n",
        "# Add the numbers (bigger font)\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(j, i, str(cm[i, j]),\n",
        "                ha='center', va='center',\n",
        "                fontsize=16, fontweight='bold', color='black')\n",
        "\n",
        "# 4) Axes, ticks, and labels\n",
        "ax.set_xticks([0, 1], labels=tick_names)\n",
        "ax.set_yticks([0, 1], labels=tick_names)\n",
        "ax.set_xlabel(\"Predicted\", fontsize=12)\n",
        "ax.set_ylabel(\"True\", fontsize=12)\n",
        "ax.set_title(\"Confusion Matrix (HistGB)\", fontsize=13, pad=10)\n",
        "\n",
        "# Optional: add thin gridlines between cells for readability\n",
        "ax.set_xticks(np.arange(-0.5, 2, 1), minor=True)\n",
        "ax.set_yticks(np.arange(-0.5, 2, 1), minor=True)\n",
        "ax.grid(which='minor', color='white', linewidth=1.5)\n",
        "ax.tick_params(which='minor', bottom=False, left=False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VJKRjoMZ13Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2879cb3"
      },
      "source": [
        "### Listing 3-11 - SHAP values for HistGradientBoosting\n",
        "Calculates SHAP (SHapley Additive exPlanations) values for the trained `HistGradientBoostingClassifier`. SHAP values help interpret the model's predictions by showing how much each feature contributes to the output for individual instances. The code ranks features by their average absolute SHAP value to identify the most important features in the model's predictions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SHAP on HistGradientBoosting (numbers only)\n",
        "# Requires: clf (fitted HGB), preprocessor, text_cols, numeric_cols,\n",
        "#           X_train_encoded, X_test_encoded\n",
        "\n",
        "import numpy as np\n",
        "import shap\n",
        "\n",
        "def to_dense(X):\n",
        "    return X.toarray() if hasattr(X, \"toarray\") else X\n",
        "\n",
        "# Dense matrices for HGB\n",
        "X_train_dense = to_dense(X_train_encoded)\n",
        "X_test_dense  = to_dense(X_test_encoded)\n",
        "\n",
        "# Feature names from the ColumnTransformer (OneHot + numeric)\n",
        "feat_names = preprocessor.get_feature_names_out(text_cols + numeric_cols)\n",
        "\n",
        "# Use TreeExplainer with default model_output=\"raw\" (required for HGB)\n",
        "explainer = shap.TreeExplainer(clf)\n",
        "\n",
        "# SHAP values for the test set; disable additivity check to avoid numerical issues\n",
        "shap_vals = explainer.shap_values(X_test_dense, check_additivity=False)\n",
        "\n",
        "# For binary classifiers, SHAP may return a list (old) or an array (new). Normalize:\n",
        "if isinstance(shap_vals, list):\n",
        "    shap_raw = shap_vals[1]        # positive class\n",
        "else:\n",
        "    shap_raw = shap_vals           # shape: (n_samples, n_features)\n",
        "\n",
        "# Rank features by mean |SHAP|\n",
        "mean_abs = np.mean(np.abs(shap_raw), axis=0)\n",
        "order = np.argsort(-mean_abs)\n",
        "\n",
        "top_k = 15\n",
        "print(\"Top features by mean |SHAP| (HGB, raw margin):\")\n",
        "for rank, idx in enumerate(order[:top_k], 1):\n",
        "    print(f\"{rank:2d}. {feat_names[idx]}: {mean_abs[idx]:.5f}\")\n",
        "\n",
        "# Optional: show base value (raw margin) and a per-sample check\n",
        "expected = float(np.atleast_1d(explainer.expected_value)[-1])  # scalar\n",
        "row = 0\n",
        "margin = expected + shap_raw[row].sum()  # raw margin (log-odds)\n",
        "prob = 1.0 / (1.0 + np.exp(-margin))     # implied probability\n",
        "\n",
        "print(f\"\\nBase value (raw margin): {expected:.5f}\")\n",
        "print(f\"Sample {row} raw margin (expected + sum SHAP): {margin:.5f}\")\n",
        "print(f\"Sample {row} implied probability (sigmoid(raw)): {prob:.3f}\")"
      ],
      "metadata": {
        "id": "Av8CbZ3M-koF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 3-12 – Beethoven Edge Feature Visualization\n",
        "\n",
        "This program demonstrates how classical computer vision techniques extract visual features from images. It uses edge detection and gradient orientation analysis to highlight prominent contours in red, blue, and yellow—mimicking how early machine learning systems learned to perceive structure before the era of deep learning."
      ],
      "metadata": {
        "id": "yB16ynZ1eTGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Beethoven Feature Visualization — Bold Red/Blue/Yellow Edges on White\n",
        "# Downloads the public-domain portrait and renders classical CV features.\n",
        "\n",
        "import urllib.request, numpy as np, cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ========= Download image =========\n",
        "URL = \"https://opensourceai-book.github.io/code/media/beethoven.png\"\n",
        "SAVE_AS = \"Beethoven.jpg\"\n",
        "urllib.request.urlretrieve(URL, SAVE_AS)\n",
        "\n",
        "# ========= Inputs =========\n",
        "FILENAME = SAVE_AS                # downloaded file\n",
        "MAX_SIDE = 1200                   # resize long side to speed up processing\n",
        "\n",
        "# ========= One-knob sensitivity (0..1) =========\n",
        "EDGE_SENSITIVITY = 0.75           # higher = keep more edges; lower = sparser\n",
        "\n",
        "# ========= Derived settings (adjusted by sensitivity) =========\n",
        "CANNY_LOW  = int(60  - 30*EDGE_SENSITIVITY)   # lower edge threshold\n",
        "CANNY_HIGH = int(150 - 40*EDGE_SENSITIVITY)   # upper edge threshold\n",
        "KEEP_TOP_PERCENT = 0.45 + 0.45*EDGE_SENSITIVITY  # fraction of strongest edges to keep\n",
        "DILATE_ITERS = 2 if EDGE_SENSITIVITY < 0.8 else 3  # stroke thickness\n",
        "MIN_REGION_SIZE = int(80 - 60*EDGE_SENSITIVITY)    # remove tiny blobs below this area\n",
        "SMOOTH_MASK = 5                                    # blur kernel merging nearby fragments\n",
        "POST_BLUR_KEEP = 0.20                              # post-blur threshold; lower keeps more\n",
        "ALPHA_MIN, ALPHA_MAX = 0.90, 1.00                  # opacity range for strokes\n",
        "UNDERLAY_GRAY = 200                                # light gray halo under strokes (0..255)\n",
        "\n",
        "# ========= Superhero palette (RGB) =========\n",
        "RED  = np.array([220,  35,  40], dtype=np.float32) # steep (near-vertical) edges\n",
        "BLUE = np.array([ 50,  90, 210], dtype=np.float32) # broad midrange orientations\n",
        "YEL  = np.array([255, 235,  80], dtype=np.float32) # shallow (near-horizontal) edges\n",
        "\n",
        "# ---------- Load & resize ----------\n",
        "img = Image.open(FILENAME).convert(\"RGB\")\n",
        "w, h = img.size\n",
        "s = min(1.0, MAX_SIDE / max(w, h))\n",
        "if s < 1.0:\n",
        "    img = img.resize((int(w*s), int(h*s)), Image.Resampling.LANCZOS)\n",
        "rgb = np.array(img, dtype=np.uint8)\n",
        "H, W = rgb.shape[:2]\n",
        "\n",
        "# ---------- Edge feature extraction ----------\n",
        "gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)\n",
        "gray_blur = cv2.GaussianBlur(gray, (3,3), 0)\n",
        "\n",
        "edges = cv2.Canny(gray_blur, CANNY_LOW, CANNY_HIGH)\n",
        "edges = cv2.dilate(edges, np.ones((2,2), np.uint8), iterations=DILATE_ITERS)\n",
        "\n",
        "gx = cv2.Sobel(gray_blur, cv2.CV_32F, 1, 0, ksize=3)\n",
        "gy = cv2.Sobel(gray_blur, cv2.CV_32F, 0, 1, ksize=3)\n",
        "mag = np.sqrt(gx*gx + gy*gy)\n",
        "ang = (np.degrees(np.arctan2(gy, gx)) + 180.0) % 180.0\n",
        "\n",
        "# ---------- Keep strongest edges ----------\n",
        "mask_edges = edges > 0\n",
        "m_vals = mag[mask_edges]\n",
        "thr = np.quantile(m_vals, 1.0 - KEEP_TOP_PERCENT) if m_vals.size else 0.0\n",
        "key = (mask_edges & (mag >= thr)).astype(np.uint8)\n",
        "\n",
        "# Remove tiny blobs\n",
        "n, labels_cc, stats, _ = cv2.connectedComponentsWithStats(key, connectivity=8)\n",
        "for i in range(1, n):\n",
        "    if stats[i, cv2.CC_STAT_AREA] < max(1, MIN_REGION_SIZE):\n",
        "        key[labels_cc == i] = 0\n",
        "\n",
        "# Merge fragments then re-threshold\n",
        "key = cv2.GaussianBlur(key.astype(np.float32), (SMOOTH_MASK, SMOOTH_MASK), 0)\n",
        "key = (key > POST_BLUR_KEEP).astype(np.uint8)\n",
        "\n",
        "# Opacity from normalized magnitude\n",
        "alpha = np.zeros_like(mag, dtype=np.float32)\n",
        "if m_vals.size:\n",
        "    mn, mx = m_vals.min(), m_vals.max()\n",
        "    sel = key.astype(bool)\n",
        "    alpha[sel] = (mag[sel] - mn) / (mx - mn + 1e-8)\n",
        "alpha = (ALPHA_MIN + (ALPHA_MAX - ALPHA_MIN) * alpha).astype(np.float32)\n",
        "\n",
        "# ---------- Color by orientation ----------\n",
        "bin_yel  = (key==1) & (ang < 45)\n",
        "bin_blue = (key==1) & (ang >= 45) & (ang < 135)\n",
        "bin_red  = (key==1) & (ang >= 135)\n",
        "\n",
        "# ---------- Compose on white ----------\n",
        "canvas = np.ones((H, W, 3), dtype=np.uint8) * 255\n",
        "if UNDERLAY_GRAY:\n",
        "    glow = cv2.GaussianBlur(key.astype(np.float32), (5,5), 0)\n",
        "    under = np.full((H, W, 3), UNDERLAY_GRAY, dtype=np.uint8)\n",
        "    canvas = (canvas*(1.0 - glow[...,None]) + under*glow[...,None]).astype(np.uint8)\n",
        "\n",
        "def paint(mask, color):\n",
        "    a = alpha[..., None]\n",
        "    overlay = (255.0*(1.0 - a) + color*a).astype(np.uint8)\n",
        "    canvas[mask] = overlay[mask]\n",
        "\n",
        "paint(bin_red,  RED)\n",
        "paint(bin_blue, BLUE)\n",
        "paint(bin_yel,  YEL)\n",
        "\n",
        "# ---------- Save ----------\n",
        "out_name = \"beethoven_edges_on_white.png\"\n",
        "Image.fromarray(canvas).save(out_name)\n",
        "\n",
        "plt.imshow(canvas); plt.axis(\"off\"); plt.tight_layout(); plt.show()\n",
        "print(f\"Saved: {out_name}\")"
      ],
      "metadata": {
        "id": "dumWOJYreVRU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}