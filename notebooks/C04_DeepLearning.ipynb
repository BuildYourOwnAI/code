{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 4: Deep Learning\n",
        "\n",
        "**Welcome to Chapter 4**. This notebook contains the listings for Chapter 4, which explains the fundamentals of Deep Learning in PyTorch."
      ],
      "metadata": {
        "id": "w7DzfChwAQNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listing 4-1 A Simply PyTorch Model\n",
        "This listing implements a subset of the general skeleton for the end-to-end lifecycle of a PyTorch project, which includes loading and preparing data, defining or loading a model, specifying the loss function and optimizer, training with validation, evaluating performance, and saving the model for later use. This code demonstrates the essential stages of a working PyTorch program without including optional steps such as inference pipelines or model deployment."
      ],
      "metadata": {
        "id": "Wcad3HisQ7DA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# 1. Load and prepare data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load the full MNIST training set (60,000) and the official test set (10,000)\n",
        "full_train_dataset = datasets.MNIST(\".\", train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(\".\", train=False, transform=transform)\n",
        "\n",
        "# Split the 60,000 training examples into train + validation\n",
        "train_size = int(0.9 * len(full_train_dataset))  # 54,000\n",
        "val_size = len(full_train_dataset) - train_size  # 6,000\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1000,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1000,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# 2. Define model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)   # flatten 28x28 -> 784\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)        # raw class scores (one per digit)\n",
        "\n",
        "model = Net()\n",
        "\n",
        "# 3. Specify loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 4. Train model\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)                 # predictions (raw scores)\n",
        "        loss = criterion(output, target)     # compare predictions to labels\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # 5. Validate model during training (validation set, not test set)\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = output.argmax(dim=1)\n",
        "            correct += (preds == target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = correct / total * 100\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch + 1}: \"\n",
        "        f\"train loss={avg_train_loss:.4f}, \"\n",
        "        f\"val loss={avg_val_loss:.4f}, \"\n",
        "        f\"val acc={val_accuracy:.2f}%\"\n",
        "    )\n",
        "\n",
        "# 6. Final test (run once at the end)\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        preds = output.argmax(dim=1)\n",
        "        correct += (preds == target).sum().item()\n",
        "        total += target.size(0)\n",
        "\n",
        "test_accuracy = correct / total * 100\n",
        "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# 7. Save model\n",
        "torch.save(model.state_dict(), \"mnist_model.pt\")\n",
        "print(\"Model saved to mnist_model.pt\")\n"
      ],
      "metadata": {
        "id": "y2s0d4iqP5g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing 4-2 Hyperparameter Tuning with Optuna\n"
      ],
      "metadata": {
        "id": "XyBRDze1Edae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 4-2 Hyperparameter tuning with Optuna (learning rate + batch size)\n",
        "\n",
        "# If Optuna is not installed in your environment, uncomment:\n",
        "# !pip -q install optuna\n",
        "\n",
        "import optuna\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# ----------------------------\n",
        "# Model (same as Listing 4-1)\n",
        "# ----------------------------\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)   # flatten 28x28 -> 784\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# ----------------------------\n",
        "# Data + device\n",
        "# ----------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "full_train = datasets.MNIST(\".\", train=True, download=True, transform=transform)\n",
        "\n",
        "# Train/validation split (do NOT tune on the test set)\n",
        "train_size = int(0.9 * len(full_train))\n",
        "val_size = len(full_train) - train_size\n",
        "train_ds, val_ds = torch.utils.data.random_split(full_train, [train_size, val_size])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()        # clear old gradients\n",
        "        preds = model(x)             # forward pass\n",
        "        loss = criterion(preds, y)   # compute loss\n",
        "        loss.backward()              # backward pass\n",
        "        optimizer.step()             # update parameters\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_accuracy(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        preds = model(x).argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "def objective(trial):\n",
        "    # Suggest hyperparameters\n",
        "    lr = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
        "\n",
        "    # DataLoaders for this trial\n",
        "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=512, shuffle=False)\n",
        "\n",
        "    # Fresh model per trial\n",
        "    model = Net().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Keep trials cheap\n",
        "    epochs_per_trial = 2\n",
        "    for _ in range(epochs_per_trial):\n",
        "        train_one_epoch(model, train_loader, optimizer)\n",
        "\n",
        "    # Validation score to maximize\n",
        "    val_acc = evaluate_accuracy(model, val_loader)\n",
        "    return val_acc\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(\"  Validation accuracy:\", study.best_value)\n",
        "print(\"  Best hyperparameters:\", study.best_params)\n"
      ],
      "metadata": {
        "id": "6AS64gRDFErp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mgmlOltCVUoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listing 4-3 Using the Trained Model to Predict Custom Digits\n",
        "This code reloads the saved model and uses it to classify new images. Because the model is already trained, the process is straightforward: load the stored weights, prepare the input images using the same preprocessing steps as during training, and then make predictions on the new images.\n"
      ],
      "metadata": {
        "id": "8oeHA0stBZoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimal MNIST inference in Colab\n",
        "\n",
        "import io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# 1) Model (same architecture as training)\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# 2) Load trained weights\n",
        "model = Net()\n",
        "model.load_state_dict(torch.load(\"/content/mnist_model.pt\", map_location=\"cpu\"))\n",
        "model.eval()\n",
        "\n",
        "# 3) Preprocessing (match MNIST: 1×28×28 + same normalization)\n",
        "preprocess = T.Compose([\n",
        "    T.Grayscale(),          # ensure 1 channel, since MNIST is grayscale\n",
        "    T.Resize((28, 28)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# 4) Upload image(s) and predict\n",
        "@torch.no_grad()\n",
        "def predict(name, content):\n",
        "    img = Image.open(io.BytesIO(content))\n",
        "    x = preprocess(img).unsqueeze(0)      # [1,1,28,28]\n",
        "    pred = model(x).argmax(dim=1).item()\n",
        "    print(f\"The predicted digit for {name} is: {pred}\")\n",
        "\n",
        "print(\"Upload MNIST-like digit image(s):\")\n",
        "for fname, content in files.upload().items():\n",
        "    predict(fname, content)\n"
      ],
      "metadata": {
        "id": "X5uCSo-cBgAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing 4-4 Reusing a Trained Model to Classify New Images through Transfer Learning\n",
        "\n",
        "This code reuses the learned weights from our MNIST network and adapts it to classify letters in the EMNIST dataset. It copies the weights for the shared layers (fc1 and fc2), replaces the final lay-er (fc3) to output 26 classes, trains only that new layer for a few epochs, and then optionally fine tunes the second layer and the head with a smaller learning rate."
      ],
      "metadata": {
        "id": "67kPFerTHoyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# 1) Shared preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# 2) Model definition (same as MNIST, but allow a different number of output classes)\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)   # flatten 28x28 -> 784\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)        # class scores\n",
        "\n",
        "# 3) Load the pretrained MNIST model weights (fc1/fc2/fc3 trained for 10 digits)\n",
        "mnist_model = Net(num_classes=10)\n",
        "mnist_model.load_state_dict(torch.load(\"mnist_model.pt\", map_location=\"cpu\"))\n",
        "\n",
        "# 4) Load EMNIST Letters data (26 classes)\n",
        "# Note: EMNIST letters labels are 1..26, so we shift them to 0..25 in the loop.\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.EMNIST('.', split='letters', train=True, download=True, transform=transform),\n",
        "    batch_size=64,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.EMNIST('.', split='letters', train=False, download=True, transform=transform),\n",
        "    batch_size=1000,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# 5) Create a new model with a 26-class output head, then copy shared weights\n",
        "model = Net(num_classes=26)\n",
        "\n",
        "model.fc1.load_state_dict(mnist_model.fc1.state_dict())  # reuse low-level features\n",
        "model.fc2.load_state_dict(mnist_model.fc2.state_dict())  # reuse higher-level features\n",
        "# model.fc3 is new (26 outputs) and starts randomly initialized\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 6) # Phase 1: freeze fc1/fc2 and train only the new classifier head (fc3)\n",
        "for p in model.fc1.parameters():\n",
        "    p.requires_grad = False\n",
        "for p in model.fc2.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "optimizer = optim.Adam(model.fc3.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        target = target - 1  # 1..26 -> 0..25\n",
        "\n",
        "        optimizer.zero_grad()             # clear old gradients\n",
        "        output = model(data)              # forward pass\n",
        "        loss = criterion(output, target)  # compute loss\n",
        "        loss.backward()                   # compute gradients\n",
        "        optimizer.step()                  # update fc3 weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            target = target - 1\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = output.argmax(dim=1)\n",
        "            correct += (preds == target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(test_loader)\n",
        "    val_accuracy = correct / total * 100\n",
        "\n",
        "    print(\n",
        "        f\"[Head-only] Epoch {epoch + 1}: \"\n",
        "        f\"train loss={avg_train_loss:.4f}, \"\n",
        "        f\"val loss={avg_val_loss:.4f}, \"\n",
        "        f\"val acc={val_accuracy:.2f}%\"\n",
        "    )\n",
        "\n",
        "# 7) Phase 2 (optional): fine-tune fc2 + fc3 with a smaller learning rate\n",
        "for p in model.fc2.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    list(model.fc2.parameters()) + list(model.fc3.parameters()),\n",
        "    lr=1e-4\n",
        ")\n",
        "\n",
        "for epoch in range(2):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        target = target - 1\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            target = target - 1\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = output.argmax(dim=1)\n",
        "            correct += (preds == target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(test_loader)\n",
        "    val_accuracy = correct / total * 100\n",
        "\n",
        "    print(\n",
        "        f\"[Fine-tune] Epoch {epoch + 1}: \"\n",
        "        f\"train loss={avg_train_loss:.4f}, \"\n",
        "        f\"val loss={avg_val_loss:.4f}, \"\n",
        "        f\"val acc={val_accuracy:.2f}%\"\n",
        "    )\n",
        "\n",
        "# 8) Save the transferred model\n",
        "torch.save(model.state_dict(), \"emnist_letters_from_mnist.pt\")\n",
        "print(\"Transferred model saved to emnist_letters_from_mnist.pt\")\n"
      ],
      "metadata": {
        "id": "j5lTakpSIIGL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}