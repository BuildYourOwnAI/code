{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 5 Neuron Bulding Blocks\n",
        "Before a neural network can recognize a digit, detect objects, understand a sentence, translate text, or complete your email, it needs to *see*, *remember*, *interpret*, and *compress* information. There are four foundational deep learning architectures that make this possible: CNNs, RNNs, Transformers, and Autoencoders. Chapter 5 explains these architectures through examples and visual explanations.\n"
      ],
      "metadata": {
        "id": "3wlxmughMhwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listing 5-1 Reconstructing Images from the MNIST Dataset Using an Autoencoder\n",
        "This listing builds a small autoencoder that learns to compress and reconstruct handwritten digits from the MNIST dataset."
      ],
      "metadata": {
        "id": "OWo3R0ru3UP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Autoencoder example: reconstructing MNIST digits in PyTorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1. Load MNIST\n",
        "transform = transforms.ToTensor()\n",
        "train_data = datasets.MNIST(\n",
        "    root=\"data\", train=True, transform=transform, download=True\n",
        ")\n",
        "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
        "\n",
        "# 2. Define a simple fully connected autoencoder\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Encoder: 784 -> 64 -> 16 (bottleneck)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28 * 28, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 16),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        # Decoder: 16 -> 64 -> 784\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(16, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 28 * 28),\n",
        "            nn.Sigmoid(),  # output pixels in [0, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        out = self.decoder(z)\n",
        "        return out.view(-1, 1, 28, 28)\n",
        "\n",
        "model = Autoencoder().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 3. Train the autoencoder to reconstruct its input\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for imgs, _ in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, imgs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch + 1}: loss {epoch_loss:.4f}\")\n",
        "\n",
        "# 4. Visualize original and reconstructed digits\n",
        "model.eval()\n",
        "imgs, _ = next(iter(train_loader))\n",
        "imgs = imgs[:8].to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    decoded = model(imgs).cpu()\n",
        "\n",
        "imgs = imgs.cpu()\n",
        "\n",
        "plt.figure(figsize=(12, 3))\n",
        "for i in range(8):\n",
        "    # Original\n",
        "    ax = plt.subplot(2, 8, i + 1)\n",
        "    plt.imshow(imgs[i].squeeze(), cmap=\"gray\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # Reconstructed\n",
        "    ax = plt.subplot(2, 8, i + 9)\n",
        "    plt.imshow(decoded[i].squeeze(), cmap=\"gray\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Original digits (top) vs reconstructed digits (bottom)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ksArdCGP3TGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listing 5-2 Using reconstruction error from an autoencoder to detect anomalies\n",
        "This program reuses the MNIST setup and teaches the network what “normal” looks like by training it only on digits 0 through 7. Later, we evaluate it on the full test set, including digits 8 and 9, and see how the reconstruction error changes."
      ],
      "metadata": {
        "id": "CgStv97fCc5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using reconstruction error from an autoencoder to detect anomalies\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 0. Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# 1. Define a simple fully connected autoencoder\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Encoder: 784 -> 64 -> 16 (bottleneck)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28 * 28, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 16),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        # Decoder: 16 -> 64 -> 784\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(16, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 28 * 28),\n",
        "            nn.Sigmoid(),  # output pixels in [0, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        out = self.decoder(z)\n",
        "        return out.view(-1, 1, 28, 28)\n",
        "\n",
        "# 2. Load MNIST train and test sets\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True,\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    transform=transform,\n",
        "    download=True,\n",
        ")\n",
        "\n",
        "# 3. Keep only digits 0–7 for training (normal data)\n",
        "\n",
        "train_mask = train_data.targets < 8\n",
        "x_train_normal = train_data.data[train_mask].float() / 255.0   # [N, 28, 28]\n",
        "x_train_normal = x_train_normal.unsqueeze(1)                    # [N, 1, 28, 28]\n",
        "\n",
        "normal_loader = DataLoader(x_train_normal, batch_size=256, shuffle=True)\n",
        "\n",
        "# 4. Create and train a new autoencoder on normal digits only\n",
        "\n",
        "model = Autoencoder().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for imgs in normal_loader:\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        recon = model(imgs)\n",
        "        loss = criterion(recon, imgs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(normal_loader.dataset)\n",
        "    print(f\"Epoch {epoch + 1}: loss {epoch_loss:.4f}\")\n",
        "\n",
        "# 5. Score the entire test set by reconstruction error\n",
        "\n",
        "x_test = test_data.data.float() / 255.0   # [N, 28, 28]\n",
        "x_test = x_test.unsqueeze(1)              # [N, 1, 28, 28]\n",
        "y_test = test_data.targets                # digit labels\n",
        "\n",
        "test_ds = TensorDataset(x_test, y_test)\n",
        "test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "all_errors = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        recon = model(imgs)\n",
        "\n",
        "        # Mean squared error per image, averaged over all pixels\n",
        "        err = ((imgs - recon) ** 2).mean(dim=[1, 2, 3]).cpu()\n",
        "\n",
        "        all_errors.append(err)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "errors = torch.cat(all_errors).numpy()\n",
        "labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "# 6. Compare reconstruction error for normal digits (0–7) and unusual digits (8–9)\n",
        "\n",
        "normal_mask = labels < 8\n",
        "anom_mask = labels >= 8\n",
        "\n",
        "# Threshold at the ninety fifth percentile of all errors\n",
        "threshold = np.percentile(errors, 95)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.hist(\n",
        "    errors[normal_mask],\n",
        "    bins=40,\n",
        "    histtype=\"step\",\n",
        "    label=\"Digits 0–7\",\n",
        ")\n",
        "plt.hist(\n",
        "    errors[anom_mask],\n",
        "    bins=40,\n",
        "    histtype=\"step\",\n",
        "    linestyle=\"--\",\n",
        "    label=\"Digits 8–9\",\n",
        ")\n",
        "plt.axvline(threshold, color=\"black\", linestyle=\":\", label=\"Threshold\")\n",
        "\n",
        "plt.xlabel(\"Reconstruction error\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Autoencoder anomaly detection on MNIST\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DLd4c6xJESb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing 5-3 Using a pretrained YOLOv5 model to detect objects in an image\n",
        "This example lets you upload an image and count the objects in it using a pre-trained YOLOv5 object-detection model."
      ],
      "metadata": {
        "id": "G-k9fmTh-1B4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8mfKr5VMgpX"
      },
      "outputs": [],
      "source": [
        "# Set up YOLOv5 and install dependencies\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -r requirements.txt\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import cv2\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Upload an image\n",
        "uploaded = files.upload()\n",
        "image_path = next(iter(uploaded))  # take the first uploaded file\n",
        "\n",
        "# Load pretrained model\n",
        "model = torch.hub.load('.', 'yolov5m', source='local')\n",
        "\n",
        "# Run inference\n",
        "results = model(image_path)\n",
        "\n",
        "# Count detected classes\n",
        "detections = results.pandas().xyxy[0]\n",
        "class_names = detections['name'].tolist()\n",
        "counts = Counter(class_names)\n",
        "\n",
        "print(\"\\nDetected Objects:\")\n",
        "for label, count in counts.items():\n",
        "    print(f\"- {label}: {count}\")\n",
        "\n",
        "# Display image with bounding boxes\n",
        "results.render()\n",
        "img = Image.fromarray(results.ims[0])\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.title(\"Detected Objects\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing 5-4 YOLOv5 video object detection implemented as repeated image detection using YOLOv5\n",
        "This code runs YOLOv5 on an uploaded video inside a Colab notebook and prints a compact per-second table. With a few additional steps, the same pattern can support traffic monitoring, wildlife observation, or automated inspection on a conveyor belt."
      ],
      "metadata": {
        "id": "suZ2k7S9XDnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Suppress nonessential warnings (including AMP FutureWarnings) ---\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# --- Setup ---\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -r requirements.txt\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "from collections import Counter, defaultdict\n",
        "from google.colab import files\n",
        "import math\n",
        "\n",
        "# --- Load pretrained model ---\n",
        "model = torch.hub.load('.', 'yolov5m', source='local')\n",
        "model.conf = 0.25  # confidence threshold (optional)\n",
        "\n",
        "# --- Upload video ---\n",
        "print(\"Please upload a video file (e.g., .mp4)\")\n",
        "uploaded = files.upload()\n",
        "video_source = next(iter(uploaded.keys()))\n",
        "\n",
        "cap = cv2.VideoCapture(video_source)\n",
        "if not cap.isOpened():\n",
        "    raise RuntimeError(f\"Could not open video source: {video_source}\")\n",
        "\n",
        "# --- Determine FPS (fallback if missing) ---\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "if not fps or math.isnan(fps) or fps < 1:\n",
        "    fps = 30.0\n",
        "\n",
        "# --- Limit runtime for notebook safety ---\n",
        "MAX_SECONDS = 15\n",
        "max_frames = int(MAX_SECONDS * fps)\n",
        "\n",
        "# Store per-second MAX counts (non-accumulating)\n",
        "per_second_max = defaultdict(Counter)\n",
        "\n",
        "frame_idx = 0  # 0-based\n",
        "while True:\n",
        "    ret, frame_bgr = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if frame_idx >= max_frames:\n",
        "        break\n",
        "\n",
        "    second = int(frame_idx / fps)\n",
        "    frame_idx += 1\n",
        "\n",
        "    # OpenCV uses BGR; YOLO expects RGB\n",
        "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Run inference\n",
        "    results = model(frame_rgb)\n",
        "    detections = results.pandas().xyxy[0]\n",
        "    frame_counts = Counter(detections[\"name\"].tolist())\n",
        "\n",
        "    # Keep the maximum count seen in any frame during this second\n",
        "    for label, count in frame_counts.items():\n",
        "        if count > per_second_max[second][label]:\n",
        "            per_second_max[second][label] = count\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# --- Final per-second table ---\n",
        "print(\"\\nPer-second detection summary (max per-frame counts):\\n\")\n",
        "\n",
        "for second in sorted(per_second_max.keys()):\n",
        "    counts = per_second_max[second]\n",
        "    summary = \", \".join(f\"{k}={v}\" for k, v in counts.most_common()) if counts else \"(none)\"\n",
        "    print(f\"t={second:2d}s | {summary}\")\n"
      ],
      "metadata": {
        "id": "P8y1yYyrXbU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listing 5-5 Removing backgrounds with a segmentation CNN\n",
        "This example uses a pretrained DeepLabV3 network, which preserves the spatial structure of the image and assigns a class label to each pixel. The result is a detailed map that shows which parts of the scene belong to the subject and which parts are background. The code uses this map to remove the background of the image."
      ],
      "metadata": {
        "id": "LsrabvTwQpX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN example: Remove the background with DeepLabV3 segmentation\n",
        "# Upload one image (a child, a person, etc.) and the CNN will output a transparent PNG.\n",
        "\n",
        "import io\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "from IPython.display import display\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# 1. Load pretrained DeepLabV3 model and its transforms\n",
        "weights = models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT\n",
        "model = models.segmentation.deeplabv3_resnet50(weights=weights).to(device).eval()\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "# 2. Upload an image\n",
        "print(\"Please upload a photo\")\n",
        "uploaded = files.upload()\n",
        "filename = next(iter(uploaded.keys()))\n",
        "\n",
        "img = Image.open(io.BytesIO(uploaded[filename])).convert(\"RGB\")\n",
        "\n",
        "# 3. Run the image through the CNN\n",
        "input_tensor = preprocess(img).unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)[\"out\"][0]  # [num_classes, H, W]\n",
        "\n",
        "labels = output.argmax(0).cpu().numpy()\n",
        "\n",
        "# 4. Build a foreground mask: anything not background (class 0)\n",
        "foreground_mask = labels != 0\n",
        "\n",
        "# 5. Make the background transparent (RGBA + alpha channel)\n",
        "img_resized = img.resize((labels.shape[1], labels.shape[0])).convert(\"RGBA\")\n",
        "img_np = np.array(img_resized).copy()\n",
        "\n",
        "alpha = np.where(foreground_mask, 255, 0).astype(np.uint8)\n",
        "img_np[..., 3] = alpha\n",
        "\n",
        "# 6. Display the result\n",
        "result_pil = Image.fromarray(img_np)\n",
        "print(\"Showing result…\")\n",
        "display(result_pil)\n",
        "\n",
        "# Optional: save a PNG with transparency\n",
        "out_name = \"background_removed.png\"\n",
        "result_pil.save(out_name)\n",
        "print(\"Saved:\", out_name)\n"
      ],
      "metadata": {
        "id": "LJslSAjobZhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listing 5-6 A simple LSTM model that trains on historical NVDA prices and plots the network’s forecast against the actual market data\n",
        "This code trains on historical data and evaluates the LSTM on the last part of that same history. It is not forecasting future dates beyond the dataset. It is learning to map a window of recent prices to the next price inside the same historical period. This setup is useful for learning and for checking whether the network has captured basic trends.  "
      ],
      "metadata": {
        "id": "OHRstEzwTe4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install yfinance matplotlib scikit-learn torch --quiet\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_timeseries(ticker, start, end, seq_len=20):\n",
        "    data = yf.download(ticker, start=start, end=end)\n",
        "    prices = data[['Close']].values.astype('float32')\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled = scaler.fit_transform(prices)\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(len(scaled) - seq_len):\n",
        "        X.append(scaled[i:i+seq_len])\n",
        "        y.append(scaled[i+seq_len])\n",
        "    return torch.tensor(X), torch.tensor(y), scaler, data.index[seq_len:]\n",
        "\n",
        "class PriceLSTM(nn.Module):\n",
        "    def __init__(self, hidden_size=50):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(1, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.fc(out[:, -1, :])\n",
        "\n",
        "def train_model(ticker='NVDA', start='2024-01-01', end='2027-12-31', epochs=200):\n",
        "    X, y, scaler, dates = get_timeseries(ticker, start, end)\n",
        "    split = int(0.8 * len(X))\n",
        "    X_train, X_test = X[:split], X[split:]\n",
        "    y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "    model = PriceLSTM()\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(X_train), y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return model, X_test, y_test, scaler, dates[split:]\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, scaler, dates, ticker):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(X_test)\n",
        "    actual = scaler.inverse_transform(y_test)\n",
        "    predicted = scaler.inverse_transform(preds.numpy())\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(dates, actual, label=\"Actual\", color=\"black\")\n",
        "    plt.plot(dates, predicted, label=\"Predicted\", linestyle=\"--\", color=\"gray\")\n",
        "    plt.title(f\"{ticker} Price Forecast\")\n",
        "    plt.xlabel(\"Date\"); plt.ylabel(\"Price\")\n",
        "    plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
        "\n",
        "# Example run\n",
        "model, X_test, y_test, scaler, dates = train_model(\n",
        "    ticker=\"NVDA\",\n",
        "    start=\"2018-01-01\",\n",
        "    end=\"2023-12-31\",\n",
        "    epochs=50,\n",
        ")\n",
        "evaluate_model(model, X_test, y_test, scaler, dates, \"NVDA\")"
      ],
      "metadata": {
        "id": "_JkMkYcPTLN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listing 5-7 A simple character-level prediction model\n",
        "The following example shows a character-level RNN that learns to predict the next letter in a short phrase typed by the user. The model trains live in your notebook and begins to recognize patterns within seconds.\n"
      ],
      "metadata": {
        "id": "T_0HznrKaPJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# ---- 1. Model definition in PyTorch ----\n",
        "class SimpleCharRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=8, hidden_size=32):\n",
        "        super(SimpleCharRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.RNN(input_size=embed_dim, hidden_size=hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len)\n",
        "        embedded = self.embedding(x)              # (batch, seq_len, embed_dim)\n",
        "        output, _ = self.rnn(embedded)            # (batch, seq_len, hidden_size)\n",
        "        last_output = output[:, -1, :]            # (batch, hidden_size)\n",
        "        logits = self.fc(last_output)             # (batch, vocab_size)\n",
        "        return logits\n",
        "\n",
        "def build_and_predict_rnn(user_text):\n",
        "    # ---- 2. Preprocessing ----\n",
        "    chars = sorted(set(user_text))\n",
        "    c2i = {c: i for i, c in enumerate(chars)}\n",
        "    i2c = {i: c for c, i in c2i.items()}\n",
        "    vocab_size = len(chars)\n",
        "\n",
        "    # Convert characters to integer indices\n",
        "    seq = [c2i[c] for c in user_text]\n",
        "    # Inputs: all but last char\n",
        "    X = torch.tensor(seq[:-1], dtype=torch.long).unsqueeze(1)   # (batch, seq_len=1)\n",
        "    # Targets: next char index\n",
        "    y = torch.tensor(seq[1:], dtype=torch.long)                 # (batch,)\n",
        "\n",
        "    # ---- 3. Model setup ----\n",
        "    model = SimpleCharRNN(vocab_size=vocab_size, embed_dim=8, hidden_size=32)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    # ---- 4. Training ----\n",
        "    model.train()\n",
        "    epochs = 300\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X)            # (batch, vocab_size)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # ---- 5. Predictions ----\n",
        "    model.eval()\n",
        "    print(\"\\nCharacter Predictions:\")\n",
        "    with torch.no_grad():\n",
        "        for ch in user_text[:-1]:\n",
        "            idx = torch.tensor([[c2i[ch]]], dtype=torch.long)   # shape (1, 1)\n",
        "            logits = model(idx)\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            next_idx = torch.argmax(probs, dim=-1).item()\n",
        "            next_char = i2c[next_idx]\n",
        "            print(f\"{ch} → {next_char}\")\n",
        "\n",
        "# ---- 6. Interactive Widget Interface ----\n",
        "text_input = widgets.Text(value=\"Hello, RNN World!\", description='Text:')\n",
        "run_button = widgets.Button(description='Predict')\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_run_clicked(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        user_text = text_input.value.strip()\n",
        "        if len(user_text) < 3:\n",
        "            print(\"Please enter at least 3 characters.\")\n",
        "        else:\n",
        "            build_and_predict_rnn(user_text)\n",
        "\n",
        "run_button.on_click(on_run_clicked)\n",
        "display(widgets.VBox([text_input, run_button, output]))\n"
      ],
      "metadata": {
        "id": "mPLO4RYEjMNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listing 5-8 A simple interactive example of sentiment analysis\n",
        "This example uses Hugging Face Transformers to classify the sentiment of a sentence with DistilBERT, a lightweight version of BERT."
      ],
      "metadata": {
        "id": "InaJ-WQExp27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzing Sentiment with a Transformer (interactive)\n",
        "!pip install transformers --quiet\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a sentiment-analysis pipeline (DistilBERT by default)\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "print(\"Enter a message to analyze its sentiment.\")\n",
        "print(\"Type 'quit' to exit.\\n\")\n",
        "\n",
        "while True:\n",
        "    s = input(\"Your message: \").strip()\n",
        "    if s.lower() in {\"quit\", \"exit\"}:\n",
        "        break\n",
        "    if not s:\n",
        "        print(\"Please type a non-empty message.\\n\")\n",
        "        continue\n",
        "    result = classifier(s)[0]\n",
        "    label = result[\"label\"]\n",
        "    score = result[\"score\"] * 100\n",
        "    print(f\"→ {label} ({score:.1f}%)\\n\")\n"
      ],
      "metadata": {
        "id": "cdFudQLNxFJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listing 5-9 A small question-answering example\n",
        "This simple example lets you type context and then ask a question about it."
      ],
      "metadata": {
        "id": "JyXPzHTvzL9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa = pipeline(\"question-answering\")\n",
        "\n",
        "context = input(\"Enter context: \")\n",
        "question = input(\"Now enter your question: \")\n",
        "\n",
        "result = qa(question=question, context=context)\n",
        "print(f\"Answer: {result['answer']} (score: {result['score']:.2f})\")\n"
      ],
      "metadata": {
        "id": "QQ8cThwZzuJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listing 5-10 A compact implementation of the chat-with-my-data pattern\n",
        "The following code asks the user to upload a PDF file through the browser. Once the file is up-loaded, the script extracts the text from each page and stores it as a single context string. The user can then enter questions about the document. Each question, together with the document text, is passed to a pretrained question-answering model. The model uses attention to locate the most relevant span and returns an answer with a confidence score."
      ],
      "metadata": {
        "id": "84ZAJt3B0YQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install transformers\n",
        "!pip install PyMuPDF\n",
        "\n",
        "from transformers import pipeline\n",
        "import fitz  # PyMuPDF\n",
        "from IPython.display import display\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Ask user to upload a PDF file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Take the first uploaded file\n",
        "file_name = next(iter(uploaded))\n",
        "pdf_data = uploaded[file_name]\n",
        "\n",
        "# Extract text from the PDF\n",
        "def extract_text_from_pdf_bytes(pdf_bytes):\n",
        "    doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    doc.close()\n",
        "    return text\n",
        "\n",
        "context = extract_text_from_pdf_bytes(pdf_data)\n",
        "\n",
        "# Initialize the QA pipeline\n",
        "qa = pipeline(\"question-answering\")\n",
        "\n",
        "# Interactive Q&A loop\n",
        "print(\"\\nPDF loaded. Ask me anything about its contents. Type 'quit' to exit.\")\n",
        "\n",
        "while True:\n",
        "    question = input(\"\\nYour question: \").strip()\n",
        "    if question.lower() == \"quit\":\n",
        "        print(\"Exiting Q&A.\")\n",
        "        break\n",
        "    if len(question) == 0:\n",
        "        print(\"Please enter a valid question.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        result = qa(question=question, context=context)\n",
        "        print(f\"Answer: {result['answer']} (score: {result['score']:.2f})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not answer the question. Reason: {e}\")\n"
      ],
      "metadata": {
        "id": "sFXKzDmR06Sp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}