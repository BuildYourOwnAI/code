{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter 7** - Multimedia and Deepfake Defense\n",
        "\n",
        "This notebook demonstrates the integration of AI in multimedia, featuring object detection and video annotation using open-source libraries like `Librosa`, `OpenCV`, `SpeechT5` and `YOLOv5`. With these tools, you’ll learn how to analyze video frames, detect objects, and overlay annotations, creating enhanced, interactive visual content. The modular design makes it easy to follow, adapt, and extend for custom applications. Libraries such as pandas and matplotlib add support for structured data handling and visualization.\n",
        "**Note**: GPU acceleration is recommended for optimal performance for some of the code cells that train AI models."
      ],
      "metadata": {
        "id": "9I6-BLIrnw8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 7-1: Download Audio Samples from Github\n",
        "This code prepares the environment by downloading required audio samples from GitHub. It includes Jerry’s podcast samples and non-Jerry audio files, ensuring they are available locally for training and testing purposes.\n",
        "\n",
        "**Note 1:** The download process can take a few mins.\n",
        "\n",
        "**Note 2:** Using WAV files with a sampling rate of 16kHz and Signed 16-bit PCM encoding ensures compatibility with SpeechT5. Consistent format avoids processing errors, maintains audio quality, and allows the model to generate accurate spectrograms. Variations in format can disrupt training and degrade synthesized speech quality."
      ],
      "metadata": {
        "id": "69VraYOeocNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "# Base GitHub repository URL for audio files\n",
        "BASE_URL = \"https://opensourceai-book.github.io/code/media/\"\n",
        "\n",
        "# List of Jerry's podcast audio samples for training and testing (Label 1)\n",
        "Jerry_Audio_Files = [\n",
        "    \"L1-Sample01-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample02-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample03-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample04-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample05-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample06-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample07-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample08-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample09-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample10-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample11-Jerry.wav\",  # Reserved for test\n",
        "    \"L1-Sample12-Jerry.wav\",  # Reserved for test\n",
        "]\n",
        "\n",
        "# List of non-Jerry audio samples for training and testing (Label 0)\n",
        "Non_Jerry_Audio_Files = [\n",
        "    \"L0-Sample01-Adolfo.wav\",  # Non-Jerry speaker\n",
        "    \"L0-Sample02-Rama.wav\",    # Non-Jerry speaker\n",
        "    \"L0-Sample03-Alex.wav\",    # Non-Jerry speaker\n",
        "    \"L0-Sample04-SynthGeorge.wav\",  # Synthetic voice\n",
        "    \"L0-Sample05-SynthJerry.wav\",   # Synthetic Jerry voice\n",
        "    \"L0-Sample06-SynthJerry.wav\",   # Synthetic Jerry voice\n",
        "    \"L0-Sample07-Teresa.wav\",  # Non-Jerry speaker\n",
        "    \"L0-Sample08-Blaine.wav\",  # Non-Jerry speaker\n",
        "    \"L0-Sample09-Bill.wav\",    # Non-Jerry speaker\n",
        "    \"L0-Sample10-Brian.wav\",   # Non-Jerry speaker\n",
        "    \"L0-Sample11-Chris.wav\",   # Non-Jerry speaker (test)\n",
        "    \"L0-Sample12-George.wav\",  # Non-Jerry speaker (test)\n",
        "]\n",
        "\n",
        "# Download a file from BASE_URL and save it to the current directory\n",
        "# if it does not already exist.\n",
        "def download_file(filename):\n",
        "\n",
        "    filepath = os.path.join(\"./\", filename)  # Local path in root\n",
        "    url = BASE_URL + filename\n",
        "    if not os.path.exists(filepath):  # Check if file exists\n",
        "        print(f\"Downloading {filename} to {filepath}...\")\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with open(filepath, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            print(f\"Downloaded {filename} successfully!\")\n",
        "        else:\n",
        "            print(f\"Failed to download {filename}. \"\n",
        "                  f\"Status code: {response.status_code}\")\n",
        "    else:\n",
        "        print(f\"{filename} already exists at {filepath}.\")\n",
        "    return filepath  # Return the full path\n",
        "\n",
        "# Download files for Label 1 (Jerry's audio files)\n",
        "print(\"Processing Label 1 (Jerry's audio files)...\")\n",
        "for file in Jerry_Audio_Files:\n",
        "    download_file(file)\n",
        "\n",
        "# Download files for Label 0 (Non-Jerry audio files)\n",
        "print(\"\\nProcessing Label 0 (Non-Jerry audio files)...\")\n",
        "for file in Non_Jerry_Audio_Files:\n",
        "    download_file(file)\n",
        "\n",
        "print(\"\\nAll files are downloaded and ready!\")"
      ],
      "metadata": {
        "id": "nD-eSoAGRCbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 7-2: Audio Feature Extraction and Visualization\n",
        "\n",
        "The first code cell defines the `extract_audio_features` function, which computes key audio features like MFCC, spectral centroid, and zero-crossing rate. These features provide summary statistics for training. We refer to this as creating an audio **fingerprint**.\n",
        "\n",
        "The second cell demonstrates how to load an audio file, extract features, and visualize them with plots for MFCCs, spectral centroid, and zero-crossing rate."
      ],
      "metadata": {
        "id": "xLQPXSjd4RoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to extract audio features (summary statistics for training)\n",
        "def extract_audio_features(file_path):\n",
        "    # Load the audio file\n",
        "    y, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "    # Extract MFCCs (Mel Frequency Cepstral Coefficients)\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "\n",
        "    # Extract spectral centroid (frequency centroid of the spectrum)\n",
        "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "\n",
        "    # Extract spectral rolloff (frequency below which a set percentage of the total\n",
        "    # spectral energy is contained)\n",
        "    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "\n",
        "    # Extract spectral bandwidth (spread of the spectrum around the centroid)\n",
        "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "\n",
        "    # Extract spectral contrast (difference between peaks and valleys in a spectrum)\n",
        "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "\n",
        "    # Extract zero-crossing rate (rate of sign changes in the signal)\n",
        "    zcr = librosa.feature.zero_crossing_rate(y)\n",
        "\n",
        "    # Extract harmonics and noise and calculate HNR (Harmonics-to-Noise Ratio)\n",
        "    harmonics, noise = librosa.effects.harmonic(y), librosa.effects.percussive(y)\n",
        "    hnr = np.mean(harmonics) / (np.mean(noise) + 1e-6)  # Avoid division by zero\n",
        "\n",
        "    # Extract chroma (distribution of pitch classes)\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "\n",
        "    # Extract RMS (Root Mean Square energy)\n",
        "    rmse = librosa.feature.rms(y=y)\n",
        "\n",
        "    # Create feature dictionary with descriptive statistics\n",
        "    features = {\n",
        "        # Average MFCC values\n",
        "        \"mfcc_mean\": np.mean(mfccs),\n",
        "        # Standard deviation of MFCC values\n",
        "        \"mfcc_std\": np.std(mfccs),\n",
        "        # Average spectral centroid\n",
        "        \"spectral_centroid_mean\": np.mean(spectral_centroid),\n",
        "        # Average spectral rolloff\n",
        "        \"spectral_rolloff_mean\": np.mean(spectral_rolloff),\n",
        "        # Avg spectral bandwidth\n",
        "        \"spectral_bandwidth_mean\": np.mean(spectral_bandwidth),\n",
        "        # Avg spectral contrast\n",
        "        \"spectral_contrast_mean\": np.mean(spectral_contrast),\n",
        "        # Std dev of spectral contrast\n",
        "        \"spectral_contrast_std\": np.std(spectral_contrast),\n",
        "        # Average zero-crossing rate\n",
        "        \"zcr_mean\": np.mean(zcr),\n",
        "        # Harmonics-to-Noise Ratio\n",
        "        \"chroma_mean\": np.mean(chroma),\n",
        "        # Average chroma features\n",
        "        \"hnr\": hnr,\n",
        "        # Average RMS energy\n",
        "        \"rmse_mean\": np.mean(rmse),\n",
        "    }\n",
        "    return features"
      ],
      "metadata": {
        "id": "7VgmWTHqr7TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cell 2 - Plot Basic Audio Features"
      ],
      "metadata": {
        "id": "j_xsQIHEFzwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio sample to plot basic features\n",
        "file_path = \"L1-Sample01-Jerry.wav\"\n",
        "\n",
        "# Download sample, if not done already\n",
        "download_file(file_path)\n",
        "\n",
        "# Extract features\n",
        "print(\"Extracting audio features...\")\n",
        "features = extract_audio_features(file_path)\n",
        "print(\"Extracted Features:\", features)\n",
        "\n",
        "# Load audio for visualization\n",
        "y, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "# Plot MFCCs\n",
        "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(mfccs, x_axis=\"time\", sr=sr, cmap=\"coolwarm\")\n",
        "plt.colorbar()\n",
        "plt.title(\"MFCCs\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot Spectral Centroid\n",
        "spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(spectral_centroid[0], label=\"Spectral Centroid\")\n",
        "plt.legend()\n",
        "plt.title(\"Spectral Centroid\")\n",
        "plt.ylabel(\"Hz\")\n",
        "plt.xlabel(\"Frames\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot Zero-Crossing Rate\n",
        "zcr = librosa.feature.zero_crossing_rate(y)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(zcr[0], label=\"Zero-Crossing Rate\")\n",
        "plt.legend()\n",
        "plt.title(\"Zero-Crossing Rate\")\n",
        "plt.xlabel(\"Frames\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fn7FPD2b3Cns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 7-3: Train Jerry Audio Detection Model\n",
        "This program trains a logistic regression model to distinguish Real Jerry audio from other audio. It uses extracted audio features, standardizes them, splits into train-test sets, and evaluates accuracy."
      ],
      "metadata": {
        "id": "k59ge6YXsRxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Define training and test samples explicitly. Use all but last two as training\n",
        "train_Jerry_Audio_Files = Jerry_Audio_Files[:-2]\n",
        "train_Non_Jerry_Audio_Files = Non_Jerry_Audio_Files[:-2]\n",
        "\n",
        "test_Jerry_Audio_Files = Jerry_Audio_Files[-2:]\n",
        "test_Non_Jerry_Audio_Files = Non_Jerry_Audio_Files[-2:]\n",
        "\n",
        "# Combine for labels\n",
        "train_labels = [1] * len(train_Jerry_Audio_Files) + [0] * len(train_Non_Jerry_Audio_Files)\n",
        "test_labels = [1] * len(test_Jerry_Audio_Files) + [0] * len(test_Non_Jerry_Audio_Files)\n",
        "\n",
        "# Extract features for training and test files\n",
        "train_files = train_Jerry_Audio_Files + train_Non_Jerry_Audio_Files\n",
        "test_files = test_Jerry_Audio_Files + test_Non_Jerry_Audio_Files\n",
        "\n",
        "print(f\"Total training files: {len(train_files)}\")\n",
        "print(f\"Total test files: {len(test_files)}\")\n",
        "\n",
        "# Extract features\n",
        "train_features = np.array([\n",
        "    list(extract_audio_features(file).values()) for file in train_files\n",
        "])\n",
        "\n",
        "test_features = np.array([\n",
        "    list(extract_audio_features(file).values()) for file in test_files\n",
        "])\n",
        "\n",
        "print(f\"Extracted training features shape: {train_features.shape}\")\n",
        "print(f\"Extracted test features shape: {test_features.shape}\")\n",
        "\n",
        "# Standardize training and test features\n",
        "scaler = StandardScaler()\n",
        "train_features_normalized = scaler.fit_transform(train_features)\n",
        "test_features_normalized = scaler.transform(test_features)\n",
        "print(\"Feature normalization completed.\")\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(train_features_normalized, train_labels)\n",
        "print(\"Model training completed.\")  # Debug: Training step\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(test_features_normalized)\n",
        "\n",
        "# Evaluate model on test set\n",
        "print(\"Test Accuracy:\", accuracy_score(test_labels, y_pred) * 100, \"%\")\n",
        "print(\"Classification Report:\")\n",
        "print(\n",
        "    classification_report(\n",
        "        test_labels, y_pred, target_names=[\"Not Real Jerry\", \"Real Jerry\"],\n",
        "        zero_division=1\n",
        "    )\n",
        ")\n",
        "\n",
        "# Print specific predictions\n",
        "print(\"Test Set Predictions:\")\n",
        "for file, true_label, pred_label in zip(\n",
        "    test_files, test_labels, y_pred\n",
        "):\n",
        "    true_class = \"Real Jerry\" if true_label == 1 else \"Not Real Jerry\"\n",
        "    predicted_class = \"Real Jerry\" if pred_label == 1 else \"Not Real Jerry\"\n",
        "    print(f\"File: {file}, True: {true_class}, Predicted: {predicted_class}\")\n"
      ],
      "metadata": {
        "id": "8oHCaGbcO0GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 7-4: Transcribe Jerry's Real Audio to Text\n",
        "This program downloads Real Jerry audio files, transcribes them using OpenAI's Whisper model, and saves the results in a dictionary for further use in other programs."
      ],
      "metadata": {
        "id": "0c_S4i4AyzhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from transformers import pipeline, WhisperProcessor, WhisperForConditionalGeneration\n",
        "import librosa\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Function to transcribe audio files using Whisper model\n",
        "def transcribe_audio_files(file_list, output_csv=\"transcriptions.csv\"):\n",
        "    \"\"\"\n",
        "    Transcribe audio files using Whisper model and save the filename and\n",
        "    transcription to a CSV file.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Load Whisper model and processor\n",
        "    print(\"Loading Whisper model...\")\n",
        "    try:\n",
        "        processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
        "        model = WhisperForConditionalGeneration.from_pretrained(\n",
        "            \"openai/whisper-small\"\n",
        "        )\n",
        "        model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(\"Whisper model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Whisper model: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Process each file\n",
        "    for file in file_list:\n",
        "        print(f\"Processing {file}...\")\n",
        "\n",
        "        try:\n",
        "            # Load and preprocess audio\n",
        "            audio, sr = librosa.load(file, sr=16000)  # Ensure 16 kHz sampling rate\n",
        "            inputs = processor(\n",
        "                audio, sampling_rate=16000, return_tensors=\"pt\", language=\"en\"\n",
        "            ).input_features\n",
        "            inputs = inputs.to(model.device)\n",
        "\n",
        "            # Transcribe the audio\n",
        "            predicted_ids = model.generate(inputs)\n",
        "            transcription = processor.batch_decode(\n",
        "                predicted_ids, skip_special_tokens=True\n",
        "            )[0]\n",
        "\n",
        "            results.append({\"filename\": file, \"transcription\": transcription})\n",
        "            print(f\"Transcription for {file}: {transcription}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error transcribing {file}: {e}\")\n",
        "            results.append({\"filename\": file, \"transcription\": None})\n",
        "\n",
        "    # Save results to CSV\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Transcriptions saved to {output_csv}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Transcribe Real Jerry files\n",
        "real_jerry_transcriptions = transcribe_audio_files(Jerry_Audio_Files)\n",
        "\n",
        "# Print transcriptions\n",
        "if real_jerry_transcriptions:\n",
        "    print(\"\\n--- Transcriptions ---\")\n",
        "    for entry in real_jerry_transcriptions:\n",
        "        print(f\"{entry['filename']}: {entry['transcription']}\")\n",
        "else:\n",
        "    print(\"No transcriptions available due to an error.\")"
      ],
      "metadata": {
        "id": "0W0Hv2qxy3hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 7-5: Voice Cloning Listings\n",
        "\n",
        "This section\n",
        "It includes the following steps:\n",
        "\n",
        "1. **Step 1:** Installs the necessary libraries and checks for GPU availability.\n",
        "2. **Step 2:** Dataset Preparation and Embedding.\n",
        "3. **Step 3:** Fine-Tuning the SpeehT5 Model.\n",
        "4. **Step 4:** Testing Jerry's cloned voice. How does it sound?\n",
        "5. **Step 5:** Comparing Feature Differences: Real vs. Cloned"
      ],
      "metadata": {
        "id": "YmQ_QxFWp4bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 - Prerequisite Setup\n",
        "This section installs the necessary libraries and checks for GPU availability to prepare the environment for using SpeechT5 and HiFi-GAN for text-to-speech synthesis.\n",
        "\n",
        "It includes the following steps:\n",
        "\n",
        "1. **Install Libraries:** Installs `datasets`, `soundfile`, `speechbrain`, `transformers`, and `accelerate` using `pip`.\n",
        "2. **Check GPU:** Verifies the availability of a GPU using `nvidia-smi`.\n",
        "3. **Import Libraries:** Imports the required modules from `transformers` and `torch`.\n",
        "4. **Load Models:** Loads the SpeechT5 processor, model, and HiFi-GAN vocoder.\n",
        "5. **Device Setup:** Checks for GPU availability and moves the model and vocoder to the appropriate device (GPU or CPU)."
      ],
      "metadata": {
        "id": "_hA1YPRC5ZSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install datasets soundfile speechbrain\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install --upgrade accelerate\n",
        "\n",
        "# Check GPU availability\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "EAGc_68-p6Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech\n",
        "from transformers import SpeechT5HifiGan\n",
        "import torch\n",
        "\n",
        "# Load the SpeechT5 processor and model\n",
        "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "\n",
        "# Load the HiFi-GAN vocoder for converting spectrograms to audio\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "\n",
        "# Check if GPU is available and move the model to GPU if possible\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "vocoder = vocoder.to(device)\n",
        "\n",
        "print(\"Prerequisite setup is complete. SpeechT5 and HiFi-GAN are loaded.\")"
      ],
      "metadata": {
        "id": "CFQJ36ZjeZej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 - Dataset Preparation and Embedding\n",
        "This program processes audio files and their transcripts to create a dataset for voice cloning. It integrates audio features, transcriptions, and speaker embeddings into a Hugging Face Dataset, ready for training or testing voice cloning models. Long samples are filtered, and the dataset is split into train-test subsets."
      ],
      "metadata": {
        "id": "_EhfJKxGsJHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datasets import Dataset, Audio\n",
        "from speechbrain.pretrained import EncoderClassifier\n",
        "import torch\n",
        "\n",
        "# Step 2: Data Preparation\n",
        "print(\"Step 2: Starting data preparation...\")\n",
        "\n",
        "# Define directories\n",
        "root_dir = \"./\"  # Root directory for audio files and dataset\n",
        "audio_files_dir = root_dir  # Audio files are now in the root directory\n",
        "dataset_dir = os.path.join(root_dir, \"processed_dataset\")  # Directory to save processed dataset\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "print(f\"Directories set up: - Dataset: {dataset_dir}\")\n",
        "\n",
        "# Load transcriptions from CSV\n",
        "csv_path = \"transcriptions.csv\"\n",
        "if not os.path.exists(csv_path):\n",
        "    raise FileNotFoundError(f\"Transcriptions file not found: {csv_path}\")\n",
        "\n",
        "transcriptions_df = pd.read_csv(csv_path)\n",
        "if \"filename\" not in transcriptions_df.columns or \"transcription\" not in transcriptions_df.columns:\n",
        "    raise ValueError(\"CSV must contain 'filename' and 'transcription' columns.\")\n",
        "\n",
        "# Prepare data from the CSV\n",
        "file_paths = [os.path.join(audio_files_dir, file) for file in transcriptions_df[\"filename\"]]\n",
        "transcriptions = transcriptions_df[\"transcription\"].tolist()\n",
        "\n",
        "# Create Hugging Face Dataset\n",
        "print(\"Creating Hugging Face dataset...\")\n",
        "data = {\"file_path\": file_paths, \"text\": transcriptions}\n",
        "dataset = Dataset.from_dict(data)\n",
        "\n",
        "# Add audio information to the dataset\n",
        "print(\"Casting audio files for dataset processing...\")\n",
        "dataset = dataset.cast_column(\"file_path\", Audio(sampling_rate=16000))\n",
        "\n",
        "# Load the SpeechBrain speaker embedding model\n",
        "print(\"Loading speaker embedding model...\")\n",
        "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
        "speaker_model = EncoderClassifier.from_hparams(\n",
        "    source=spk_model_name,\n",
        "    run_opts={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
        "    savedir=os.path.join(\"/tmp\", spk_model_name)\n",
        ")\n",
        "\n",
        "# Function to process each dataset example\n",
        "def prepare_dataset(example):\n",
        "    audio = example[\"file_path\"]\n",
        "    # Extract features and tokenize text\n",
        "    example = processor(\n",
        "        text=example[\"text\"],\n",
        "        audio_target=audio[\"array\"],\n",
        "        sampling_rate=audio[\"sampling_rate\"],\n",
        "        return_attention_mask=False,\n",
        "    )\n",
        "    example[\"labels\"] = example[\"labels\"][0]\n",
        "\n",
        "    # Generate speaker embedding\n",
        "    example[\"speaker_embeddings\"] = speaker_model.encode_batch(\n",
        "        torch.tensor(audio[\"array\"]).unsqueeze(0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    ).squeeze().cpu().numpy()\n",
        "\n",
        "    return example\n",
        "\n",
        "# Process the dataset\n",
        "print(\"Processing dataset to tokenize and add speaker embeddings...\")\n",
        "dataset = dataset.map(prepare_dataset, remove_columns=[\"file_path\"])\n",
        "\n",
        "# List to store removed examples\n",
        "removed_examples = []\n",
        "\n",
        "# Adjusted filtering function\n",
        "def is_not_too_long(example):\n",
        "    input_length = len(example[\"input_ids\"])\n",
        "    if input_length >= 200:\n",
        "        removed_examples.append({\"text\": example[\"text\"], \"input_length\": input_length})\n",
        "    return input_length < 200\n",
        "\n",
        "# Filter dataset and log removed examples\n",
        "print(\"Filtering out long examples and logging removed samples...\")\n",
        "dataset = dataset.filter(is_not_too_long)\n",
        "\n",
        "# Log the removed examples\n",
        "if removed_examples:\n",
        "    print(f\"Removed {len(removed_examples)} examples for exceeding the token limit:\")\n",
        "    for i, example in enumerate(removed_examples):\n",
        "        print(f\"{i+1}. Length: {example['input_length']} - Text: {example['text'][:50]}...\")\n",
        "else:\n",
        "    print(\"No examples were removed for being too long.\")\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "print(\"Splitting dataset into train and test sets...\")\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "# Save processed dataset\n",
        "print(f\"Saving processed dataset to: {dataset_dir}\")\n",
        "dataset.save_to_disk(dataset_dir)\n",
        "\n",
        "print(\"Data preparation complete!\")\n"
      ],
      "metadata": {
        "id": "rTIWgAMPsLic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 - Fine-Tuning the Model\n",
        "This program fine-tunes a SpeechT5 model for text-to-speech conversion using a processed dataset. It includes a custom data collator for speaker embeddings, trains the model with Hugging Face's Seq2SeqTrainer, and saves the fine-tuned model and processor to the Hugging Face Hub."
      ],
      "metadata": {
        "id": "S2xy21k4t3QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Hugging Face token\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"Your_token_goes_here\"\n",
        "print(\"Hugging Face token set successfully.\")"
      ],
      "metadata": {
        "id": "bt7bgJ1nvUvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from datasets import DatasetDict\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Step 3: Fine-Tuning the Model\n",
        "print(\"Step 3: Starting fine-tuning process...\")\n",
        "\n",
        "# Load processed dataset as DatasetDict\n",
        "print(\"Loading processed dataset...\")\n",
        "dataset_dir = \"./processed_dataset\"\n",
        "dataset = DatasetDict.load_from_disk(dataset_dir)\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "@dataclass\n",
        "class TTSDataCollatorWithPadding:\n",
        "    \"\"\"\n",
        "    Custom collator class to handle padding and preparing batches\n",
        "    for SpeechT5 training.\n",
        "    \"\"\"\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(\n",
        "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
        "        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n",
        "        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n",
        "\n",
        "        # Collate inputs and labels into batches\n",
        "        batch = self.processor.pad(\n",
        "            input_ids=input_ids,\n",
        "            labels=label_features,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Replace padding with -100 for correct loss masking\n",
        "        batch[\"labels\"] = batch[\"labels\"].masked_fill(\n",
        "            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100\n",
        "        )\n",
        "\n",
        "        # Remove unused keys\n",
        "        del batch[\"decoder_attention_mask\"]\n",
        "\n",
        "        # Adjust target lengths for reduction factor\n",
        "        if model.config.reduction_factor > 1:\n",
        "            target_lengths = torch.tensor([\n",
        "                len(feature[\"input_values\"]) for feature in label_features\n",
        "            ])\n",
        "            target_lengths = target_lengths.new([\n",
        "                length - length % model.config.reduction_factor\n",
        "                for length in target_lengths\n",
        "            ])\n",
        "            max_length = max(target_lengths)\n",
        "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
        "\n",
        "        # Add speaker embeddings to the batch\n",
        "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
        "\n",
        "        return batch\n",
        "\n",
        "# Initialize the data collator\n",
        "data_collator = TTSDataCollatorWithPadding(processor=processor)\n",
        "\n",
        "# Configure training arguments\n",
        "print(\"Configuring training arguments...\")\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./speecht5_finetuned_model\",\n",
        "    push_to_hub=True,\n",
        "    hub_token=os.environ[\"HF_TOKEN\"],\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=1e-4,\n",
        "    warmup_steps=500,\n",
        "    max_steps=2000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    logging_steps=50,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    greater_is_better=False,\n",
        "    label_names=[\"labels\"],\n",
        ")\n",
        "\n",
        "print(\"Initializing the trainer...\")\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=processor.tokenizer,\n",
        ")\n",
        "\n",
        "# Initialize the trainer\n",
        "print(\"Initializing the trainer...\")\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=processor.tokenizer,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model and processor\n",
        "print(\"Saving fine-tuned model and processor...\")\n",
        "trainer.push_to_hub(\n",
        "    dataset_tags=\"custom_speech_dataset\",\n",
        "    model_name=\"SpeechT5 Fine-Tuned on Custom Dataset\",\n",
        "    dataset=\"Custom Speech Dataset\",\n",
        "    language=\"en\",\n",
        "    tasks=\"text-to-speech\",\n",
        ")\n",
        "processor.save_pretrained(\"./speecht5_finetuned_model\")\n",
        "print(\"Fine-tuning process complete!\")"
      ],
      "metadata": {
        "id": "I6OfFGgSt63n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Testing and Play Synthesized Speech\n",
        "This program tests the fine-tuned SpeechT5 model by generating a spectrogram and converting it to audio using a vocoder. It synthesizes speech from custom text input using the selected speaker embedding, visualizes the spectrogram, and saves the generated audio file for playback."
      ],
      "metadata": {
        "id": "o1QcwZybwCTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "from transformers import (\n",
        "    SpeechT5Processor,\n",
        "    SpeechT5ForTextToSpeech,\n",
        "    SpeechT5HifiGan\n",
        ")\n",
        "\n",
        "# Step 4: Testing the Fine-Tuned Model\n",
        "print(\"Step 4: Testing the fine-tuned model...\")\n",
        "\n",
        "# Load the fine-tuned model and processor\n",
        "print(\"Loading the fine-tuned model and vocoder...\")\n",
        "model_name = \"./speecht5_finetuned_model\"  # Path to fine-tuned model\n",
        "processor = SpeechT5Processor.from_pretrained(model_name)\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(model_name)\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "\n",
        "# Ensure model is on the correct device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "vocoder = vocoder.to(device)\n",
        "\n",
        "# Select a speaker embedding\n",
        "print(\"Using speaker embedding from the fine-tuned dataset...\")\n",
        "example = dataset[\"test\"][0]  # Select the first test example\n",
        "speaker_embeddings = torch.tensor(\n",
        "    example[\"speaker_embeddings\"]\n",
        ").unsqueeze(0).to(device)\n",
        "\n",
        "# Define the input text for synthesis\n",
        "text = (\"Hey ladies and gentlemen, thank you for tuning in to the \"\n",
        "        \"Wild Ducks podcast featuring your host, Jerry Cuomo.\")\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = processor(text=text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate the spectrogram\n",
        "print(\"Generating spectrogram...\")\n",
        "spectrogram = model.generate_speech(\n",
        "    inputs[\"input_ids\"].to(device), speaker_embeddings\n",
        ")\n",
        "\n",
        "# Visualize the spectrogram\n",
        "plt.figure()\n",
        "plt.imshow(\n",
        "    spectrogram.squeeze().cpu().numpy().T,\n",
        "    aspect=\"auto\", origin=\"lower\"\n",
        ")\n",
        "plt.title(\"Generated Spectrogram\")\n",
        "plt.show()\n",
        "\n",
        "# Convert the spectrogram to audio using the vocoder\n",
        "print(\"Converting spectrogram to audio...\")\n",
        "with torch.no_grad():\n",
        "    audio = vocoder(spectrogram)\n",
        "\n",
        "# Save and play the generated audio\n",
        "audio_output_path = \"Jerry-Cloned-Sample01.wav\"\n",
        "print(f\"Saving synthesized audio to: {audio_output_path}\")\n",
        "import soundfile as sf\n",
        "sf.write(\n",
        "    audio_output_path,\n",
        "    audio.squeeze().cpu().numpy(),\n",
        "    samplerate=16000\n",
        ")\n",
        "\n",
        "print(\"Playing synthesized audio...\")\n",
        "Audio(audio_output_path, rate=16000)"
      ],
      "metadata": {
        "id": "JXjhcegDwEcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5 - Comparing Real Audio versus Memorex (Cloned)\n",
        "\n",
        "This program extracts audio features from real and cloned samples, computes their differences, and visualizes them in a bar chart. The plot highlights subtle variations in the audio fingerprints, helping identify key features where synthetic audio deviates from real recordings.\n"
      ],
      "metadata": {
        "id": "3LGpRX3j-SGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to extract audio features remains the same as provided above\n",
        "\n",
        "# Paths to real and cloned audio samples\n",
        "real_audio_path = \"L1-Sample11-Jerry.wav\"\n",
        "cloned_audio_path = \"Jerry-Cloned-Sample01.wav\"\n",
        "\n",
        "# Extract features for real and cloned audio\n",
        "real_features = extract_audio_features(real_audio_path)\n",
        "cloned_features = extract_audio_features(cloned_audio_path)\n",
        "\n",
        "# Create a DataFrame for easy comparison\n",
        "feature_df = pd.DataFrame([real_features, cloned_features], index=[\"Real\", \"Cloned\"])\n",
        "\n",
        "# Normalize features for comparison (min-max scaling)\n",
        "normalized_feature_df = (feature_df - feature_df.min()) / (feature_df.max() - feature_df.min())\n",
        "\n",
        "# Plot feature comparison\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute feature-wise differences\n",
        "feature_differences = feature_df.loc[\"Real\"] - feature_df.loc[\"Cloned\"]\n",
        "\n",
        "# Plot the feature differences\n",
        "plt.figure(figsize=(12, 6))\n",
        "feature_differences.plot(kind=\"bar\", color=\"red\", edgecolor=\"black\")\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title(\"Feature-wise Differences: Real vs. Cloned Audio\")\n",
        "plt.ylabel(\"Difference in Feature Value\")\n",
        "plt.xlabel(\"Audio Features\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "\n",
        "# Add grid for better readability\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sg4JZURxJn9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video Analysis"
      ],
      "metadata": {
        "id": "UaLU7B7ZHkW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 7-6: Scene Detection with OpenCV\n",
        "Detects video scenes using `OpenCV`. Outputs the scene number, with start and start times, and scene duration.\n",
        "\n",
        "** Note:** This program attempts to download a sample video file from this books Github. If you get an error downloading, simply rerun the first code cell in this notebook that defines the `download_file` function."
      ],
      "metadata": {
        "id": "qH3iKkEZiuaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install opencv-python numpy torch torchvision scenedetect"
      ],
      "metadata": {
        "id": "SIzcX-YPcXSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from scenedetect import SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "from scenedetect.backends.opencv import VideoCaptureAdapter\n",
        "import cv2  # For OpenCV VideoCapture\n",
        "\n",
        "# Input video file path\n",
        "INPUT_VIDEO_FILE = \"Jerry-Video-Sample01.mp4\"\n",
        "download_file(INPUT_VIDEO_FILE)  # Ensure the file is downloaded\n",
        "\n",
        "# Check if the input video file exists\n",
        "if not os.path.exists(INPUT_VIDEO_FILE):\n",
        "    print(f\"Error: File '{INPUT_VIDEO_FILE}' not found.\")\n",
        "    exit()\n",
        "\n",
        "# Initialize OpenCV VideoCapture for reading the video\n",
        "video_capture = cv2.VideoCapture(INPUT_VIDEO_FILE)\n",
        "if not video_capture.isOpened():\n",
        "    print(f\"Error: Unable to open video file '{INPUT_VIDEO_FILE}'.\")\n",
        "    exit()\n",
        "\n",
        "# Create a VideoCaptureAdapter for SceneDetect compatibility\n",
        "video_adapter = VideoCaptureAdapter(video_capture)\n",
        "\n",
        "# Initialize SceneManager for scene detection\n",
        "scene_manager = SceneManager()\n",
        "\n",
        "# Add a ContentDetector to detect scene transitions\n",
        "# Lower threshold (e.g., 15.0) = more sensitive to changes\n",
        "scene_manager.add_detector(ContentDetector(threshold=15.0))\n",
        "\n",
        "# Perform scene detection on the video\n",
        "scene_manager.detect_scenes(video_adapter)\n",
        "\n",
        "# Retrieve the list of detected scenes with start and end times\n",
        "scene_list = scene_manager.get_scene_list()\n",
        "\n",
        "# Filter scenes to exclude those shorter than 3 seconds\n",
        "filtered_scene_list = [\n",
        "    (start, end)\n",
        "    for start, end in scene_list\n",
        "    if (end - start).get_seconds() >= 3  # Minimum scene length filter\n",
        "]\n",
        "\n",
        "# Output the number of detected scenes\n",
        "print(f\"Detected {len(filtered_scene_list)} scenes.\")\n",
        "\n",
        "# Print details of each filtered scene\n",
        "for i, (start_time, end_time) in enumerate(filtered_scene_list):\n",
        "    print(f\"Scene {i + 1}: Start - {start_time}, End - {end_time}\")\n",
        "\n",
        "# Release the video capture resource\n",
        "video_capture.release()"
      ],
      "metadata": {
        "id": "VRZbu9_n1J6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 7-7: Video Object Detection and Annotation\n",
        "\n",
        "The code demonstrates using `YOLOv5` for real-time object detection on video frames. It processes each frame, detects objects, annotates with bounding boxes and labels, and saves the output video.\n"
      ],
      "metadata": {
        "id": "tyTeuGKQcIRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from torchvision.transforms import functional as F\n",
        "import warnings\n",
        "\n",
        "# Suppress specific warnings, such as FutureWarning\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Load the pre-trained YOLOv5 model for object detection\n",
        "# 'yolov5s' is a small, pre-trained YOLOv5 model optimized for speed\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "\n",
        "# Define input video file and output video file paths\n",
        "INPUT_VIDEO_FILE = \"Jerry-Video-Sample01.mp4\"  # Input video to process\n",
        "OUTPUT_VIDEO_FILE = \"Jerry-Video-Sample02.mp4\"  # Annotated output video\n",
        "\n",
        "# Ensure the input video file is downloaded or exists\n",
        "download_file(INPUT_VIDEO_FILE)\n",
        "\n",
        "# Load the input video using OpenCV\n",
        "cap = cv2.VideoCapture(INPUT_VIDEO_FILE)\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Unable to open video file '{INPUT_VIDEO_FILE}'.\")\n",
        "    exit()\n",
        "\n",
        "# Get video properties: width, height, and frames per second (FPS)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Initialize the video writer to save the output video with annotations\n",
        "out = cv2.VideoWriter(OUTPUT_VIDEO_FILE,\n",
        "                      cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "                      fps, (width, height))\n",
        "\n",
        "# Process the video frame by frame\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()  # Read the next frame\n",
        "    if not ret:  # If no more frames are available, exit the loop\n",
        "        break\n",
        "\n",
        "    # Convert the frame to RGB format (required by YOLOv5)\n",
        "    results = model(frame)  # Perform object detection on the frame\n",
        "\n",
        "    # Get detection results as a pandas DataFrame\n",
        "    detected_objects = results.pandas().xyxy[0]\n",
        "\n",
        "    # Annotate the frame with bounding boxes and labels\n",
        "    for _, row in detected_objects.iterrows():\n",
        "        # Extract bounding box coordinates and object details\n",
        "        x1, y1 = int(row['xmin']), int(row['ymin'])\n",
        "        x2, y2 = int(row['xmax']), int(row['ymax'])\n",
        "        conf, cls = row['confidence'], row['name']\n",
        "        label = f\"{cls} {conf:.2f}\"  # Format label with class and confidence\n",
        "\n",
        "        # Draw bounding box on the frame\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "        # Add label above the bounding box\n",
        "        cv2.putText(frame, label, (x1, y1 - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.5, (255, 0, 0), 2)\n",
        "\n",
        "    # Write the annotated frame to the output video\n",
        "    out.write(frame)\n",
        "\n",
        "# Release video resources after processing\n",
        "cap.release()  # Release input video\n",
        "out.release()  # Save the output video\n",
        "cv2.destroyAllWindows()  # Close OpenCV windows"
      ],
      "metadata": {
        "id": "rt20JYdncNl5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}