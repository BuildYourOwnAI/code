{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 9 – AI at Scale\n",
        "\n",
        "This notebook builds on the BYOAI-LIAR dataset to explore how scale,\n",
        "performance, and reliability together create trust. It fine-tunes a **T5**\n",
        "model for factuality classification and measures how well it performs under\n",
        "different scaling conditions.\n",
        "\n",
        "## Notebook structure\n",
        "- **9-1 (A–D)** — Prepares data, tokenizes inputs, and creates\n",
        "train/validation/test splits for fine-tuning.  \n",
        "- **Listing 9-2** — Benchmarks inference time across input lengths on GPU and\n",
        "CPU to show how latency grows with scale.  \n",
        "- **Listing 9-3** — Tests batching efficiency, comparing throughput and\n",
        "per-sample latency for different batch sizes.  \n",
        "- **Listing 9-4** — Saves and logs a versioned model run with metrics,\n",
        "manifest, and performance probe.  \n",
        "- **Listing 9-5** — Uploads the model and metadata to Hugging Face Hub with a\n",
        "generated model card.  \n",
        "- **Listing 9-6** — Loads the published model locally and calls it remotely via\n",
        "`gradio_client`, demonstrating real-world inference.\n",
        "\n",
        "## Requirements\n",
        "Python ≥ 3.10  |  GPU recommended  \n",
        "`transformers`, `datasets`, `torch`, `pandas`, `matplotlib`, `huggingface_hub`, `gradio_client`\n",
        "\n",
        "Store your Hugging Face token in Colab:\n",
        "```python\n",
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n"
      ],
      "metadata": {
        "id": "dY22l7TzSr6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 9-1: Fine-Tuning T5 on the BYOAI_LIAR Dataset\n",
        "\n",
        "This multi-part listing walks through the full workflow for preparing, training, and evaluating a **T5-small** model on the **BYOAI_LIAR** factuality dataset using a text-to-text format.\n",
        ">T5-small is a compact text-to-text Transformer that treats every NLP task as sequence generation. Pre-trained on diverse language objectives, it converts inputs into structured textual outputs. In this project, it is fine-tuned as a classifier on the BYOAI_LIAR dataset, generating one of six factuality labels (e.g., true, false, half-true) from a statement and its context. A BERT-based discriminative model achieved similar accuracy, showing that prompt structure and training design can rival model choice in impact.\n",
        "\n",
        "It begins with data preparation (9-1A), where the labeled CSV is normalized into a canonical set of six truth labels and converted into compact T5-style input prompts. Next, a baseline inference step (9-1B) runs an un-fine-tuned T5 model to establish a random-performance reference point. The fine-tuning phase (9-1C) trains the model using grouped, leakage-safe splits and Adafactor optimization.   Finally, (9-1D) evaluates and compares performance across multiple training durations.\n",
        "\n",
        "> ***EXPECTED RESULTS:**\n",
        ">\n",
        ">Through these experiments, shorter runs (three epochs) slightly underfit at **40% accuracy**, while longer runs (eight epochs) plateaued near **42%**.  \n",
        "A **six-epoch model** achieved the best balance, reaching **43% exact accuracy** and **≈79% border-tolerant accuracy**, where most “errors” differed by only one neighboring truth label.  \n",
        ">\n",
        ">Compared with the **11%** baseline accuracy of the untrained T5 (roughly random among six labels), fine-tuning yielded a **nearly fourfold improvement**.  \n",
        "This demonstrates that even a compact T5 model can internalize graded truth distinctions when trained with structured prompts and modest compute.\n",
        "\n",
        "*Note:* Run all subcells in sequence on a GPU-enabled runtime for correct execution and reproducible results.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2cGuRHqHfPPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Listing 9-1A — BYOAI_LIAR Dataset Loader and Preparer (T5 Classification)\n",
        "\n",
        "This cell loads and prepares the **BYOAI_LIAR** dataset for fine-tuning a T5-style text-to-text classifier.  \n",
        "It reads the source CSV, standardizes all factuality labels into a canonical set, and constructs a compact `input_text` prompt for each record that includes the statement, context, tags, and chapter title.  \n",
        "\n",
        "The data is split into training, validation, and test sets using grouped sampling by `chunk_id` to prevent near-duplicate leakage across splits.  \n",
        "Label validity is enforced, and the resulting subsets are packaged into a Hugging Face `DatasetDict` for downstream tokenization and training.  \n",
        "\n",
        "Finally, the cell prints dataset statistics, label distributions, and top chapter counts per split, along with a few sample records—providing a quick quality check before model fine-tuning begins."
      ],
      "metadata": {
        "id": "1sCSO6oieuJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Listing 9-1A. BYOAI_LIAR dataset loader and preparer (T5 classification)\n",
        "# Pipeline:\n",
        "# - Load CSV and select expected columns\n",
        "# - Normalize labels into 'target_text' using a canonical set\n",
        "# - Build a compact 'input_text' template for T5\n",
        "# - Grouped train/val/test split by chunk_id to avoid leakage\n",
        "# - Package as Hugging Face DatasetDict and print split summaries\n",
        "# - Post-split label guard and safe sample printing\n",
        "# ============================================================================\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import Tuple\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "# --------------------------- Config -----------------------------------------\n",
        "CSV_PATH_OR_URL = \"https://raw.githubusercontent.com/buildyourownai/code/main/datasets/byoai_liar.csv\"\n",
        "TEST_SIZE  = 0.10   # test fraction\n",
        "VAL_SIZE   = 0.10   # validation fraction\n",
        "SEED       = 42\n",
        "\n",
        "# --------------------------- Canonical labels -------------------------------\n",
        "FACTUALITY_LABELS = [\n",
        "    \"true\", \"mostly-true\", \"half-true\", \"barely-true\", \"false\", \"pants-fire\",\n",
        "]\n",
        "\n",
        "# --------------------------- Input template ---------------------------------\n",
        "def build_input_text(statement: str, context: str, subject_tags: str, chapter_title: str) -> str:\n",
        "    stmt = str(statement or \"\").strip()\n",
        "    ctx  = str(context or \"\").strip()\n",
        "    tags = str(subject_tags or \"\").strip()\n",
        "    ch   = str(chapter_title or \"\").strip()\n",
        "    return (\n",
        "        \"classify:\\n\"\n",
        "        f\"statement: {stmt}\\n\"\n",
        "        f\"context: {ctx}\\n\"\n",
        "        f\"tags: {tags}\\n\"\n",
        "        f\"chapter: {ch}\\n\"\n",
        "        \"choices: true | mostly-true | half-true | barely-true | false | pants-fire\\n\"\n",
        "        \"answer:\"\n",
        "    )\n",
        "# --------------------------- Load and prune ---------------------------------\n",
        "keep_cols = [\n",
        "    \"id\", \"chunk_id\", \"label\", \"statement\",\n",
        "    \"context\", \"label_reason\", \"subject_tags\",\n",
        "    \"chapter\", \"chapter_title\",\n",
        "]\n",
        "df = pd.read_csv(CSV_PATH_OR_URL, encoding=\"utf-8-sig\")[keep_cols].copy()\n",
        "\n",
        "# Normalize labels and keep only canonical ones\n",
        "df[\"target_text\"] = df[\"label\"].astype(str).str.strip().str.lower()\n",
        "df = df[df[\"target_text\"].isin(FACTUALITY_LABELS)].reset_index(drop=True)\n",
        "\n",
        "# Build input_text\n",
        "df[\"input_text\"] = df.apply(\n",
        "    lambda r: build_input_text(r[\"statement\"], r[\"context\"], r[\"subject_tags\"], r[\"chapter_title\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# --------------------------- Grouped split ----------------------------------\n",
        "def grouped_train_val_test_split(\n",
        "    data: pd.DataFrame,\n",
        "    group_col: str = \"chunk_id\",\n",
        "    test_size: float = TEST_SIZE,\n",
        "    val_size: float = VAL_SIZE,\n",
        "    seed: int = SEED,\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Split by group to avoid near-duplicate leakage across splits.\"\"\"\n",
        "    g1 = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
        "    train_val_idx, test_idx = next(g1.split(data, groups=data[group_col]))\n",
        "    train_val_df, test_df = data.iloc[train_val_idx], data.iloc[test_idx]\n",
        "\n",
        "    val_fraction = val_size / (1.0 - test_size)\n",
        "    g2 = GroupShuffleSplit(n_splits=1, test_size=val_fraction, random_state=seed)\n",
        "    train_idx, val_idx = next(g2.split(train_val_df, groups=train_val_df[group_col]))\n",
        "    train_df, val_df = train_val_df.iloc[train_idx], train_val_df.iloc[val_idx]\n",
        "\n",
        "    return (\n",
        "        train_df.reset_index(drop=True),\n",
        "        val_df.reset_index(drop=True),\n",
        "        test_df.reset_index(drop=True),\n",
        "    )\n",
        "\n",
        "train_df, val_df, test_df = grouped_train_val_test_split(df, group_col=\"chunk_id\")\n",
        "\n",
        "# Sanity check for group leakage (counts should be zero)\n",
        "leak_tr_va = set(train_df[\"chunk_id\"]) & set(val_df[\"chunk_id\"])\n",
        "leak_tr_te = set(train_df[\"chunk_id\"]) & set(test_df[\"chunk_id\"])\n",
        "leak_va_te = set(val_df[\"chunk_id\"]) & set(test_df[\"chunk_id\"])\n",
        "print(f\"[groups] overlap train-val={len(leak_tr_va)} | train-test={len(leak_tr_te)} | val-test={len(leak_va_te)}\")\n",
        "\n",
        "# --------------------------- Post-split label guard -------------------------\n",
        "# Enforce canonical labels (defensive) and drop any stray rows\n",
        "# --------------------------- Input template ---------------------------------\n",
        "def build_input_text(statement: str, context: str, subject_tags: str, chapter_title: str) -> str:\n",
        "    stmt = str(statement or \"\").strip()\n",
        "    ctx  = str(context or \"\").strip()\n",
        "    tags = str(subject_tags or \"\").strip()\n",
        "    ch   = str(chapter_title or \"\").strip()\n",
        "    return (\n",
        "        \"classify:\\n\"\n",
        "        f\"statement: {stmt}\\n\"\n",
        "        f\"context: {ctx}\\n\"\n",
        "        f\"tags: {tags}\\n\"\n",
        "        f\"chapter: {ch}\\n\"\n",
        "        \"choices: true | mostly-true | half-true | barely-true | false | pants-fire\\n\"\n",
        "        \"answer:\"\n",
        "    )\n",
        "\n",
        "# --------------------------- Post-split label guard -------------------------\n",
        "def _enforce_labels(d: pd.DataFrame) -> pd.DataFrame:\n",
        "    ok = d[\"target_text\"].isin(FACTUALITY_LABELS)\n",
        "    if not ok.all():\n",
        "        print(f\"[labels] dropping {(~ok).sum()} rows with non-canonical labels\")\n",
        "        d = d[ok].copy()\n",
        "    return d\n",
        "\n",
        "train_df = _enforce_labels(train_df)\n",
        "val_df   = _enforce_labels(val_df)\n",
        "test_df  = _enforce_labels(test_df)\n",
        "\n",
        "# --------------------------- Package as HF datasets -------------------------\n",
        "cols = [\"input_text\", \"target_text\"]\n",
        "train_ds = Dataset.from_pandas(train_df[cols], preserve_index=False)\n",
        "val_ds   = Dataset.from_pandas(val_df[cols],   preserve_index=False)\n",
        "test_ds  = Dataset.from_pandas(test_df[cols],  preserve_index=False)\n",
        "\n",
        "byoai_dataset = DatasetDict({\"train\": train_ds, \"validation\": val_ds, \"test\": test_ds})\n",
        "\n",
        "# --------------------------- Split summaries --------------------------------\n",
        "print(\"=== BYOAI_LIAR Dataset Summary ===\")\n",
        "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
        "\n",
        "print(\"\\nTrain label distribution:\")\n",
        "print(Counter(train_df[\"target_text\"]))\n",
        "print(\"\\nVal label distribution:\")\n",
        "print(Counter(val_df[\"target_text\"]))\n",
        "print(\"\\nTest label distribution:\")\n",
        "print(Counter(test_df[\"target_text\"]))\n",
        "\n",
        "for name, d in {\"train\": train_df, \"val\": val_df, \"test\": test_df}.items():\n",
        "    print(f\"\\nTop chapters in {name}:\")\n",
        "    for k, v in d[\"chapter_title\"].value_counts().head(5).items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "# --------------------------- Sample peek ------------------------------------\n",
        "print(\"\\nSample input_text / target_text:\")\n",
        "for i in range(min(20, len(train_df))):\n",
        "    tgt = str(train_df.iloc[i][\"target_text\"]).strip().split()[0]  # guard against stray tokens\n",
        "    print(f\"\\nInput:\\n{train_df.iloc[i]['input_text']}\\nTarget: {tgt}\")"
      ],
      "metadata": {
        "id": "OILZkvviMbQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Listing 9-1B — Baseline Inference with an Unfine-Tuned T5-Small Model\n",
        "\n",
        "This cell evaluates an un-fine-tuned `t5-small` model on the **BYOAI_LIAR** dataset to establish a factuality baseline before training.  \n",
        "Using the same compact input format defined earlier (`classify:\\nstatement: ...`), the model scores each possible factuality label from the canonical set and selects the one with the lowest loss.  \n",
        "\n",
        "A few random training examples are printed to show the model’s raw predictions, and a small validation subset is then evaluated to compute an overall baseline accuracy.  \n",
        "Because the model has not been trained for this task, its predictions tend to cluster around simple answers such as “true” or “false.”  \n",
        "\n",
        "This step confirms that model loading, tokenization, and label scoring work properly, and provides a numerical reference—typically near random accuracy (≈0.17)—against which fine-tuning improvements can later be measured."
      ],
      "metadata": {
        "id": "dE1qY7aGIGIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Listing 9-1B. Baseline inference with an unfine-tuned T5-small model ===\n",
        "# Provides quick reference outputs on BYOAI inputs before fine-tuning.\n",
        "# Uses the same compact input template (\"classify:\\nstatement: ...\") as training.\n",
        "#\n",
        "# REQUIRES (from 9-1A in this session):\n",
        "# - train_df, val_df : DataFrames with ['input_text','target_text']\n",
        "# - FACTUALITY_LABELS : list of canonical factuality labels\n",
        "# - transformers : T5Tokenizer, T5ForConditionalGeneration\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "\n",
        "# --------------------------- Model & tokenizer -------------------------------\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)   # loads SentencePiece vocab for T5\n",
        "model     = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"=== Baseline (label scoring): Un-tuned T5-Small ===\")\n",
        "\n",
        "# --------------------------- Pre-tokenize labels -----------------------------\n",
        "with torch.no_grad():\n",
        "    _LABEL_TOKEN_IDS = [\n",
        "        tokenizer(text_target=lab, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "        for lab in FACTUALITY_LABELS\n",
        "    ]\n",
        "\n",
        "# --------------------------- Scoring-based predictor -------------------------\n",
        "def predict_label_by_scoring(text: str, max_input_len=128) -> str:\n",
        "    # Tokenize input prompt for T5\n",
        "    enc = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",      # return PyTorch tensors\n",
        "        truncation=True,          # truncate to fit within model context\n",
        "        max_length=max_input_len, # max input length for this quick check\n",
        "        padding=False             # single-example inference: no padding needed\n",
        "    )\n",
        "    enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "    # Evaluate sequence loss for each candidate label; pick the lowest\n",
        "    best_label, best_score = None, float(\"inf\")\n",
        "    with torch.no_grad():\n",
        "        for lab, tgt in zip(FACTUALITY_LABELS, _LABEL_TOKEN_IDS):\n",
        "            out = model(**enc, labels=tgt)\n",
        "            score = out.loss.item() * tgt.size(1)   # length-adjusted comparison\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                best_label = lab\n",
        "    return best_label\n",
        "\n",
        "# --------------------------- Preview a few examples --------------------------\n",
        "samples = train_df.sample(n=min(5, len(train_df)), random_state=42)\n",
        "\n",
        "for _, row in samples.iterrows():\n",
        "    text = str(row[\"input_text\"])\n",
        "    true = str(row[\"target_text\"])\n",
        "    pred = predict_label_by_scoring(text)\n",
        "\n",
        "    print(\"— input —\")\n",
        "    for ln in text.splitlines()[:4]:\n",
        "        print(ln)\n",
        "    print(f\"pred: {pred} | true: {true}\\n\")\n",
        "\n",
        "# --------------------------- Simple validation accuracy ----------------------\n",
        "N = min(300, len(val_df))  # small slice for a fast baseline number\n",
        "val_sample = val_df.sample(n=N, random_state=42)\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for _, row in val_sample.iterrows():\n",
        "    y_true.append(str(row[\"target_text\"]))\n",
        "    y_pred.append(predict_label_by_scoring(str(row[\"input_text\"])))\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "print(f\"=== Baseline validation accuracy (n={N}) ===\\naccuracy: {acc:.3f}\\n\")\n",
        "\n",
        "print(\"(Use this baseline accuracy as a reference before fine-tuning.)\")"
      ],
      "metadata": {
        "id": "Xj5stZQ8IWQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Listing 9-1C: Fine-Tuning T5 on the BYOAI_LIAR Dataset\n",
        "\n",
        "With the baseline established, this cell fine-tunes the `t5-small` model on the\n",
        "**BYOAI_LIAR** factuality dataset. The goal is to teach T5 to generate one of\n",
        "six truthfulness labels—from “pants-fire” to “true”—using its\n",
        "text-to-text learning framework.\n",
        "\n",
        "The dataset uses a uniform input format that combines the statement, context,\n",
        "subject tags, and chapter title, giving the model concise but meaningful cues\n",
        "about each example’s source and topic. Grouped splitting by `chunk_id` ensures\n",
        "that similar statements never cross between training and evaluation, helping\n",
        "the model generalize more reliably.\n",
        "\n",
        "Training typically runs for three to five epochs, which provides stable\n",
        "convergence on this dataset size without lengthy runtimes. After fine-tuning,\n",
        "the model can classify new statements derived from the book’s chapters,\n",
        "demonstrating how transformer fine-tuning can turn a structured, custom dataset\n",
        "into a working AI classifier for truth labeling."
      ],
      "metadata": {
        "id": "hmHVFV05gftj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phLnQsHSRpDV"
      },
      "outputs": [],
      "source": [
        "# === Listing 9-1C — Fine-tuning the BYOAI_LIAR Classifier (T5-small) =========\n",
        "# Fine-tunes a compact T5-small model on the BYOAI_LIAR dataset for factuality\n",
        "# classification. The model learns to predict truth labels given compact\n",
        "# statement-context inputs prepared in prior steps.\n",
        "#\n",
        "# REQUIRES:\n",
        "#   - byoai_dataset : DatasetDict with a \"train\" split\n",
        "#   - build_tokenize_fn() helper (for tokenizing inputs/labels)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "from transformers import (\n",
        "    T5Tokenizer,                # text ↔ token IDs conversion\n",
        "    T5ForConditionalGeneration, # T5 model for text-to-text tasks\n",
        "    Trainer,                    # manages training/eval loops\n",
        "    TrainingArguments,          # config class for training params\n",
        "    set_seed                    # ensures reproducible results\n",
        ")\n",
        "import torch\n",
        "\n",
        "# --------------------------- Tokenization -----------------------------------\n",
        "def build_tokenize_fn(\n",
        "    tokenizer,\n",
        "    max_input_length: int = 256,   # was 128; allow more context\n",
        "):\n",
        "    \"\"\"Tokenizes both input and target fields for T5.\"\"\"\n",
        "    def _tok(batch):\n",
        "        model_inputs = tokenizer(\n",
        "            batch[\"input_text\"],\n",
        "            padding=\"max_length\",       # pad to fixed length for batching\n",
        "            truncation=True,\n",
        "            max_length=max_input_length,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "        labels = tokenizer(\n",
        "            text_target=batch[\"target_text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=8,               # short labels (single-word targets)\n",
        "        )\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_inputs\n",
        "    return _tok\n",
        "\n",
        "# Reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Load base model and tokenizer\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize dataset (transform text fields → token IDs)\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenize = build_tokenize_fn(tokenizer, max_input_length=256)  # match above\n",
        "tokenized = byoai_dataset.map(tokenize, batched=True)\n",
        "\n",
        "train_data = tokenized[\"train\"]\n",
        "\n",
        "# --------------------------- Training setup ---------------------------------\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./results\",              # directory for checkpoints\n",
        "    per_device_train_batch_size=4,       # samples per device per step\n",
        "    gradient_accumulation_steps=4,       # effective batch size = 16\n",
        "    num_train_epochs=6,                  # number of full training passes\n",
        "    optim=\"adafactor\",                   # T5-friendly optimizer\n",
        "    learning_rate=5e-4,                  # base LR for Adafactor\n",
        "    warmup_ratio=0.1,                    # fraction of warmup steps\n",
        "    weight_decay=0.01,                   # L2 regularization\n",
        "    logging_dir=\"./logs\",                # logs folder\n",
        "    report_to=\"none\",                    # disable external logging\n",
        "    remove_unused_columns=False,         # retain all model inputs\n",
        "    fp16=torch.cuda.is_available(),      # mixed precision on supported GPUs\n",
        ")\n",
        "\n",
        "# --- Initialize Hugging Face Trainer ----------------------------------------\n",
        "trainer = Trainer(\n",
        "    model=model,                         # model to train\n",
        "    args=args,                           # training configuration\n",
        "    train_dataset=train_data             # dataset used for training\n",
        ")\n",
        "\n",
        "# --------------------------- Training run -----------------------------------\n",
        "print(\"\\n=== Starting fine-tuning ===\")\n",
        "trainer.train()\n",
        "\n",
        "# --------------------------- Post-training summary --------------------------\n",
        "if trainer.state.log_history:\n",
        "    losses = [\n",
        "        x.get(\"loss\", x.get(\"eval_loss\"))\n",
        "        for x in trainer.state.log_history\n",
        "        if \"loss\" in x or \"eval_loss\" in x\n",
        "    ]\n",
        "    if losses:\n",
        "        print(f\"\\nFinal training loss: {losses[-1]:.4f}\")\n",
        "\n",
        "param_count = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model parameters: {param_count:,}\")\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Model checkpoint saved to: {args.output_dir}\")\n",
        "print(\"\\nTraining complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Listing 9-1D: Evaluating the Fine-Tuned T5 Model\n",
        "\n",
        "After fine-tuning, this cell evaluates the `t5-small` model on examples from\n",
        "the BYOAI test split. The goal is to measure how well the model distinguishes\n",
        "truthfulness levels such as *true*, *half-true*, and *pants-fire*.\n",
        "\n",
        "The code prints a few qualitative examples—each showing the statement,\n",
        "predicted label, and ground truth—followed by a quantitative accuracy score\n",
        "over the entire test set. It also reports the most frequent confusion pairs,\n",
        "making it easier to see where adjacent truth levels overlap.\n",
        "\n",
        "Together, these checks provide a balanced view of how fine-tuning improved the\n",
        "model’s ability to recognize subtle differences in factuality across the BYOAI\n",
        "dataset."
      ],
      "metadata": {
        "id": "UP_bN0_cxVk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Evaluate fine-tuned T5 on the BYOAI test split ========================\n",
        "# REQUIRES:\n",
        "#   - model, tokenizer (fine-tuned)\n",
        "#   - test_df with columns: statement, context, subject_tags, chapter_title,\n",
        "#     target_text\n",
        "#   - build_input_text() from Listing 9-1A\n",
        "#\n",
        "# Notes:\n",
        "# - Lines wrapped to <=79 chars for print/publication clarity.\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def _predict_label_from_prompt(prompt: str) -> str:\n",
        "    \"\"\"Encode prompt, decode a short label, return a clean lowercase token.\"\"\"\n",
        "    enc = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "    enc = {k: v.to(device) for k, v in enc.items()}\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=6,      # short label like \"true\"\n",
        "            do_sample=False,       # deterministic decoding\n",
        "            num_beams=1\n",
        "        )\n",
        "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    pred = text.strip().lower()\n",
        "    return pred if pred in FACTUALITY_LABELS else \"(unknown)\"\n",
        "\n",
        "def predict_label(row) -> str:\n",
        "    \"\"\"\n",
        "    Build the same input template used for training and predict one label.\n",
        "    \"\"\"\n",
        "    prompt = build_input_text(\n",
        "        statement=str(row[\"statement\"]),\n",
        "        context=str(row.get(\"context\", \"\")),\n",
        "        subject_tags=str(row.get(\"subject_tags\", \"\")),\n",
        "        chapter_title=str(row.get(\"chapter_title\", \"\")),\n",
        "    )\n",
        "    return _predict_label_from_prompt(prompt)\n",
        "\n",
        "def eval_accuracy(df):\n",
        "    \"\"\"\n",
        "    Compute simple accuracy on the test split.\n",
        "    Returns arrays of predictions, gold labels, and overall accuracy.\n",
        "    \"\"\"\n",
        "    preds, golds = [], []\n",
        "    for _, row in df.iterrows():\n",
        "        gold = str(row[\"target_text\"]).strip().lower()\n",
        "        pred = predict_label(row)\n",
        "        preds.append(pred)\n",
        "        golds.append(gold)\n",
        "    preds = np.array(preds)\n",
        "    golds = np.array(golds)\n",
        "    overall = float((preds == golds).mean())\n",
        "    return preds, golds, overall\n",
        "\n",
        "def preview_examples(df, n=8):\n",
        "    \"\"\"\n",
        "    Show a few test examples with model predictions and gold labels.\n",
        "    \"\"\"\n",
        "    sample = df.sample(min(n, len(df)), random_state=42)\n",
        "    print(\"=== Qualitative predictions (BYOAI test split) ===\")\n",
        "    for _, r in sample.iterrows():\n",
        "        pred = predict_label(r)\n",
        "        print(f\"S: {r['statement']}\")\n",
        "        print(f\"C: {r.get('context', '')}\")\n",
        "        print(f\"pred: {pred} | gold: {r['target_text']}\\n\")\n",
        "\n",
        "# Show a few qualitative samples\n",
        "preview_examples(test_df, n=8)\n",
        "\n",
        "print(\"\\n=== Quantitative evaluation (BYOAI test split) ===\")\n",
        "preds, golds, acc = eval_accuracy(test_df)\n",
        "print(f\"Overall accuracy: {acc:.3f}\")\n",
        "\n",
        "# Summarize top confusion pairs for quick inspection\n",
        "pairs = list(zip(golds, preds))\n",
        "cm = Counter(pairs)\n",
        "print(\"\\nTop confusion pairs (gold → pred):\")\n",
        "for (g, p), c in cm.most_common(10):\n",
        "    print(f\"{g:>12} → {p:<12} : {c}\")\n",
        "\n",
        "# --- Border-tolerant evaluation (±1 neighboring label is counted correct) ---\n",
        "\n",
        "print(\"\\n=== Calculating Border-toleration of neighboring labels ===\")\n",
        "LABEL_ORDER = [\n",
        "    \"pants-fire\", \"false\", \"barely-true\",\n",
        "    \"half-true\", \"mostly-true\", \"true\"\n",
        "]\n",
        "LABEL_TO_IDX = {lbl: i for i, lbl in enumerate(LABEL_ORDER)}\n",
        "\n",
        "def neighbor_accuracy(golds, preds, tol=1):\n",
        "    \"\"\"Accuracy allowing predictions within ±tol steps on ordered scale.\"\"\"\n",
        "    ok = 0\n",
        "    total = 0\n",
        "    by_lbl = {lbl: {\"ok\": 0, \"n\": 0} for lbl in LABEL_ORDER}\n",
        "    for g, p in zip(golds, preds):\n",
        "        if g not in LABEL_TO_IDX or p not in LABEL_TO_IDX:\n",
        "            continue\n",
        "        gi, pi = LABEL_TO_IDX[g], LABEL_TO_IDX[p]\n",
        "        hit = abs(gi - pi) <= tol\n",
        "        ok += int(hit)\n",
        "        total += 1\n",
        "        by_lbl[g][\"ok\"] += int(hit)\n",
        "        by_lbl[g][\"n\"]  += 1\n",
        "    acc = ok / total if total else 0.0\n",
        "    per = {\n",
        "        lbl: (v[\"ok\"] / v[\"n\"] if v[\"n\"] else 0.0) for lbl, v in by_lbl.items()\n",
        "    }\n",
        "    return acc, per\n",
        "\n",
        "bacc, per = neighbor_accuracy(golds, preds, tol=1)\n",
        "print(f\"\\nBorder-tolerant accuracy (±1): {bacc:.3f}\")\n",
        "print(\"Per-label tolerant accuracy:\")\n",
        "for lbl in LABEL_ORDER:\n",
        "    if lbl in per:\n",
        "        print(f\"  {lbl:>12}: {per[lbl]:.3f}\")"
      ],
      "metadata": {
        "id": "5F39Cq_E7f6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 9‑2:  Measuring Inference Time Across Input Lengths\n",
        "This listing measures how inference time scales with input length using the fine-tuned T5 model. It defines helper functions to generate synthetic test inputs, perform a warm-up pass to stabilize GPU performance, and then benchmark inference across a range of token lengths on both CPU and GPU. The results are plotted to show how average response time increases as inputs grow larger, providing a clear view of where latency begins to climb—an important consideration when evaluating model performance at scale. The `warm_up_model()` helper can be reused in your own AI projects to ensure consistent, reliable benchmark results.\n"
      ],
      "metadata": {
        "id": "MdBj91-ypbFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Listing 9-2: Measuring Inference Time Across Input Lengths =============\n",
        "# REQUIRES:\n",
        "#   - Fine-tuned T5 model and tokenizer from Listing 9-1D (variables: model, tokenizer)\n",
        "#   - GPU runtime recommended for accurate comparison\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "import torch, time, matplotlib.pyplot as plt\n",
        "\n",
        "def warm_up_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "):\n",
        "    \"\"\"Run a short warm-up to stabilize performance before benchmarking.\"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    _ = model.generate(\n",
        "        **tokenizer(\"warm up\", return_tensors=\"pt\").to(device),\n",
        "        max_new_tokens=16\n",
        "    )\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def benchmark_inference_time(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    bins=None,\n",
        "    samples_per_bin=5,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "):\n",
        "    \"\"\"Benchmark average inference latency across increasing input sizes.\"\"\"\n",
        "    warm_up_model(model, tokenizer, device)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Default token length bins: 50–1049 in steps of 50\n",
        "    bins = bins or list(range(50, 1050, 50))\n",
        "    timing = {}\n",
        "\n",
        "    for b in bins:\n",
        "        label = f\"{b}-{b+49}\"\n",
        "        timing[label] = []\n",
        "        max_length = b + 16\n",
        "\n",
        "        for i in range(samples_per_bin + 2):  # +2 for warm-up\n",
        "            repeated = \"The sky is blue. \" * (b // 5)\n",
        "            prompt = f\"summarize: {repeated}\"\n",
        "\n",
        "            inputs = tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=max_length\n",
        "            )\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.synchronize()\n",
        "            start = time.time()\n",
        "            _ = model.generate(**inputs, max_new_tokens=16)\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "            if i >= 2:  # Skip warm-up passes\n",
        "                timing[label].append(time.time() - start)\n",
        "    return timing\n",
        "\n",
        "\n",
        "def plot_inference_times(timing_dicts, labels, title):\n",
        "    \"\"\"Plot average inference time per input length bin.\"\"\"\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    for timing, label in zip(timing_dicts, labels):\n",
        "        avg_times = [sum(timing[k]) / len(timing[k]) for k in timing]\n",
        "        keys = list(timing.keys())\n",
        "\n",
        "        print(f\"\\n{label} Inference Times:\")\n",
        "        for bin_label, tval in zip(keys, avg_times):\n",
        "            print(f\"  {bin_label}: {tval:.4f} sec\")\n",
        "\n",
        "        plt.plot(keys, avg_times, marker=\"o\", label=label)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Token Length (bins)\")\n",
        "    plt.ylabel(\"Average Inference Time (sec)\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# --- Run the benchmark on GPU and CPU --------------------------------------\n",
        "gpu_timing = benchmark_inference_time(model, tokenizer, device=\"cuda\")\n",
        "cpu_timing = benchmark_inference_time(model, tokenizer, device=\"cpu\")\n",
        "\n",
        "plot_inference_times(\n",
        "    [gpu_timing, cpu_timing],\n",
        "    [\"T5-small (GPU)\", \"T5-small (CPU)\"],\n",
        "    \"T5 Inference Time vs Input Length (GPU vs CPU)\"\n",
        ")"
      ],
      "metadata": {
        "id": "dQAGBPa6C71a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 9‑3: Measuring the Benefit of Batching in T5 Inference\n",
        "\n",
        "This experiment benchmarks how batching affects inference performance in our fine-tuned T5 model. The listing spans two cells: the first defines a helper function to generate synthetic inputs and time model responses across different batch sizes, while the second runs the benchmark and prints a summary table. For each batch size, the code measures average latency per sample, total throughput in samples per second, and relative speedup compared to batch size 1."
      ],
      "metadata": {
        "id": "IzDj9pDyLAZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Listing 9-3: Batching Impact — Latency and Throughput ==================\n",
        "# REQUIRES:\n",
        "#   - Fine-tuned T5 model and tokenizer from Listing 9-1D (model, tokenizer)\n",
        "#   - A GPU runtime is recommended for meaningful batch-size comparison\n",
        "#   - warm_up_model(model, tokenizer, device) defined earlier (Listing 9-2)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --------------------------- Helper functions -------------------------------\n",
        "\n",
        "def benchmark_inference(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    batch_size,\n",
        "    token_len=512,\n",
        "    max_tokens=512,\n",
        "    padding=\"max_length\",\n",
        "    repeat=5,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Measure T5 inference latency and throughput at a fixed batch size.\n",
        "    Uses synthetic inputs at roughly 'token_len' to standardize length.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    random.seed(42)\n",
        "\n",
        "    # Reuse the shared warm-up for stable first-timing behavior\n",
        "    warm_up_model(model, tokenizer, device=device)\n",
        "\n",
        "    # Build synthetic prompts of roughly 'token_len' tokens\n",
        "    phrases = [\n",
        "        \"The sky is blue\", \"Water is wet\", \"Cats chase mice\",\n",
        "        \"Birds fly south\", \"Ice is cold\", \"Fire is hot\",\n",
        "        \"Rain falls down\", \"Fish swim fast\", \"Clouds block sun\"\n",
        "    ]\n",
        "    inputs = []\n",
        "    for _ in range(batch_size):\n",
        "        sentence = \". \".join(random.choices(phrases, k=token_len // 10))\n",
        "        inputs.append(f\"summarize: {sentence}\")\n",
        "\n",
        "    # Avoid truncation by allowing a bit more than token_len\n",
        "    max_length = max(token_len + 16, max_tokens)\n",
        "\n",
        "    # Time repeated inference runs\n",
        "    elapsed_times = []\n",
        "    for _ in range(repeat):\n",
        "        enc = tokenizer(\n",
        "            inputs,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=padding,\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        )\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        t0 = time.time()\n",
        "        _ = model.generate(**enc, max_new_tokens=16)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        elapsed_times.append(time.time() - t0)\n",
        "\n",
        "    avg_batch_time = sum(elapsed_times) / repeat\n",
        "    avg_time_per_sample = avg_batch_time / batch_size\n",
        "    throughput = batch_size / avg_batch_time\n",
        "\n",
        "    return {\n",
        "        \"batch_size\": batch_size,\n",
        "        \"token_len\": token_len,\n",
        "        \"time_per_sample\": avg_time_per_sample,\n",
        "        \"batch_time\": avg_batch_time,\n",
        "        \"throughput\": throughput\n",
        "    }\n",
        "\n",
        "\n",
        "def plot_and_print_batch_results(\n",
        "    results,\n",
        "    title=\"Batching Impact: Latency vs. Throughput\"\n",
        "):\n",
        "    \"\"\"Render a dual-axis plot and print a compact results table.\"\"\"\n",
        "    batch_labels = [str(r[\"batch_size\"]) for r in results]\n",
        "    latencies = [r[\"time_per_sample\"] for r in results]\n",
        "    throughputs = [r[\"throughput\"] for r in results]\n",
        "\n",
        "    bar_color = \"#0074D9\"\n",
        "    line_color = \"#FF4136\"\n",
        "    grid_color = \"#AAAAAA\"\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(9, 5))\n",
        "\n",
        "    ax1.bar(\n",
        "        batch_labels, throughputs,\n",
        "        color=bar_color, edgecolor=\"black\", label=\"Throughput\"\n",
        "    )\n",
        "    ax1.set_xlabel(\"Batch Size\")\n",
        "    ax1.set_ylabel(\"Throughput (samples/sec)\", color=bar_color)\n",
        "    ax1.tick_params(axis=\"y\", labelcolor=bar_color)\n",
        "    ax1.set_ylim(0, max(throughputs) * 1.2)\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(\n",
        "        batch_labels, latencies,\n",
        "        color=line_color, marker=\"o\", linewidth=2, label=\"Latency\"\n",
        "    )\n",
        "    ax2.set_ylabel(\"Avg Time per Sample (sec)\", color=line_color)\n",
        "    ax2.tick_params(axis=\"y\", labelcolor=line_color)\n",
        "    ax2.set_ylim(0, max(latencies) * 1.2)\n",
        "\n",
        "    ax1.grid(True, axis=\"y\", linestyle=\"--\", color=grid_color)\n",
        "    plt.title(title)\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Compact, mono-spaced table for the console\n",
        "    print(f\"{'Batch':<8}{'Latency (s)':<15}{'Throughput (samples/s)':<25}\")\n",
        "    for r in results:\n",
        "        print(\n",
        "            f\"{r['batch_size']:<8}\"\n",
        "            f\"{r['time_per_sample']:<15.4f}\"\n",
        "            f\"{r['throughput']:<25.2f}\"\n",
        "        )\n",
        "\n",
        "# ------------------------------ Main logic ----------------------------------\n",
        "\n",
        "# Choose a range of batch sizes for the scaling sweep\n",
        "batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
        "\n",
        "# Run the benchmark across the selected batch sizes\n",
        "results = [benchmark_inference(model, tokenizer, bs) for bs in batch_sizes]\n",
        "\n",
        "# Visualize and print the summary of latency and throughput\n",
        "plot_and_print_batch_results(results)\n"
      ],
      "metadata": {
        "id": "otKfuylxICot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 9-4: Saving and Logging a Versioned Model Run\n",
        "\n",
        "This two-part listing shows how to persist and document a fine-tuned T5 model\n",
        "so it can be reused, compared, and benchmarked in later sessions.\n",
        "\n",
        "The **first cell** defines helper functions for checkpoint management,\n",
        "metadata capture, and performance measurement. These utilities save the model\n",
        "and tokenizer, record essential training details such as dataset size, epochs,\n",
        "and learning rate, and measure inference speed on a stable test prompt.\n",
        "\n",
        "The **second cell** applies those helpers to save the trained model into a\n",
        "structured checkpoint directory and run a short benchmark inference. It then\n",
        "records a structured JSONL log entry that includes the predicted label,\n",
        "latency, throughput, token counts, and device information.\n",
        "\n",
        "Together, the two cells make each model version traceable and comparable across\n",
        "experiments. Run **both cells in sequence**—first the helpers, then the control\n",
        "logic—to produce a fully documented, reproducible checkpoint."
      ],
      "metadata": {
        "id": "w3fACrxyWB9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Listing 9-4A: Helpers for saving & logging a versioned run =============\n",
        "# Contents:\n",
        "# - save_model_and_tokenizer(): write model + tokenizer to a checkpoint\n",
        "# - save_training_manifest(): record run/dataset metadata\n",
        "# - save_label_vocab_and_template(): persist label set and input template\n",
        "# - save_readme(): write a simple model README.md in the checkpoint folder\n",
        "# - time_prediction(): measure latency/throughput on one prompt\n",
        "# - write_log_entry(): append JSONL with ISO timestamp and rotation\n",
        "# - get_device_info(): compact device summary\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Optional, Dict, List\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def save_model_and_tokenizer(model, tokenizer, checkpoint_dir: str) -> None:\n",
        "    \"\"\"Save model + tokenizer; ensure tokenizer_config has a model_type.\"\"\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    model.save_pretrained(checkpoint_dir, safe_serialization=True)\n",
        "    tokenizer.save_pretrained(checkpoint_dir)\n",
        "\n",
        "    cfg_path = os.path.join(checkpoint_dir, \"tokenizer_config.json\")\n",
        "    if os.path.exists(cfg_path):\n",
        "        with open(cfg_path, \"r+\", encoding=\"utf-8\") as f:\n",
        "            cfg = json.load(f)\n",
        "            cfg[\"model_type\"] = cfg.get(\"model_type\", \"t5\")\n",
        "            f.seek(0)\n",
        "            json.dump(cfg, f, indent=2)\n",
        "            f.truncate()\n",
        "\n",
        "\n",
        "def save_training_manifest(\n",
        "    checkpoint_dir: str,\n",
        "    *,\n",
        "    model_name: str,\n",
        "    epochs: int,\n",
        "    seed: int,\n",
        "    train_size: int,\n",
        "    val_size: int,\n",
        "    test_size: int,\n",
        "    labels: List[str],\n",
        "    prompt_template: str,\n",
        "    learning_rate: float,\n",
        "    batch_size: int,\n",
        "    dataset_source: str,\n",
        "    git_rev: Optional[str] = None,\n",
        "    metrics: Optional[Dict] = None,\n",
        ") -> None:\n",
        "    \"\"\"Write a concise manifest of run configuration and dataset sizes.\"\"\"\n",
        "    manifest = {\n",
        "        \"timestamp\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
        "        \"model_name\": model_name,\n",
        "        \"epochs\": epochs,\n",
        "        \"seed\": seed,\n",
        "        \"train_size\": train_size,\n",
        "        \"val_size\": val_size,\n",
        "        \"test_size\": test_size,\n",
        "        \"labels\": list(labels),\n",
        "        \"prompt_template\": prompt_template.strip(),\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"dataset_source\": dataset_source,\n",
        "        \"git_rev\": git_rev,\n",
        "        \"metrics\": metrics or {},\n",
        "    }\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    with open(os.path.join(checkpoint_dir, \"training_manifest.json\"), \"w\",\n",
        "              encoding=\"utf-8\") as f:\n",
        "        json.dump(manifest, f, indent=2)\n",
        "\n",
        "\n",
        "def save_label_vocab_and_template(\n",
        "    checkpoint_dir: str,\n",
        "    labels: List[str],\n",
        "    input_template_str: str,\n",
        ") -> None:\n",
        "    \"\"\"Persist label vocabulary and the input template used in training.\"\"\"\n",
        "    with open(os.path.join(checkpoint_dir, \"labels.json\"), \"w\",\n",
        "              encoding=\"utf-8\") as f:\n",
        "        json.dump(sorted(labels), f, indent=2)\n",
        "    with open(os.path.join(checkpoint_dir, \"input_template.txt\"), \"w\",\n",
        "              encoding=\"utf-8\") as f:\n",
        "        f.write(input_template_str.strip() + \"\\n\")\n",
        "\n",
        "\n",
        "def save_readme(\n",
        "    checkpoint_dir: str,\n",
        "    title: str,\n",
        "    bullets: List[str],\n",
        ") -> None:\n",
        "    \"\"\"Create a simple README.md summarizing what this checkpoint contains.\"\"\"\n",
        "    with open(os.path.join(checkpoint_dir, \"README.md\"), \"w\",\n",
        "              encoding=\"utf-8\") as f:\n",
        "        f.write(f\"# {title}\\n\\n\")\n",
        "        for b in bullets:\n",
        "            f.write(f\"- {b}\\n\")\n",
        "\n",
        "\n",
        "def time_prediction(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    text: str,\n",
        "    *,\n",
        "    max_input_len: int = 256,\n",
        "    max_new_tokens: int = 8,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Measure latency and rough throughput on one prompt. Returns:\n",
        "    {prediction, latency_s, throughput_samples_per_s, tokens_in, tokens_out}\n",
        "    \"\"\"\n",
        "    # Warmup encode to avoid first-call overhead in timing\n",
        "    _ = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
        "                  max_length=max_input_len)\n",
        "\n",
        "    enc = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_input_len,\n",
        "    )\n",
        "    enc = {k: v.to(model.device) for k, v in enc.items()}\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    with torch.inference_mode():\n",
        "        out = model.generate(\n",
        "            **enc,\n",
        "            do_sample=False,       # deterministic\n",
        "            num_beams=1,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "    dt = time.time() - t0\n",
        "\n",
        "    pred = tokenizer.decode(out[0], skip_special_tokens=True).strip()\n",
        "    toks_in = int(enc[\"input_ids\"].numel())\n",
        "    toks_out = int(out[0].numel())\n",
        "    return {\n",
        "        \"prediction\": pred,\n",
        "        \"latency_s\": dt,\n",
        "        \"throughput_samples_per_s\": (1.0 / dt) if dt > 0 else float(\"inf\"),\n",
        "        \"tokens_in\": toks_in,\n",
        "        \"tokens_out\": toks_out,\n",
        "    }\n",
        "\n",
        "def write_log_entry(\n",
        "    log_entry: dict,\n",
        "    log_path: str,\n",
        "    rotate_at_mb: int = 10,\n",
        ") -> None:\n",
        "    \"\"\"Append a JSONL row (adds ISO timestamp) and rotate if file grows too big.\"\"\"\n",
        "    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
        "    entry = {\"ts\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
        "             **log_entry}\n",
        "\n",
        "    if os.path.exists(log_path) and (\n",
        "        os.path.getsize(log_path) > rotate_at_mb * 1024 * 1024\n",
        "    ):\n",
        "        base, ext = os.path.splitext(log_path)\n",
        "        stamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        os.rename(log_path, f\"{base}.{stamp}{ext or '.log'}\")\n",
        "\n",
        "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "\n",
        "def get_device_info() -> dict:\n",
        "    \"\"\"Return a compact summary of the active compute device.\"\"\"\n",
        "    info = {\"device\": str(torch.device(\"cuda\" if torch.cuda.is_available()\n",
        "                                       else \"cpu\"))}\n",
        "    if torch.cuda.is_available():\n",
        "        info.update({\n",
        "            \"cuda_name\": torch.cuda.get_device_name(0),\n",
        "            \"cuda_capability\": \".\".join(map(str, torch.cuda.get_device_capability(0))),\n",
        "            \"total_mem_gb\": round(torch.cuda.get_device_properties(0).total_memory\n",
        "                                  / (1024**3), 2),\n",
        "        })\n",
        "    return info"
      ],
      "metadata": {
        "id": "WHMob_AYWU6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Listing 9-4B: Control logic to save & log a versioned run =============\n",
        "# REQUIRES:\n",
        "#   - model, tokenizer (already fine-tuned)\n",
        "#   - train_df/val_df/test_df (or sizes known)\n",
        "#   - build_input_text() or an equivalent template string (optional for log)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# -------------------- Configuration ----------------------------------------\n",
        "MODEL_NAME      = \"byoai-t5-liar-classifier\"\n",
        "CHECKPOINT_DIR  = f\"./models/{MODEL_NAME}\"\n",
        "LOG_PATH        = f\"{CHECKPOINT_DIR}/model_log.jsonl\"\n",
        "\n",
        "# If you have these from earlier cells, use them; otherwise set integers.\n",
        "train_size = len(train_df) if \"train_df\" in globals() else 0\n",
        "val_size   = len(val_df)   if \"val_df\"   in globals() else 0\n",
        "test_size  = len(test_df)  if \"test_df\"  in globals() else 0\n",
        "\n",
        "# Minimal label vocab and input template for the manifest\n",
        "LABELS = [\"pants-fire\", \"false\", \"barely-true\", \"half-true\", \"mostly-true\", \"true\"]\n",
        "INPUT_TEMPLATE = (\n",
        "    \"classify:\\n\"\n",
        "    \"statement: {statement}\\n\"\n",
        "    \"context: {context}\\n\"\n",
        "    \"tags: {tags}\\n\"\n",
        "    \"chapter: {chapter}\"\n",
        ")\n",
        "\n",
        "# These may come from your training cell; set to your actual choices\n",
        "EPOCHS       = 5\n",
        "SEED         = 42\n",
        "LR           = 3e-4\n",
        "BATCH_SIZE   = 4\n",
        "DATASET_SRC  = \"byoai_liar.csv\"\n",
        "GIT_REV      = None   # e.g., captured via `git rev-parse --short HEAD`\n",
        "\n",
        "# -------------------- Save checkpoint --------------------------------------\n",
        "save_model_and_tokenizer(model, tokenizer, CHECKPOINT_DIR)\n",
        "\n",
        "# Write a minimal README; safe to overwrite\n",
        "save_readme(\n",
        "    CHECKPOINT_DIR,\n",
        "    title=f\"{MODEL_NAME}\",\n",
        "    bullets=[\n",
        "        \"Fine-tuned T5-small for factuality classification (BYOAI_LIAR).\",\n",
        "        \"Includes tokenizer, labels.json, input template, and manifest.\",\n",
        "        \"Weights saved in safetensors format.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Persist training manifest + label vocab + template\n",
        "save_training_manifest(\n",
        "    CHECKPOINT_DIR,\n",
        "    model_name=MODEL_NAME,\n",
        "    epochs=EPOCHS,\n",
        "    seed=SEED,\n",
        "    train_size=train_size,\n",
        "    val_size=val_size,\n",
        "    test_size=test_size,\n",
        "    labels=LABELS,\n",
        "    prompt_template=INPUT_TEMPLATE,\n",
        "    learning_rate=LR,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    dataset_source=DATASET_SRC,\n",
        "    git_rev=GIT_REV,\n",
        "    metrics={},  # add your final eval metrics here if available\n",
        ")\n",
        "save_label_vocab_and_template(CHECKPOINT_DIR, LABELS, INPUT_TEMPLATE)\n",
        "\n",
        "# -------------------- Quick performance probe ------------------------------\n",
        "# A short, synthetic prompt in the same template style used for training\n",
        "synthetic_prompt = (\n",
        "    \"classify:\\n\"\n",
        "    \"statement: Open-source frameworks like PyTorch are widely used in AI.\\n\"\n",
        "    \"context: framework usage in AI projects\\n\"\n",
        "    \"tags: open-source, deep-learning, frameworks\\n\"\n",
        "    \"chapter: Deep Learning\"\n",
        ")\n",
        "\n",
        "perf = time_prediction(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    text=synthetic_prompt,\n",
        "    max_input_len=256,\n",
        "    max_new_tokens=8,\n",
        ")\n",
        "\n",
        "# -------------------- Structured JSONL log ---------------------------------\n",
        "log_entry = {\n",
        "    \"model_instance\": MODEL_NAME,\n",
        "    \"checkpoint_dir\": CHECKPOINT_DIR,\n",
        "    \"run_timestamp\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
        "    \"dataset_source\": DATASET_SRC,\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"learning_rate\": LR,\n",
        "    \"train/val/test\": [train_size, val_size, test_size],\n",
        "    \"device\": get_device_info(),\n",
        "    \"probe_prompt_tokens_in\": perf[\"tokens_in\"],\n",
        "    \"probe_pred_tokens_out\": perf[\"tokens_out\"],\n",
        "    \"probe_latency_s\": round(perf[\"latency_s\"], 4),\n",
        "    \"probe_throughput_sps\": round(perf[\"throughput_samples_per_s\"], 2),\n",
        "    \"probe_prediction\": perf[\"prediction\"],\n",
        "    \"notes\": \"Versioned save + single-prompt timing probe for reproducibility.\",\n",
        "}\n",
        "\n",
        "write_log_entry(log_entry, LOG_PATH)\n",
        "\n",
        "print(f\"Model saved to: {CHECKPOINT_DIR}\")\n",
        "print(f\"Log appended to: {LOG_PATH}\")\n",
        "print(\n",
        "    f\"Probe → latency: {perf['latency_s']:.4f}s | \"\n",
        "    f\"throughput: {perf['throughput_samples_per_s']:.2f} samp/s | \"\n",
        "    f\"pred: {perf['prediction']}\"\n",
        ")"
      ],
      "metadata": {
        "id": "duRNqttAhzx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 9-5: Uploading a Trained Model and Metadata to Hugging Face\n",
        "\n",
        "This listing publishes the fine-tuned T5 model, tokenizer, and related metadata\n",
        "to the Hugging Face Hub. It creates or updates the repository, writes a\n",
        "Hub-compliant model card with YAML metadata, and uploads all files from the\n",
        "local checkpoint directory.\n",
        "\n",
        "The model card summarizes training details such as dataset size, epochs,\n",
        "and label set, along with usage notes and limitations. This ensures that the\n",
        "model is documented and discoverable when viewed on the Hub.\n",
        "\n",
        "Running this cell makes the model publicly available for download, testing,\n",
        "and reuse through the Hugging Face interface or API. Before running, confirm\n",
        "that you have authenticated with your Hugging Face account using  \n",
        "`huggingface-cli login` and that your local checkpoint folder is complete.\n",
        "\n",
        "Together with Listing 9-4, this step moves the model from a private runtime to\n",
        "a versioned, shareable, and reproducible artifact."
      ],
      "metadata": {
        "id": "RHfkV71_QD8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub"
      ],
      "metadata": {
        "id": "7a8XNRVAkZv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Listing 9-5: Upload model & model card to Hugging Face =================\n",
        "# Publishes the fine-tuned T5 model folder to the Hub. If a README.md is not\n",
        "# present (or needs to be refreshed), this cell writes a Hub-compliant model\n",
        "# card with a YAML header so the repo validates cleanly.\n",
        "#\n",
        "# REQUIRES:\n",
        "#   - 'huggingface_hub' installed and logged in (huggingface-cli login)\n",
        "#   - CHECKPOINT_DIR created by Listing 9-4\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime, timezone\n",
        "from huggingface_hub import HfApi, upload_folder\n",
        "\n",
        "# -------------------- Configuration ----------------------------------------\n",
        "USER           = \"gcuomo\"                    # your HF username/org\n",
        "REPO_NAME      = \"byoai-t5-liar-classifier\"  # repo name on the Hub\n",
        "CHECKPOINT_DIR = f\"./models/{REPO_NAME}\"\n",
        "REPO_ID        = f\"{USER}/{REPO_NAME}\"\n",
        "\n",
        "# Optional: lightweight defaults for the model card\n",
        "TASK        = \"text-classification\"\n",
        "PIPELINE    = \"text-classification\"\n",
        "LICENSE     = \"apache-2.0\"\n",
        "LANGUAGE    = [\"en\"]\n",
        "TAGS        = [\"t5\", \"factuality\", \"liar\", \"byoai\", \"education\", \"book\"]\n",
        "DATASETS    = [\"gcuomo/byoai-liar\"]   # or a short note if private/local\n",
        "LIBS        = [\"transformers\", \"datasets\"]\n",
        "MODEL_DESC  = (\n",
        "    \"T5-small fine-tuned to classify short statements into six \"\n",
        "    \"LIAR-style factuality labels: pants-fire, false, barely-true, \"\n",
        "    \"half-true, mostly-true, true. Training data derives from the \"\n",
        "    \"Build Your Own AI book project (BYOAI_LIAR), combining chunk-\"\n",
        "    \"grounded statements with concise contexts.\"\n",
        ")\n",
        "\n",
        "# -------------------- Pull a few details from the training manifest --------\n",
        "manifest_path = os.path.join(CHECKPOINT_DIR, \"training_manifest.json\")\n",
        "train_meta = {}\n",
        "if os.path.exists(manifest_path):\n",
        "    try:\n",
        "        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            train_meta = json.load(f)\n",
        "    except Exception:\n",
        "        train_meta = {}\n",
        "\n",
        "epochs      = train_meta.get(\"epochs\", 5)\n",
        "train_size  = train_meta.get(\"train_size\", None)\n",
        "val_size    = train_meta.get(\"val_size\", None)\n",
        "test_size   = train_meta.get(\"test_size\", None)\n",
        "labels      = train_meta.get(\"labels\", [\n",
        "    \"pants-fire\", \"false\", \"barely-true\",\n",
        "    \"half-true\", \"mostly-true\", \"true\"\n",
        "])\n",
        "template    = (train_meta.get(\"prompt_template\") or\n",
        "               \"classify:\\\\nstatement: ...\\\\ncontext: ...\\\\ntags: ...\\\\nchapter: ...\")\n",
        "\n",
        "# -------------------- Write/refresh model card (README.md) -----------------\n",
        "readme_path = os.path.join(CHECKPOINT_DIR, \"README.md\")\n",
        "created = not os.path.exists(readme_path)\n",
        "\n",
        "epochs_val      = globals().get(\"epochs\", \"N/A\")\n",
        "train_size_val  = globals().get(\"train_size\", \"N/A\")\n",
        "val_size_val    = globals().get(\"val_size\", \"N/A\")\n",
        "test_size_val   = globals().get(\"test_size\", \"N/A\")\n",
        "accuracy_val    = (metrics.get(\"accuracy\") if \"metrics\" in globals() and isinstance(metrics, dict) else \"N/A\")\n",
        "\n",
        "# Build a Hub-compliant model card with valid YAML\n",
        "model_card = f\"\"\"---\n",
        "language: [\"en\"]\n",
        "license: apache-2.0\n",
        "datasets:\n",
        "  - buildyourownai/byoai_liar\n",
        "library_name: transformers\n",
        "pipeline_tag: text-classification\n",
        "tags:\n",
        "  - t5\n",
        "  - factuality\n",
        "  - liar\n",
        "  - open-source\n",
        "  - build-your-own-ai\n",
        "model-index:\n",
        "- name: {REPO_NAME}\n",
        "  results:\n",
        "    - task:\n",
        "        type: text-classification\n",
        "        name: BYOAI factuality classification\n",
        "      dataset:\n",
        "        name: BYOAI_LIAR\n",
        "        type: byoai_liar\n",
        "      metrics:\n",
        "        - type: accuracy\n",
        "          value: {accuracy_val}\n",
        "---\n",
        "\n",
        "# {REPO_NAME}\n",
        "\n",
        "Fine-tuned **T5-small** to classify statements into six factuality labels:\n",
        "`pants-fire`, `false`, `barely-true`, `half-true`, `mostly-true`, `true`.\n",
        "\n",
        "**Source:** Generated from the book *Build Your Own AI* dataset (BYOAI_LIAR).\n",
        "Includes short, structured inputs:\n",
        "classify:\n",
        "statement:\n",
        "context:\n",
        "tags:\n",
        "chapter:\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "tok = AutoTokenizer.from_pretrained(\"{REPO_ID}\")\n",
        "mdl = AutoModelForSeq2SeqLM.from_pretrained(\"{REPO_ID}\")\n",
        "prompt = '''classify:\n",
        "statement: RAG retrieves passages from a vector store like ChromaDB before generating.\n",
        "context: RAG retrieval then generation\n",
        "tags: data-prep, feature-engineering, rag\n",
        "chapter: Prepping Data for AI'''\n",
        "out = mdl.generate(**tok(prompt, return_tensors=\"pt\", truncation=True, max_length=128))\n",
        "print(tok.decode(out[0], skip_special_tokens=True))\n",
        "\n",
        "## Training\n",
        "\t•\tBase model: t5-small\n",
        "\t•\tEpochs: {epochs_val}\n",
        "\t•\tTrain/Val/Test sizes: {train_size_val} / {val_size_val} / {test_size_val}\n",
        "\t•\tLabels: pants-fire, false, barely-true, half-true, mostly-true, true\n",
        "\t•\tPrompt template as above.\n",
        "\n",
        "## Limitations\n",
        "\n",
        "Border classes (e.g., true vs mostly-true) can be confused. Provide short,\n",
        "specific context and tags for best results.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this model in academic or educational work, please cite:\n",
        "\n",
        "> Cuomo, G., & De Jesús, J. *Build Your Own AI*. BYOAI Project, 2025-2026.\n",
        "\"\"\"\n",
        "\n",
        "with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(model_card)\n",
        "\n",
        "print(f\"{'Created' if created else 'Updated'} model card → {readme_path}\")\n",
        "\n",
        "# -------------------- Create repo and upload folder ------------------------\n",
        "api = HfApi()\n",
        "api.create_repo(repo_id=REPO_ID, exist_ok=True)\n",
        "\n",
        "upload_folder(\n",
        "    repo_id=REPO_ID,\n",
        "    folder_path=CHECKPOINT_DIR,\n",
        "    path_in_repo=\".\",\n",
        "    commit_message = (\n",
        "       f\"Upload checkpoint and model card ({datetime.now(timezone.utc).isoformat()})\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(f\"✅ Upload complete: https://huggingface.co/{REPO_ID}\")"
      ],
      "metadata": {
        "id": "jneuVzUMkg_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 9-6: Loading and Running the Model Locally\n",
        "\n",
        "This listing demonstrates how to load the fine-tuned BYOAI T5 factuality model\n",
        "from the Hugging Face Hub and run it directly in a local Python environment.\n",
        "It retrieves both the model and tokenizer, constructs a prompt, and produces\n",
        "a truthfulness label prediction in real time.\n",
        "\n",
        "You can run this in environments such as Colab, Jupyter, or a private\n",
        "cloud runtime. Once loaded, inference happens locally — no internet\n",
        "connection is required after the first download.\n",
        "\n",
        "This example provides a quick verification that the model functions as\n",
        "expected once published and illustrates how other developers or readers\n",
        "can experiment with the classifier using natural-language inputs."
      ],
      "metadata": {
        "id": "V3JkUV6VReab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Listing 9-6: Load & run the fine-tuned model locally ==================\n",
        "# Loads the published BYOAI factuality classifier from the Hugging Face Hub,\n",
        "# prepares the tokenizer, and runs quick, local predictions.\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "\n",
        "# -------------------- Model location ---------------------------------------\n",
        "model_name = \"gcuomo/byoai-t5-liar-classifier\"  # Replace with your HF repo ID\n",
        "\n",
        "# -------------------- Load model & tokenizer -------------------------------\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# -------------------- Inference helper -------------------------------------\n",
        "def run_prediction(statement: str, max_input_len: int = 128):\n",
        "    \"\"\"\n",
        "    Runs a single factuality classification using the published model.\n",
        "    Input is formatted to match the text-to-text training template.\n",
        "    \"\"\"\n",
        "    prompt = f\"classify:\\nstatement: {statement}\"\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_input_len,\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=8,\n",
        "            do_sample=False,\n",
        "            num_beams=1,\n",
        "        )\n",
        "\n",
        "    label = tokenizer.decode(output[0], skip_special_tokens=True).strip().lower()\n",
        "    print(f\"Statement: {statement}\")\n",
        "    print(f\"Predicted label: {label}\\n\")\n",
        "\n",
        "# -------------------- Example usage ----------------------------------------\n",
        "tests = [\n",
        "\n",
        "    # TRUE (Clément Delangue) – accurate book-related claim\n",
        "    (\"Clément Delangue is the CEO and co-founder of Hugging Face.\", \"true\"),\n",
        "\n",
        "    # MOSTLY-TRUE – remove a nuance that usually applies\n",
        "    (\"Fine-tuning always improves a T5 model’s accuracy on any dataset.\", \"mostly-true\"),\n",
        "\n",
        "    # HALF-TRUE – one detail wrong (tool/metric/threshold)\n",
        "    (\"Feature engineering improved F1 score by adding TF-IDF from PyTorch tensors.\", \"half-true\"),\n",
        "\n",
        "    # BARELY-TRUE – topic words, but overreach\n",
        "    (\"Granite models solve safety, fairness, and privacy in one step.\", \"barely-true\"),\n",
        "\n",
        "    # FALSE – flip the core claim\n",
        "    (\"Standardizing inputs makes agentic AI experiments less reproducible.\", \"false\"),\n",
        "\n",
        "    # PANTS-FIRE – impossible/extreme\n",
        "    (\"Our classifier reads minds to label truth without any text.\", \"pants-fire\"),\n",
        "]\n",
        "\n",
        "for stmt, gold in tests:\n",
        "    run_prediction(stmt)\n",
        "    print(\"gold:\", gold, \"\\n\")\n"
      ],
      "metadata": {
        "id": "HkOzlnOuByGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calling the Hosted Model via Hugging Face Spaces\n",
        "\n",
        "This cell shows how to invoke the hosted model remotely using the\n",
        "`gradio_client` library. The model runs inside a Hugging Face Space and\n",
        "exposes a simple API endpoint for inference. This allows you to test\n",
        "statements from any Python environment without managing servers or\n",
        "dependencies.\n",
        "\n",
        "Note: On the free tier, Spaces enter sleep mode when idle. The first\n",
        "request automatically wakes the Space, which can take several minutes\n",
        "to respond. Subsequent requests run normally.\n",
        "\n",
        "Before running this code, install the client with:\n",
        "`pip install gradio_client`"
      ],
      "metadata": {
        "id": "1e-XusD0XNM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install the client (only needs to be run once per Colab session)\n",
        "!pip install -q gradio_client"
      ],
      "metadata": {
        "id": "u7dl6ROemxgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Remote inference via Hugging Face Space (Gradio Client) ===============\n",
        "# REQUIRES:\n",
        "#   - A deployed Space with an active 'predict' endpoint\n",
        "#   - The statement text to classify (and optional gold label for reference)\n",
        "#\n",
        "# Notes:\n",
        "# - On the free tier, Spaces sleep when idle. The first request will\n",
        "#   automatically wake the Space and can take 3–5 minutes to respond.\n",
        "#   Subsequent requests run normally.\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "from gradio_client import Client\n",
        "from time import perf_counter\n",
        "\n",
        "# Initialize the client using your Space ID (username/space-name)\n",
        "client = Client(\"gcuomo/byoai-liar-demo\")\n",
        "\n",
        "# Define statement and optional gold label for comparison\n",
        "statement = \"The book 'Build Your Own AI' explores Hugging Face models.\"\n",
        "gold_label = \"half_true\"  # optional; leave empty if unknown\n",
        "\n",
        "# Run remote inference with timing\n",
        "try:\n",
        "    start = perf_counter()\n",
        "    result = client.predict(statement)\n",
        "    elapsed = perf_counter() - start\n",
        "\n",
        "    # Display formatted results\n",
        "    print(\"=== Remote BYOAI_LIAR Inference ===\")\n",
        "    print(f\"Statement:       {statement}\")\n",
        "    print(f\"Predicted:       {result}\")\n",
        "    if gold_label:\n",
        "        print(f\"Gold (expected): {gold_label}\")\n",
        "    print(f\"Inference time:  {elapsed:.2f} sec\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"⚠️  Error connecting to the remote Space:\", e)\n",
        "    print(\"If this is the first request, the Space may still be waking up.\")\n"
      ],
      "metadata": {
        "id": "7CrTWOKzXR6Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}