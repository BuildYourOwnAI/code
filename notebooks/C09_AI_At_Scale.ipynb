{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 9 – AI at Scale\n",
        "This notebook walks through the end-to-end workflow for training, evaluating, and scaling a fine-tuned `T5 model` on a hybrid `LIAR` dataset. The dataset combines 2,500 fact-checked political claims from the original LIAR benchmark with 225 additional statements generated from the Open Source AI book. These synthetic entries mimic the tone and compression style of real claims, but focus on AI-related topics like open-source tooling, model capabilities, and community beliefs. The notebook includes training, baseline evaluation, scaling experiments, and model publishing. Each listing spans multiple cells grouped around specific tasks like logging, benchmarking, or inference testing.\n",
        "\n",
        "**Note:**\n",
        "This notebook uses the Hugging Face Hub to download datasets and upload model checkpoints. To access certain datasets (like liar) and to publish your model to the Hub, you'll need to provide a Hugging Face access token. Colab will prompt you to enter your HF_TOKEN the first time it's needed, and securely store it for the session. You can create or manage your token at huggingface.co/settings/tokens.\n"
      ],
      "metadata": {
        "id": "dY22l7TzSr6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 9.1 – Fine-Tuning T5 on the Merged LIAR Dataset\n",
        "\n",
        "This listing walks through the full workflow for preparing, training, and testing a T5 model using a summarization-style format. It begins with a helper cell that loads and merges the datasets, defines utility functions, and runs a quick baseline using the untrained model. From there, it moves into fine-tuning on the combined dataset and finishes with a few sample predictions to check that everything is working as expected.\n",
        "\n",
        "*Note:* Be sure to run all the code cells below in order to ensure everything works as expected.\n"
      ],
      "metadata": {
        "id": "2cGuRHqHfPPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "GtuNh54ITgvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Functions for Preprocessing and Tokenization\n",
        "\n",
        "This cell defines utility functions for formatting the LIAR dataset to work with T5.\n",
        "The `preprocess()` function wraps political statements into a text-to-text prompt,\n",
        "and `tokenize()` handles batch-safe tokenization for both inputs and target labels.\n",
        "These functions are used in the main training flow to prepare the model's data.\n"
      ],
      "metadata": {
        "id": "1sCSO6oieuJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions for preparing, tokenizing, and testing with T5 on the LIAR + OSAI dataset\n",
        "\n",
        "# === Constants ===\n",
        "\n",
        "BASE_URL = \"https://opensourceai-book.github.io/code/datasets/\"\n",
        "INFO_FILE = \"open_source_ai-liar.csv\"\n",
        "MAX_TOKENS = 128\n",
        "\n",
        "# Canonical labels used by the model\n",
        "FACTUALITY_LABELS = [\n",
        "    \"pants-fire\", \"false\", \"barely-true\",\n",
        "    \"half-true\", \"mostly-true\", \"true\"\n",
        "]\n",
        "\n",
        "# For legacy use (e.g., LIAR numeric labels)\n",
        "LABEL_MAP_NUMERIC = {str(i): label for i, label in enumerate(FACTUALITY_LABELS)}\n",
        "LABEL_TO_INDEX = {label: i for i, label in enumerate(FACTUALITY_LABELS)}\n",
        "VALID_LABELS = set(FACTUALITY_LABELS)\n",
        "\n",
        "# Tokenize input and target fields for use with T5\n",
        "def tokenize(batch, tokenizer):\n",
        "    input_texts = batch[\"input_text\"]\n",
        "    target_texts = batch[\"target_text\"]\n",
        "\n",
        "    if len(input_texts) != len(target_texts):\n",
        "        raise ValueError(\"Mismatched input and target sizes.\")\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        input_texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_TOKENS\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        text_target=target_texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=16\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Run a single prediction on a freeform statement.\n",
        "# Optionally compare against a known true label if available.\n",
        "def run_sample_prediction(model, tokenizer, statement, true_label=None):\n",
        "    input_text = f\"summarize: {statement}\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_TOKENS\n",
        "    )\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    output = model.generate(**inputs, max_new_tokens=8)\n",
        "\n",
        "    prediction = tokenizer.decode(output[0], skip_special_tokens=True).strip().lower()\n",
        "    mapped_label = prediction if prediction in VALID_LABELS else f\"(unknown: {prediction})\"\n",
        "\n",
        "    print(\"\\n=== Sample Prediction ===\")\n",
        "    print(\"Statement:\", statement)\n",
        "    print(\"Prediction:\", mapped_label)\n",
        "    if true_label:\n",
        "        print(\"Expected:  \", true_label)\n",
        "\n",
        "    return {\n",
        "        \"statement\": statement,\n",
        "        \"prediction\": mapped_label,\n",
        "        \"true_label\": true_label\n",
        "    }\n",
        "\n",
        "# Load datasets and dataframes\n",
        "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# === Load and initialize LIAR + OSAI datasets ===\n",
        "\n",
        "osai_df = pd.read_csv(BASE_URL + INFO_FILE)\n",
        "osai_df[\"target_text\"] = osai_df[\"label\"].astype(str).str.strip().str.lower()\n",
        "osai_df = osai_df[osai_df[\"target_text\"].isin(VALID_LABELS)]\n",
        "osai_df[\"input_text\"] = \"summarize: \" + osai_df[\"statement\"].astype(str)\n",
        "osai_dataset = Dataset.from_pandas(osai_df[[\"input_text\", \"target_text\"]])\n",
        "\n",
        "# Load and map LIAR dataset\n",
        "liar_raw = load_dataset(\"liar\", trust_remote_code=True)\n",
        "\n",
        "def format_liar(example):\n",
        "    return {\n",
        "        \"input_text\": f\"summarize: {example['statement']}\",\n",
        "        \"target_text\": LABEL_MAP_NUMERIC.get(str(example[\"label\"]), \"unknown\")\n",
        "    }\n",
        "\n",
        "liar_formatted = liar_raw.map(format_liar, remove_columns=liar_raw[\"train\"].column_names)\n",
        "liar_train_subset = liar_formatted[\"train\"].select(range(2500))\n",
        "liar_test_set = liar_formatted[\"test\"].filter(lambda x: x[\"input_text\"] and x[\"target_text\"])\n",
        "\n",
        "# Convert LIAR to DataFrame\n",
        "liar_df = pd.DataFrame(liar_train_subset)\n",
        "\n",
        "# Merge Hugging Face LIAR and local OSAI datasets\n",
        "merged_train = concatenate_datasets([liar_train_subset, osai_dataset])\n",
        "merged_train = merged_train.filter(lambda x: x[\"input_text\"] and x[\"target_text\"])\n",
        "\n",
        "# Store merged datasets\n",
        "liar_merged_dataset = DatasetDict({\n",
        "    \"train\": merged_train,\n",
        "    \"test\": liar_test_set\n",
        "})\n",
        "\n",
        "# === Dataset summary and preview ===\n",
        "\n",
        "print(\"\\n=== LIAR + OSAI Merged Dataset Summary ===\")\n",
        "print(f\"Train set size: {len(liar_merged_dataset['train'])}\")\n",
        "print(f\"Test set size:  {len(liar_merged_dataset['test'])}\")\n",
        "\n",
        "# Label distribution from LIAR + OSAI merged data\n",
        "label_counts = Counter(liar_df[\"target_text\"].tolist() + osai_df[\"target_text\"].tolist())\n",
        "print(\"\\nLabel distribution across merged data:\")\n",
        "for label, count in sorted(label_counts.items()):\n",
        "    print(f\"{label:<15} {count}\")\n",
        "\n",
        "# Sample entries from LIAR\n",
        "print(\"\\nSample LIAR entries (from Hugging Face):\")\n",
        "for i in range(min(3, len(liar_df))):\n",
        "    print(f\"\\nLIAR Example {i+1}\")\n",
        "    print(\"Input: \", liar_df.iloc[i][\"input_text\"])\n",
        "    print(\"Target:\", liar_df.iloc[i][\"target_text\"])\n",
        "\n",
        "# Sample entries from OSAI\n",
        "print(\"\\nSample OSAI entries (from local CSV):\")\n",
        "for i in range(min(3, len(osai_df))):\n",
        "    print(f\"\\nOSAI Example {i+1}\")\n",
        "    print(\"Input: \", osai_df.iloc[i][\"input_text\"])\n",
        "    print(\"Target:\", osai_df.iloc[i][\"target_text\"])"
      ],
      "metadata": {
        "id": "OW95Na20ez9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Establishing a Baseline Before Fine-Tuning\n",
        "\n",
        "Before training T5 on the merged LIAR dataset, we’ll run a baseline inference using the untrained model. This gives us a reference point to evaluate how much the model improves after fine-tuning. In this experiment, we’ll select a few samples from each dataset from the test and train split and observe how the base model performs out of the box."
      ],
      "metadata": {
        "id": "dE1qY7aGIGIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load base (untrained) model and tokenizer\n",
        "model_name = \"t5-small\"\n",
        "bl_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "bl_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "print(\"=== Baseline Inference Using Untrained T5 ===\")\n",
        "\n",
        "# 3 samples from LIAR (via liar_df)\n",
        "print(\"\\n--- LIAR Examples (from Hugging Face) ---\")\n",
        "for i in range(min(3, len(liar_df))):\n",
        "    row = liar_df.iloc[i]\n",
        "    run_sample_prediction(bl_model, bl_tokenizer, row[\"input_text\"].replace(\"summarize: \", \"\"), true_label=row[\"target_text\"])\n",
        "\n",
        "# 3 samples from OSAI (via osai_df)\n",
        "print(\"\\n--- OSAI Examples (from local CSV) ---\")\n",
        "for i in range(min(3, len(osai_df))):\n",
        "    row = osai_df.iloc[i]\n",
        "    run_sample_prediction(bl_model, bl_tokenizer, row[\"input_text\"].replace(\"summarize: \", \"\"), true_label=row[\"target_text\"])\n"
      ],
      "metadata": {
        "id": "Xj5stZQ8IWQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fine-Tuning T5 on the Merged LIAR Dataset\n",
        "\n",
        "With the baseline results in hand, we’re ready to fine-tune T5 on a combined dataset. This version merges 2,500 samples from the original LIAR benchmark with all entries from our custom CSV of AI-generated statements. The goal is to help the model learn to generate truthfulness labels using T5’s text-to-text format.\n",
        "\n",
        "We tested different training durations and found that running for 3 to 5 epochs offers a good balance—enough to improve accuracy without overfitting.\n"
      ],
      "metadata": {
        "id": "hmHVFV05gftj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phLnQsHSRpDV"
      },
      "outputs": [],
      "source": [
        "# Fine-tune T5 on the merged LIAR dataset (HF + OSAI)\n",
        "\n",
        "from transformers import (\n",
        "    T5Tokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        "    EvalPrediction\n",
        ")\n",
        "import torch\n",
        "\n",
        "# Set reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=True)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize using updated utility function (now requires tokenizer explicitly)\n",
        "tokenized = liar_merged_dataset.map(lambda batch: tokenize(batch, tokenizer), batched=True)\n",
        "\n",
        "# Drop raw columns used for training\n",
        "tokenized = tokenized.remove_columns([\"input_text\", \"target_text\"])\n",
        "\n",
        "# Prepare training set\n",
        "train_data = tokenized[\"train\"]\n",
        "\n",
        "# Define training arguments\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=5,\n",
        "    remove_unused_columns=False,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=250,\n",
        "    save_steps=500,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_data\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Print final loss from the trainer (if stored)\n",
        "if trainer.state.log_history:\n",
        "    final_logs = [log for log in trainer.state.log_history if \"loss\" in log]\n",
        "    if final_logs:\n",
        "        print(f\"Final training loss: {final_logs[-1]['loss']:.4f}\")\n",
        "\n",
        "# Print model size (parameter count)\n",
        "param_count = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model parameters: {param_count:,}\")\n",
        "\n",
        "# Confirm training\n",
        "print(\"\\nTraining complete.\")\n",
        "print(f\"Trained on {len(train_data)} samples.\")\n",
        "print(f\"Model checkpoint saved to: {args.output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating the Fine-Tuned T5 Model\n",
        "\n",
        "After training, we evaluate the fine-tuned T5 model on examples from the test\n",
        "split of the merged LIAR dataset. This helps us compare predictions against\n",
        "our earlier baseline and observe how the model's responses have improved.\n"
      ],
      "metadata": {
        "id": "UP_bN0_cxVk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"===  Inference Using the FINETUNED version of T5 ===\")\n",
        "\n",
        "# 3 samples from LIAR (via liar_df)\n",
        "print(\"\\n--- LIAR Examples (from Hugging Face) ---\")\n",
        "for i in range(min(3, len(liar_df))):\n",
        "    row = liar_df.iloc[i]\n",
        "    run_sample_prediction(model, tokenizer, row[\"input_text\"].replace(\"summarize: \", \"\"), true_label=row[\"target_text\"])\n",
        "\n",
        "# 3 samples from OSAI (via osai_df)\n",
        "print(\"\\n--- OSAI Examples (from local CSV) ---\")\n",
        "for i in range(min(3, len(osai_df))):\n",
        "    row = osai_df.iloc[i]\n",
        "    run_sample_prediction(model, tokenizer, row[\"input_text\"].replace(\"summarize: \", \"\"), true_label=row[\"target_text\"])\n"
      ],
      "metadata": {
        "id": "5F39Cq_E7f6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 9‑2:  Measuring Inference Time Across Input Lengths\n",
        "\n",
        "This listing benchmarks how long it takes the fine-tuned T5 model to generate outputs across a range of input lengths. The first cell defines helper functions to create synthetic prompts, run timed inferences on both GPU and CPU, and plot the results. The second cell runs the benchmark using those tools, measuring average inference time across token-length bins. Together, they give us a clear picture of how input size and hardware impact latency—an important factor when thinking about scaling beyond the notebook.\n"
      ],
      "metadata": {
        "id": "MdBj91-ypbFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "# Benchmark T5 inference time across increasing input sizes.\n",
        "# Returns a dictionary of average latency by token length bin.\n",
        "def benchmark_inference_time(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    bins=None,\n",
        "    samples_per_bin=5,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Optional warm-up to stabilize performance\n",
        "    _ = model.generate(\n",
        "        **tokenizer(\"warm up\", return_tensors=\"pt\").to(device),\n",
        "        max_new_tokens=16\n",
        "    )\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Default token length bins: 50–1049 in steps of 50\n",
        "    if bins is None:\n",
        "        bins = list(range(50, 1050, 50))\n",
        "\n",
        "    timing = {}\n",
        "\n",
        "    for b in bins:\n",
        "        label = f\"{b}-{b+49}\"\n",
        "        timing[label] = []\n",
        "\n",
        "        # Dynamically extend max_length to avoid truncating longer bins\n",
        "        max_length = b + 16\n",
        "\n",
        "        for i in range(samples_per_bin + 2):  # +2 to absorb caching\n",
        "            repeated = \"The sky is blue. \" * (b // 5)\n",
        "            prompt = f\"summarize: {repeated}\"\n",
        "\n",
        "            inputs = tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,  # Safe truncation if prompt slightly exceeds max_length\n",
        "                max_length=max_length\n",
        "            )\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "            start = time.time()\n",
        "            _ = model.generate(**inputs, max_new_tokens=16)\n",
        "\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "            elapsed = time.time() - start\n",
        "\n",
        "            # Skip warm-up samples\n",
        "            if i >= 2:\n",
        "                timing[label].append(elapsed)\n",
        "\n",
        "    return timing\n",
        "\n",
        "# Plot average inference time per input length bin.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Accepts multiple timing dictionaries for comparison.\n",
        "def plot_inference_times(timing_dicts, labels, title):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "\n",
        "    for timing, label in zip(timing_dicts, labels):\n",
        "        avg_times = [sum(timing[k]) / len(timing[k]) for k in timing]\n",
        "        keys = list(timing.keys())\n",
        "\n",
        "        # Print results to console\n",
        "        print(f\"\\n{label} Inference Times:\")\n",
        "        for bin_label, time_val in zip(keys, avg_times):\n",
        "            print(f\"  {bin_label}: {time_val:.4f} sec\")\n",
        "\n",
        "        # Plot results\n",
        "        plt.plot(keys, avg_times, marker=\"o\", label=label)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Token Length (bins)\")\n",
        "    plt.ylabel(\"Average Inference Time (sec)\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "dQAGBPa6C71a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Measuring T5 Inference Time by Input Length\n",
        "This cell runs the benchmark on both GPU and CPU using the same model and tokenizer.\n",
        "It measures average inference time across a range of input lengths and\n",
        "plots the results to compare performance between the two devices."
      ],
      "metadata": {
        "id": "Qa3I8YsXrNa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Benchmark on GPU\n",
        "gpu_timing = benchmark_inference_time(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    device=\"cuda\"\n",
        ")\n",
        "\n",
        "# Benchmark on CPU\n",
        "cpu_timing = benchmark_inference_time(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    device=\"cpu\"\n",
        ")\n",
        "\n",
        "# Plot the results\n",
        "plot_inference_times(\n",
        "    [gpu_timing, cpu_timing],\n",
        "    [\"T5-small (GPU)\", \"T5-small (CPU)\"],\n",
        "    \"T5 Inference Time vs Input Length (GPU vs CPU)\"\n",
        ")"
      ],
      "metadata": {
        "id": "rS6mPtjsC-v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 9‑3: Measuring the Benefit of Batching in T5 Inference\n",
        "\n",
        "This experiment benchmarks how batching affects inference performance in our fine-tuned T5 model. The listing spans two cells: the first defines a helper function to generate synthetic inputs and time model responses across different batch sizes, while the second runs the benchmark and prints a summary table. For each batch size, the code measures average latency per sample, total throughput in samples per second, and relative speedup compared to batch size 1."
      ],
      "metadata": {
        "id": "IzDj9pDyLAZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Benchmark T5 inference with batching across batch sizes\n",
        "def benchmark_inference(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    batch_size,\n",
        "    token_len=512,\n",
        "    max_tokens=512,\n",
        "    padding=\"max_length\",\n",
        "    repeat=5,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "):\n",
        "    \"\"\"Benchmark T5 inference using random prompts at fixed length and batch size.\"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    random.seed(42)\n",
        "\n",
        "    # Generate synthetic inputs of roughly token_len size\n",
        "    phrases = [\n",
        "        \"The sky is blue\", \"Water is wet\", \"Cats chase mice\",\n",
        "        \"Birds fly south\", \"Ice is cold\", \"Fire is hot\",\n",
        "        \"Rain falls down\", \"Fish swim fast\", \"Clouds block sun\"\n",
        "    ]\n",
        "    inputs = []\n",
        "    for _ in range(batch_size):\n",
        "        sentence = \". \".join(random.choices(phrases, k=token_len // 10))\n",
        "        inputs.append(f\"summarize: {sentence}\")\n",
        "\n",
        "    # Adjust tokenizer max length to avoid truncation\n",
        "    max_length = max(token_len + 16, max_tokens)\n",
        "\n",
        "    # Warm-up run to stabilize GPU/CPU\n",
        "    enc = tokenizer(\n",
        "        inputs[:1],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=padding,\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "    _ = model.generate(**{k: v.to(device) for k, v in enc.items()})\n",
        "\n",
        "    # Measure repeated inference times\n",
        "    elapsed_times = []\n",
        "    for _ in range(repeat):\n",
        "        enc = tokenizer(\n",
        "            inputs,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=padding,\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        )\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        _ = model.generate(**enc, max_new_tokens=16)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        elapsed_times.append(time.time() - start)\n",
        "\n",
        "    avg_batch_time = sum(elapsed_times) / repeat\n",
        "    avg_time_per_sample = avg_batch_time / batch_size\n",
        "    throughput = batch_size / avg_batch_time\n",
        "\n",
        "    return {\n",
        "        \"batch_size\": batch_size,\n",
        "        \"token_len\": token_len,\n",
        "        \"time_per_sample\": avg_time_per_sample,\n",
        "        \"batch_time\": avg_batch_time,\n",
        "        \"throughput\": throughput\n",
        "    }\n",
        "\n",
        "# Plot and print results for batching benchmarks\n",
        "def plot_and_print_batch_results(results, title=\"Batching Impact: Latency vs Throughput\"):\n",
        "    batch_labels = [str(r[\"batch_size\"]) for r in results]\n",
        "    latencies = [r[\"time_per_sample\"] for r in results]\n",
        "    throughputs = [r[\"throughput\"] for r in results]\n",
        "\n",
        "    bar_color = \"#0074D9\"\n",
        "    line_color = \"#FF4136\"\n",
        "    grid_color = \"#AAAAAA\"\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(9, 5))\n",
        "\n",
        "    ax1.bar(batch_labels, throughputs, color=bar_color, edgecolor=\"black\", label=\"Throughput\")\n",
        "    ax1.set_xlabel(\"Batch Size\")\n",
        "    ax1.set_ylabel(\"Throughput (samples/sec)\", color=bar_color)\n",
        "    ax1.tick_params(axis=\"y\", labelcolor=bar_color)\n",
        "    ax1.set_ylim(0, max(throughputs) * 1.2)\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(batch_labels, latencies, color=line_color, marker=\"o\", linewidth=2, label=\"Latency\")\n",
        "    ax2.set_ylabel(\"Avg Inference Time per Sample (sec)\", color=line_color)\n",
        "    ax2.tick_params(axis=\"y\", labelcolor=line_color)\n",
        "    ax2.set_ylim(0, max(latencies) * 1.2)\n",
        "\n",
        "    ax1.grid(True, axis=\"y\", linestyle=\"--\", color=grid_color)\n",
        "    plt.title(title)\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"{'Batch':<8}{'Latency (s)':<15}{'Throughput (samples/s)':<25}\")\n",
        "    for r in results:\n",
        "        print(f\"{r['batch_size']:<8}{r['time_per_sample']:<15.4f}{r['throughput']:<25.2f}\")\n"
      ],
      "metadata": {
        "id": "otKfuylxICot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run Benchmark and Plot the Results"
      ],
      "metadata": {
        "id": "vGJf8ycUqSdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
        "results = [\n",
        "    benchmark_inference(model, tokenizer, bs)\n",
        "    for bs in batch_sizes\n",
        "]\n",
        "plot_and_print_batch_results(results)\n"
      ],
      "metadata": {
        "id": "He6C3gCIqZCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 9‑4: Saving and Logging a Versioned Model Run\n",
        "\n",
        "This example saves a trained T5 model and tokenizer to a versioned checkpoint\n",
        "directory, then logs a structured record of an inference run to a local file.\n",
        "The log includes metadata like timestamp, input length, labels, and inference time.\n"
      ],
      "metadata": {
        "id": "w3fACrxyWB9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "import random\n",
        "\n",
        "def save_model_and_tokenizer(model, tokenizer, checkpoint_dir):\n",
        "    \"\"\"Save model and tokenizer with optional tokenizer patch.\"\"\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    model.save_pretrained(checkpoint_dir, safe_serialization=True)\n",
        "    tokenizer.save_pretrained(checkpoint_dir)\n",
        "\n",
        "    # Patch tokenizer_config with model type if needed\n",
        "    config_path = f\"{checkpoint_dir}/tokenizer_config.json\"\n",
        "    if os.path.exists(config_path):\n",
        "        with open(config_path, \"r+\") as f:\n",
        "            config = json.load(f)\n",
        "            config[\"model_type\"] = \"t5\"\n",
        "            f.seek(0)\n",
        "            json.dump(config, f, indent=2)\n",
        "            f.truncate()\n",
        "\n",
        "def summarize_batch_metrics(results):\n",
        "    \"\"\"Extract key performance scaling info from batch test results.\"\"\"\n",
        "    best = max(results, key=lambda r: r[\"throughput\"])\n",
        "    return {\n",
        "        \"tested_batch_sizes\": [r[\"batch_size\"] for r in results],\n",
        "        \"throughput_per_batch\": [round(r[\"throughput\"], 2) for r in results],\n",
        "        \"latency_per_batch\": [\n",
        "            round(r[\"time_per_sample\"], 4) for r in results\n",
        "        ],\n",
        "        \"sweet_spot_batch_size\": best[\"batch_size\"],\n",
        "        \"sweet_spot_throughput\": round(best[\"throughput\"], 2),\n",
        "        \"sweet_spot_latency\": round(best[\"time_per_sample\"], 4)\n",
        "    }\n",
        "\n",
        "# Utility to run a prediction and time it\n",
        "def time_prediction(model, tokenizer, text, max_input_len=256):\n",
        "    import time\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_input_len\n",
        "    )\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    start = time.time()\n",
        "    output = model.generate(**inputs, max_new_tokens=16)\n",
        "    duration = time.time() - start\n",
        "\n",
        "    prediction = tokenizer.decode(output[0], skip_special_tokens=True).strip().lower()\n",
        "    return prediction, duration\n",
        "\n",
        "def write_log_entry(log_entry, log_path):\n",
        "    \"\"\"Append structured JSONL entry to the given log file.\"\"\"\n",
        "    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
        "    with open(log_path, \"a\") as f:\n",
        "        f.write(json.dumps(log_entry) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "WHMob_AYWU6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import random\n",
        "\n",
        "# Define model name and checkpoint location\n",
        "model_name = \"open-source-ai-t5-liar-lens\"\n",
        "checkpoint_dir = f\"./models/{model_name}\"\n",
        "log_path = f\"{checkpoint_dir}/model_log.jsonl\"\n",
        "\n",
        "# Save model and tokenizer to local checkpoint folder\n",
        "save_model_and_tokenizer(model, tokenizer, checkpoint_dir)\n",
        "\n",
        "# Generate a synthetic prompt (used for inference benchmarking)\n",
        "phrases = [\n",
        "    \"The sky is blue\", \"Water is wet\", \"Cats chase mice\",\n",
        "    \"Birds fly south\", \"Ice is cold\", \"Fire is hot\",\n",
        "    \"Rain falls down\", \"Fish swim fast\", \"Clouds block sun\"\n",
        "]\n",
        "raw_statement = \". \".join(random.choices(phrases, k=512 // 10))\n",
        "sample_text = raw_statement\n",
        "true_label = \"unknown (benchmark prompt)\"\n",
        "\n",
        "# Run timed prediction using prompt format used in training\n",
        "prediction, elapsed_time = time_prediction(\n",
        "    model, tokenizer, f\"summarize: {sample_text}\"\n",
        ")\n",
        "\n",
        "# Build structured log entry for reproducibility\n",
        "log_entry = {\n",
        "    \"model_instance\": model_name,\n",
        "    \"base_model\": \"t5-small\",\n",
        "    \"dataset\": \"LIAR\",\n",
        "    \"checkpoint\": checkpoint_dir,\n",
        "    \"batch_size\": 4,\n",
        "    \"epochs\": 5,\n",
        "    \"version_datetime_stamp\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"inference_sample_index\": \"synthetic\",\n",
        "    \"inference_input_length\": len(\n",
        "        tokenizer.tokenize(f\"summarize: {sample_text}\")\n",
        "    ),\n",
        "    \"predicted_label\": prediction,\n",
        "    \"true_label\": true_label,\n",
        "    \"inference_time_sec\": round(elapsed_time, 3),\n",
        "    \"notes\": (\n",
        "        \"Benchmark run for summarization-style classification using \"\n",
        "        \"fine-tuned T5. Prompt synthesized from randomized short \"\n",
        "        \"factual phrases.\"\n",
        "    ),\n",
        "    \"batching_scaling_summary\": summarize_batch_metrics(results)\n",
        "}\n",
        "\n",
        "# Save log to JSONL for later analysis or publishing\n",
        "write_log_entry(log_entry, log_path)\n",
        "\n",
        "print(f\"Model saved to: {checkpoint_dir}\")\n",
        "print(f\"Log saved to:   {log_path}\")"
      ],
      "metadata": {
        "id": "duRNqttAhzx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 9-5: Uploading Model and Metadata to Hugging Face\n",
        "This cell automates the process of publishing a trained model to the Hugging Face Hub. It uses the huggingface_hub API to create the repository (if needed) and upload all model artifacts stored in the checkpoint directory, including weights, tokenizer files, and metadata. Once uploaded, the model is publicly available for others to download, test, or fine-tune.\n",
        "\n",
        "To run this, make sure you’ve:\n",
        "\n",
        "- Run the pip install\n",
        "\n",
        "- Replaced your_huggingface_repo_name_here with your actual username\n",
        "\n",
        "- Run model.save_pretrained() and tokenizer.save_pretrained() earlier to populate the folder\n",
        "\n",
        "- This step ensures your work isn’t locked in a local runtime — it’s published, versioned, and ready for reuse."
      ],
      "metadata": {
        "id": "RHfkV71_QD8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub"
      ],
      "metadata": {
        "id": "7a8XNRVAkZv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, upload_folder\n",
        "\n",
        "# Define repo info\n",
        "repo_name = \"open-source-ai-t5-liar-lens\"\n",
        "checkpoint_dir = \"./models/open-source-ai-t5-liar-lens\"\n",
        "user = \"your_huggingface_repo_name_here\"  # Your Hugging Face username\n",
        "repo_id = f\"{user}/{repo_name}\"\n",
        "\n",
        "# Create the repo if it doesn't already exist\n",
        "api = HfApi()\n",
        "api.create_repo(repo_id=repo_id, exist_ok=True)\n",
        "\n",
        "# Upload model folder with commit message and chunked commits\n",
        "upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    folder_path=checkpoint_dir,\n",
        "    path_in_repo=\".\",\n",
        "    commit_message=(\n",
        "        \"Fine-tuned T5-small model on hybrid LIAR dataset including 225 \"\n",
        "        \"AI-generated quotes from the Open Source AI book. Includes benchmark \"\n",
        "        \"log showing latency and throughput scaling across batch sizes. \"\n",
        "        \"Saved in safetensors format.\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "jneuVzUMkg_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 9-6: Loading and Running the Model Locally\n",
        "This example loads the fine-tuned model directly from the Hugging Face Hub using the transformers library. It downloads the model and tokenizer, formats the input using a summarization-style prompt, and generates a factuality label. This approach is useful for running predictions in a local Python environment such as Colab, a Jupyter notebook, or a private cloud instance."
      ],
      "metadata": {
        "id": "V3JkUV6VReab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"gcuomo/open-source-ai-t5-liar-lens\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def run_prediction(statement):\n",
        "    prompt = f\"summarize: {statement}\"\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    output = model.generate(**inputs, max_new_tokens=8)\n",
        "    label = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
        "    print(\"Statement:\", statement)\n",
        "    print(\"Predicted label:\", label)\n",
        "\n",
        "# Example usage\n",
        "run_prediction(\"Python is the fastest programming language available.\")\n",
        "run_prediction(\"The book 'Open Source AI' explores Hugging Face and T5 models.\")\n"
      ],
      "metadata": {
        "id": "HkOzlnOuByGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calling the Hosted Model via Hugging Face Spaces\n",
        "This cell demonstrates how to invoke the hosted model remotely using the `gradio_client` library. The model runs inside a Hugging Face Space and exposes a simple API endpoint. This setup lets you test statements from any Python environment without managing infrastructure, making it ideal for lightweight integrations and shared access.\n",
        "\n",
        "Note: Make sure to run the pip install gradio_client cell below before using this code."
      ],
      "metadata": {
        "id": "1e-XusD0XNM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install the client (only needs to be run once per Colab session)\n",
        "!pip install -q gradio_client"
      ],
      "metadata": {
        "id": "u7dl6ROemxgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "# Initialize the client using your Space ID (username/space-name)\n",
        "client = Client(\"gcuomo/open-source-ai-liar\")\n",
        "\n",
        "# Call the remote predict function with a statement\n",
        "statement = \"The book 'Open Source AI' explores Hugging Face and T5 models.\"\n",
        "result = client.predict(statement)\n",
        "\n",
        "# Display both input and result\n",
        "print(\"Statement:\", statement)\n",
        "print(\"Predicted label:\", result)\n"
      ],
      "metadata": {
        "id": "7CrTWOKzXR6Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}