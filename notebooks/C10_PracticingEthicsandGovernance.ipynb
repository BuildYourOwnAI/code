{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 10 Practicing Ethics and Governance\n",
        "When AI systems are well designed and governed, they can improve access to services, increase efficiency, and support better decision-making. When they are poorly designed or deployed without sufficient oversight, they can erode privacy, generate misinformation, amplify discrimination, and undermine trust. Chapter 10 introduces the foundations of practicing ethics and governance in AI."
      ],
      "metadata": {
        "id": "3wlxmughMhwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing 10-1 Using Great Expectations for Data Quality Checks\n",
        "This minimal example, which can be expanded into full pipelines, shows how data validation can become an automated, repeatable governance practice."
      ],
      "metadata": {
        "id": "G-k9fmTh-1B4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8mfKr5VMgpX"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------------------\n",
        "# Step 1: Install and import dependencies\n",
        "# --------------------------------------------------------------\n",
        "!pip install great_expectations pandas scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import great_expectations as gx\n",
        "from great_expectations.expectations import (\n",
        "    ExpectColumnValuesToNotBeNull,\n",
        "    ExpectColumnValuesToBeBetween,\n",
        "    ExpectTableRowCountToBeBetween,\n",
        ")\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Step 2. Load a tabular dataset into a DataFrame\n",
        "# --------------------------------------------------------------\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Step 3. Create a GX Data Context (in-memory)\n",
        "# --------------------------------------------------------------\n",
        "context = gx.get_context()\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Step 4. Add a pandas Data Source\n",
        "# --------------------------------------------------------------\n",
        "data_source = context.data_sources.add_pandas(\"breast_cancer_source\")\n",
        "\n",
        "# 2. Add a Data Asset for this DataFrame\n",
        "data_asset = data_source.add_dataframe_asset(name=\"breast_cancer_asset\")\n",
        "\n",
        "# 3. Add a Batch Definition that uses the whole DataFrame\n",
        "batch_def = data_asset.add_batch_definition_whole_dataframe(\"whole_dataframe\")\n",
        "\n",
        "# 4. Pass the in-memory DataFrame as batch parameters\n",
        "batch = batch_def.get_batch(batch_parameters={\"dataframe\": df})\n",
        "\n",
        "# Define expectations\n",
        "exp1 = ExpectColumnValuesToNotBeNull(column=\"mean radius\")\n",
        "exp2 = ExpectColumnValuesToBeBetween(\n",
        "    column=\"mean texture\",\n",
        "    min_value=0,\n",
        "    max_value=100,\n",
        ")\n",
        "exp3 = ExpectTableRowCountToBeBetween(\n",
        "    min_value=100,\n",
        "    max_value=10000,\n",
        ")\n",
        "\n",
        "# Validate expectations against the Batch\n",
        "res1 = batch.validate(exp1)\n",
        "res2 = batch.validate(exp2)\n",
        "res3 = batch.validate(exp3)\n",
        "\n",
        "print(\"Null check on 'mean radius':\", res1.success)\n",
        "print(\"Range check on 'mean texture':\", res2.success)\n",
        "print(\"Row count check:\", res3.success)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing 10-2 Tracking Model Training and Metrics with MLflow\n",
        "Experiment tracking is a core governance function. Without systematic tracking, reproducibility degrades, audit trails disappear, and model lineage becomes ambiguous. Governance requires that teams be able to answer basic but critical questions: Which data was used? Which parameters were chosen? Which metrics justified deployment? What changed between versions?\n",
        "Tools such as MLflow allow teams to log parameters, metrics, and artifacts in a structured and queryable format.\n",
        "\n",
        "This example demonstrates how a model training run can be recorded as a governed experiment.\n"
      ],
      "metadata": {
        "id": "eb4p9h3voIld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------\n",
        "# Step 1: Install and import dependencies\n",
        "# --------------------------------------------------------------\n",
        "!pip install mlflow\n",
        "\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Step 2: Load dataset and split into train/test sets\n",
        "# --------------------------------------------------------------\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.data,\n",
        "    data.target,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=data.target\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Step 3: Start an MLflow tracking run\n",
        "# --------------------------------------------------------------\n",
        "with mlflow.start_run(run_name=\"rf_governance_example\"):\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # Step 4: Define and log hyperparameters\n",
        "    # ----------------------------------------------------------\n",
        "    n_estimators = 100\n",
        "    max_depth = 5\n",
        "\n",
        "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
        "    mlflow.log_param(\"max_depth\", max_depth)\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # Step 5: Train the model\n",
        "    # ----------------------------------------------------------\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # Step 6: Evaluate and log performance metrics\n",
        "    # ----------------------------------------------------------\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    auc = roc_auc_score(y_test, y_prob)\n",
        "    mlflow.log_metric(\"roc_auc\", auc)\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # Step 7: Log the trained model artifact\n",
        "    # ----------------------------------------------------------\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=model,\n",
        "        artifact_path=\"model\",\n",
        "        input_example=X_train[:5]\n",
        "    )\n",
        "\n",
        "    print(\"Logged run with ROC AUC:\", auc)\n"
      ],
      "metadata": {
        "id": "Pn3IWRwooSG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing 10-3\n",
        "The Fairlearn library provides tools for measuring group-wise performance and making disparities visible. This example trains a simple classifier and then computes accuracy and selection rate separately for two groups defined by a protected attribute such as gender, region, or another sensitive feature.\n"
      ],
      "metadata": {
        "id": "sWAFs7DfoLrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# Step 1: Install dependencies\n",
        "# ---------------------------------------------------------\n",
        "!pip install fairlearn scikit-learn\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 2: Import libraries\n",
        "# ---------------------------------------------------------\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from fairlearn.metrics import MetricFrame, selection_rate\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 3: Create a synthetic dataset with a binary label\n",
        "# ---------------------------------------------------------\n",
        "X, y = make_classification(\n",
        "    n_samples=2000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 4: Create a synthetic \"group\" attribute\n",
        "#         (a stand-in for a protected attribute)\n",
        "# ---------------------------------------------------------\n",
        "rng = np.random.default_rng(42)\n",
        "group_attr = rng.integers(0, 2, size=y.shape[0])  # values 0 or 1\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 5: Split into train/test, keeping labels stratified\n",
        "# ---------------------------------------------------------\n",
        "X_train, X_test, y_train, y_test, g_train, g_test = train_test_split(\n",
        "    X, y, group_attr, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "# ---------------------------------------------------------\n",
        "# Step 6: Train a baseline classifier and evaluate\n",
        "#         group-wise metrics\n",
        "# ---------------------------------------------------------\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "metric_frame = MetricFrame(\n",
        "    metrics={\n",
        "        \"accuracy\": accuracy_score,\n",
        "        \"selection_rate\": selection_rate,\n",
        "    },\n",
        "    y_true=y_test,\n",
        "    y_pred=y_pred,\n",
        "    sensitive_features=g_test,\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 7: Print results\n",
        "# ---------------------------------------------------------\n",
        "print(\"Overall accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nAccuracy by group:\")\n",
        "print(metric_frame.by_group[\"accuracy\"])\n",
        "\n",
        "print(\"\\nSelection rate by group:\")\n",
        "print(metric_frame.by_group[\"selection_rate\"])"
      ],
      "metadata": {
        "id": "AN0RyYrVoOgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listing 10-4: Using SHAP to Identify Influential Features\n",
        "This code trains a tree-based model and computes SHAP values on a held-out dataset. Instead of displaying the full distribution of SHAP values, which can be visually dense, the code aggregates them into mean absolute contribution scores per feature. This provides a compact summary of which inputs most strongly influence predictions overall.\n"
      ],
      "metadata": {
        "id": "mJDLzEUuqMJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# Step 1: Install dependencies\n",
        "# ---------------------------------------------------------\n",
        "!pip install shap scikit-learn pandas matplotlib\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 2: Import required libraries\n",
        "# ---------------------------------------------------------\n",
        "import shap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 3: Load dataset and create feature matrix\n",
        "# ---------------------------------------------------------\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 4: Split into training and test sets\n",
        "# ---------------------------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 5: Train a tree-based classifier\n",
        "# ---------------------------------------------------------\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=None,\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 6: Create SHAP explainer and compute SHAP values\n",
        "# ---------------------------------------------------------\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 7: Select SHAP values for positive class\n",
        "# ---------------------------------------------------------\n",
        "if isinstance(shap_values, list):\n",
        "    shap_values_class_1 = shap_values[1]\n",
        "else:\n",
        "    shap_values_class_1 = shap_values\n",
        "\n",
        "shap_values_class_1 = np.array(shap_values_class_1)\n",
        "if shap_values_class_1.ndim > 2:\n",
        "    shap_values_class_1 = shap_values_class_1.mean(axis=-1)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 8: Compute mean absolute SHAP value per feature\n",
        "# ---------------------------------------------------------\n",
        "mean_abs_shap = np.mean(np.abs(shap_values_class_1), axis=0)\n",
        "mean_abs_shap = np.asarray(mean_abs_shap, dtype=float).ravel()\n",
        "\n",
        "feature_names = np.array(X_test.columns)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 9: Rank features by importance\n",
        "# ---------------------------------------------------------\n",
        "order = np.argsort(mean_abs_shap)[::-1]\n",
        "top_k = min(10, len(mean_abs_shap))\n",
        "top_idx = order[:top_k]\n",
        "\n",
        "top_features = feature_names[top_idx]\n",
        "top_importances = mean_abs_shap[top_idx]\n",
        "\n",
        "print(\"Top features by mean absolute SHAP value:\")\n",
        "for name, val in zip(top_features, top_importances):\n",
        "    print(f\"{name}: {val:.4f}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Step 10: Visualize feature importance\n",
        "# ---------------------------------------------------------\n",
        "plt.figure(figsize=(8, 5))\n",
        "positions = np.arange(len(top_features))\n",
        "plt.barh(positions, top_importances)\n",
        "plt.yticks(positions, top_features)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel(\"mean(|SHAP value|)\")\n",
        "plt.title(\"Top features by mean absolute SHAP value\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XDJVRuF1qBbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Listing 10-5 Opacus Differential Privacy Example\n",
        "This example illustrates how differential privacy can be incorporated directly into model training workflows."
      ],
      "metadata": {
        "id": "NCrf_hJiD1Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------\n",
        "# Step 1: Install and import dependencies\n",
        "# --------------------------------------------------------------\n",
        "!pip install opacus\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from opacus import PrivacyEngine\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Step 2: Load and prepare a small tabular dataset\n",
        "# --------------------------------------------------------------\n",
        "data = load_breast_cancer()\n",
        "X = torch.tensor(data.data, dtype=torch.float32)\n",
        "y = torch.tensor(data.target, dtype=torch.long)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "train_ds = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Step 3: Define a simple classifier model\n",
        "# --------------------------------------------------------------\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(X_train.shape[1], 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 2)\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Step 4: Attach the PrivacyEngine with a target privacy budget\n",
        "# --------------------------------------------------------------\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    epochs=3,\n",
        "    target_epsilon=5.0,\n",
        "    target_delta=1e-5,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Step 5: Train the model under differential privacy\n",
        "# --------------------------------------------------------------\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    for xb, yb in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Step 6: Retrieve and report the achieved privacy budget\n",
        "# --------------------------------------------------------------\n",
        "epsilon = privacy_engine.get_epsilon(delta=1e-5)\n",
        "print(f\"Training finished with ε = {epsilon:.2f}, δ = 1e-5\")"
      ],
      "metadata": {
        "id": "kGtUmzXFEF40"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}