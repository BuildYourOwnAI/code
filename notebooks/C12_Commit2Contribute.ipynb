{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 12: Commit to Contribute\n",
        "\n",
        "This notebook powers the final chapter by automating the extraction, curation, and visualization of open-source AI projects mentioned throughout the book. Using CrewAI agents, it builds a structured glossary and an interactive reference architecture. It supports reproducibility, highlights licensing and contribution pathways, and shows how automation can document and sustain the open-source ecosystem.\n"
      ],
      "metadata": {
        "id": "pkseHwQe__hy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up"
      ],
      "metadata": {
        "id": "wgY70RnP_wKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U --quiet 'crewai[tools]' aisuite databricks-sdk"
      ],
      "metadata": {
        "id": "ohHK7cVJ_8p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install markdown2 python-docx"
      ],
      "metadata": {
        "id": "U2sVXQo9nyfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qHxiLO5_Qc4"
      },
      "outputs": [],
      "source": [
        "# Constants and API Key Configuration\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# === Load API keys securely from Google Colab Secrets ===\n",
        "def load_api_keys():\n",
        "    keys = {\n",
        "        \"HF_TOKEN\": userdata.get(\"HF_TOKEN\"),\n",
        "        \"SERPER_API_KEY\": userdata.get(\"SERPER_API_KEY\"),\n",
        "        \"OPENAI_API_KEY\": userdata.get(\"OPENAI_API_KEY\"),\n",
        "    }\n",
        "    for key, value in keys.items():\n",
        "        if not value:\n",
        "            raise ValueError(f\"‚ùå Missing {key}. Please set this API key in Colab secrets.\")\n",
        "        os.environ[key] = value\n",
        "    print(\"‚úÖ All API keys loaded and configured successfully.\")\n",
        "\n",
        "# Execute API key loading upon running this cell\n",
        "load_api_keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 12-1: Agents that Extract and Merge AI Glossary Entries\n",
        "\n",
        "This listing uses CrewAI agents to scan chapter files, extract open-source project mentions, and generate structured JSON files per chapter. A second script consolidates these entries, merges duplicates, and exports a clean CSV with hyperlinks. Together, they automate building a glossary from long-form content using agentic workflows.\n"
      ],
      "metadata": {
        "id": "CHmzF12hAzLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VERSION 2"
      ],
      "metadata": {
        "id": "m2BJB59MZnbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Imports ===\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from crewai_tools import FileReadTool, DirectoryReadTool, FileWriterTool\n",
        "\n",
        "# === Constants ===\n",
        "DEFAULT_MODEL = \"gpt-4o-mini\"\n",
        "CHAPTER_DIR = \"/content/chapters\"\n",
        "GLOSSARY_MD_PATH = \"open_source_glossary.md\"\n",
        "\n",
        "# === Tools ===\n",
        "file_tool = FileReadTool()\n",
        "dir_tool = DirectoryReadTool(directory=CHAPTER_DIR)\n",
        "file_writer_tool = FileWriterTool()\n",
        "\n",
        "# === Agent Definitions ===\n",
        "\n",
        "directory_enumerator = Agent(\n",
        "    role=\"Directory Enumerator\",\n",
        "    goal=\"List all chapter files inside the provided directory.\",\n",
        "    backstory=\"You are responsible for returning a complete list of text files representing chapters.\",\n",
        "    tools=[dir_tool],\n",
        "    llm=DEFAULT_MODEL,\n",
        ")\n",
        "\n",
        "chapter_reader = Agent(\n",
        "    role=\"Chapter Scanner\",\n",
        "    goal=\"Scan each chapter file (don't skip any) and extract structured open-source project information.\",\n",
        "    backstory=\"You read chapter files from a book on open-source AI and extract complete project data, saving per-chapter output.\",\n",
        "    tools=[file_tool, file_writer_tool],\n",
        "    llm=DEFAULT_MODEL,\n",
        ")\n",
        "\n",
        "glossary_writer = Agent(\n",
        "    role=\"Glossary Assembler\",\n",
        "    goal=\"Generate a clean Markdown glossary of all open-source projects mentioned in the book.\",\n",
        "    backstory=\"You transform structured project data into a readable glossary for publication.\",\n",
        "    tools=[file_tool, dir_tool],\n",
        "    llm=DEFAULT_MODEL,\n",
        ")\n",
        "\n",
        "# === Task Definitions ===\n",
        "\n",
        "list_chapter_files_task = Task(\n",
        "    description=(\n",
        "        \"List all chapter files in the given directory. These are text-based files, \"\n",
        "        \"each representing a chapter from a book on open-source AI. \"\n",
        "        \"Return a list of file paths (or names) to be passed to the Chapter Scanner.\"\n",
        "    ),\n",
        "    expected_output=\"List of file paths or filenames for all chapter files.\",\n",
        "    agent=directory_enumerator,\n",
        ")\n",
        "\n",
        "extract_projects_task = Task(\n",
        "    description=(\n",
        "        \"You will be given a list of file paths. \\n\"\n",
        "        \"You must process every file, do not skip any!\\n\"\n",
        "        \"Each file is a chapter from a book on open-source AI.\\n\\n\"\n",
        "        \"For each file:\\n\"\n",
        "        \"- Read the file using the file reading tool.\\n\"\n",
        "        \"- Extract the chapter number and title if present (format: 'Chapter X: Title').\\n\"\n",
        "        \"- Identify all open-source projects, frameworks and tools mentioned.\\n\"\n",
        "        \"- For each project, extract or infer the following:\\n\"\n",
        "        \"  - Project name\\n\"\n",
        "        \"  - Creator (person or organization)\\n\"\n",
        "        \"  - Description (1 sentence)\\n\"\n",
        "        \"  - Year (of inception)\\n\"\n",
        "        \"  - URL (homepage or repository)\\n\"\n",
        "        #\"- If you are not certain write'N/A'.\\n\\n\"\n",
        "        \"After analyzing each file, write the output as a JSON file using the file writing tool.\\n\"\n",
        "        \"Use the same name as the input file, but change the extension to '.json'.\\n\"\n",
        "        \"Write the .json file to the same directory as the original chapter file.\"\n",
        "    ),\n",
        "    expected_output=(\n",
        "        \"One JSON file saved per chapter, in the same folder as the chapter file. \"\n",
        "        \"Each file contains a structured list of project dictionaries.\"\n",
        "    ),\n",
        "    agent=chapter_reader,\n",
        "    context=[list_chapter_files_task],\n",
        ")\n",
        "\n",
        "generate_glossary_task = Task(\n",
        "    description=(\n",
        "        \"Read all .json files in the same directory as the chapter files. \"\n",
        "        \"These files contain structured lists of open-source projects extracted per chapter.\\n\"\n",
        "        \"- Merge all entries into one unified list.\\n\"\n",
        "        \"Write a clean Markdown-formatted glossary. Each project should include:\\n\"\n",
        "        \"- Project name\\n\"\n",
        "        \"- Creator\\n\"\n",
        "        \"- Description\\n\"\n",
        "        \"- Estimated year of inception\\n\"\n",
        "        \"- List of chapters it appears in\\n\"\n",
        "        \"- Project URL\\n\"\n",
        "        \"Ensure consistent formatting and readability.\"\n",
        "    ),\n",
        "    expected_output=\"Markdown glossary combining all chapter-level extractions.\",\n",
        "    agent=glossary_writer,\n",
        "    context=[extract_projects_task],\n",
        "    output_file=GLOSSARY_MD_PATH\n",
        ")\n",
        "\n",
        "# === Crew Definition ===\n",
        "\n",
        "glossary_crew = Crew(\n",
        "    agents=[directory_enumerator, chapter_reader, glossary_writer],\n",
        "    tasks=[list_chapter_files_task, extract_projects_task, generate_glossary_task],\n",
        "    process=Process.sequential,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# === Run Program ===\n",
        "\n",
        "def run_open_source_glossary():\n",
        "    print(f\"\\nüìö Starting glossary generation from: {CHAPTER_DIR}\")\n",
        "    start = time.time()\n",
        "    glossary_crew.kickoff()\n",
        "    end = time.time()\n",
        "    print(\"\\n‚úÖ Open-source glossary complete.\")\n",
        "    print(f\"‚è±Ô∏è Duration: {end - start:.2f} seconds.\")\n",
        "    print(f\"üìÑ Glossary written to: {GLOSSARY_MD_PATH}\")\n",
        "    print(f\"üìÅ Per-chapter output saved in: {CHAPTER_DIR} (as .json files)\")\n",
        "\n",
        "# === Entry Point ===\n",
        "if __name__ == \"__main__\":\n",
        "    run_open_source_glossary()\n"
      ],
      "metadata": {
        "id": "GqmW7xpAz4Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 2: Save Chapter JSONs to a CSV"
      ],
      "metadata": {
        "id": "m8LevySHu2c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "\n",
        "CHAPTER_DIR = \"/content/chapters\"\n",
        "OUTPUT_CSV = \"open_source_glossary.csv\"\n",
        "\n",
        "def normalize_project_name(name):\n",
        "    return name.strip().lower()\n",
        "\n",
        "def merge_project_entries(entries):\n",
        "    merged = {}\n",
        "    for entry in entries:\n",
        "        name_key = normalize_project_name(entry.get(\"name\", \"\"))\n",
        "        if not name_key:\n",
        "            continue\n",
        "\n",
        "        if name_key not in merged:\n",
        "            merged[name_key] = entry\n",
        "        else:\n",
        "            existing = merged[name_key]\n",
        "            # Merge chapter lists\n",
        "            existing_chapters = set(existing.get(\"chapter_list\", []))\n",
        "            new_chapters = set(entry.get(\"chapter_list\", []))\n",
        "            existing[\"chapter_list\"] = sorted(existing_chapters.union(new_chapters))\n",
        "    return list(merged.values())\n",
        "\n",
        "def standardize_entry(raw, chapter_label):\n",
        "    # Try both naming styles\n",
        "    name = raw.get(\"project_name\") or raw.get(\"name\", \"N/A\")\n",
        "    creator = raw.get(\"creator\", \"N/A\")\n",
        "    description = raw.get(\"description\", \"N/A\")\n",
        "    year = raw.get(\"year\") or raw.get(\"year_inception\", \"N/A\")\n",
        "    url = raw.get(\"url\", \"N/A\")\n",
        "\n",
        "    return {\n",
        "        \"name\": name,\n",
        "        \"creator\": creator,\n",
        "        \"description\": description,\n",
        "        \"inception_year\": year,\n",
        "        \"project_url\": url,\n",
        "        \"chapter_list\": [chapter_label]\n",
        "    }\n",
        "\n",
        "def load_all_projects_from_json(directory):\n",
        "    all_entries = []\n",
        "    json_files = [f for f in os.listdir(directory) if f.endswith(\".json\")]\n",
        "\n",
        "    if not json_files:\n",
        "        print(f\"‚ö†Ô∏è No .json files found in: {directory}\")\n",
        "        return all_entries\n",
        "\n",
        "    for filename in json_files:\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        try:\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "                chapter_number = data.get(\"chapter\") or data.get(\"chapter_number\")\n",
        "                chapter_title = data.get(\"title\", \"Unknown Title\")\n",
        "                chapter_label = f\"Chapter {chapter_number}: {chapter_title}\"\n",
        "\n",
        "                for raw_proj in data.get(\"projects\", []):\n",
        "                    entry = standardize_entry(raw_proj, chapter_label)\n",
        "                    all_entries.append(entry)\n",
        "\n",
        "                print(f\"‚úÖ Loaded {len(data.get('projects', []))} projects from {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to read {filename}: {e}\")\n",
        "    return all_entries\n",
        "\n",
        "def write_csv(projects, output_path):\n",
        "    fieldnames = [\"name\", \"creator\", \"description\", \"inception_year\", \"chapter_list\", \"HYPERLINK\"]\n",
        "    with open(output_path, \"w\", newline='', encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for proj in projects:\n",
        "            url = proj.get(\"project_url\", \"\")\n",
        "            name = proj.get(\"name\", \"\")\n",
        "            hyperlink = f'=HYPERLINK(\"{url}\", \"{name}\")' if url and name else \"\"\n",
        "\n",
        "            writer.writerow({\n",
        "                \"name\": name,\n",
        "                \"creator\": proj.get(\"creator\", \"\"),\n",
        "                \"description\": proj.get(\"description\", \"\"),\n",
        "                \"inception_year\": proj.get(\"inception_year\", \"\"),\n",
        "                \"chapter_list\": \", \".join(proj.get(\"chapter_list\", [])),\n",
        "                \"HYPERLINK\": hyperlink\n",
        "            })\n",
        "    print(f\"üìÑ CSV with hyperlinks saved to: {output_path}\")\n",
        "\n",
        "def run_merge_and_export():\n",
        "    print(f\"\\nüì• Reading JSON files from: {CHAPTER_DIR}\")\n",
        "    all_entries = load_all_projects_from_json(CHAPTER_DIR)\n",
        "    print(f\"üì¶ Found {len(all_entries)} total project entries\")\n",
        "\n",
        "    merged_projects = merge_project_entries(all_entries)\n",
        "    print(f\"üîÅ Merged to {len(merged_projects)} unique project entries\")\n",
        "\n",
        "    write_csv(merged_projects, OUTPUT_CSV)\n",
        "\n",
        "# === Run It ===\n",
        "if __name__ == \"__main__\":\n",
        "    run_merge_and_export()\n"
      ],
      "metadata": {
        "id": "ddkL0o5O18YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Glossary Output"
      ],
      "metadata": {
        "id": "9Dl8dVnsqXld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glossary of Open-Source Projects\n",
        "\n",
        "## Chapter 1\n",
        "### Python\n",
        "- **Creator:** Guido van Rossum  \n",
        "- **Description:** Python is a high-level, interpreted programming language known for its readability and versatility, widely used for web development, data analysis, artificial intelligence, and more.  \n",
        "- **Estimated Year of Inception:** 1991  \n",
        "- **Project URL:** [python.org](https://www.python.org/)\n",
        "\n",
        "### NumPy\n",
        "- **Creator:** Travis Olliphant, et al.  \n",
        "- **Description:** NumPy is a fundamental package for scientific computing in Python that provides support for large multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.  \n",
        "- **Estimated Year of Inception:** 2006  \n",
        "- **Project URL:** [numpy.org](https://numpy.org/)\n",
        "\n",
        "### Pandas\n",
        "- **Creator:** Wes McKinney  \n",
        "- **Description:** Pandas is an open-source data analysis and manipulation library for Python, providing data structures and functions needed to work with structured data effectively.  \n",
        "- **Estimated Year of Inception:** 2008  \n",
        "- **Project URL:** [pandas.pydata.org](https://pandas.pydata.org/)\n",
        "\n",
        "### PyTorch\n",
        "- **Creator:** Facebook AI Research  \n",
        "- **Description:** PyTorch is an open-source machine learning library based on the Torch library, used for applications such as natural language processing and deep learning.  \n",
        "- **Estimated Year of Inception:** 2016  \n",
        "- **Project URL:** [pytorch.org](https://pytorch.org/)\n",
        "\n",
        "### Matplotlib\n",
        "- **Creator:** John D. Hunter  \n",
        "- **Description:** Matplotlib is a plotting library for Python and its numerical mathematics extension NumPy, allowing for the creation of static, animated, and interactive visualizations.  \n",
        "- **Estimated Year of Inception:** 2003  \n",
        "- **Project URL:** [matplotlib.org](https://matplotlib.org/)\n",
        "\n",
        "### Jupyter Notebooks\n",
        "- **Creator:** Project Jupyter  \n",
        "- **Description:** Jupyter Notebooks is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text.  \n",
        "- **Estimated Year of Inception:** 2014  \n",
        "- **Project URL:** [jupyter.org](https://jupyter.org/)\n",
        "\n",
        "### Google Colab\n",
        "- **Creator:** Google  \n",
        "- **Description:** Google Colaboratory, or Colab, is a free cloud service for Python that allows you to write and execute code in a web-based Jupyter environment, with easy access to GPUs.  \n",
        "- **Estimated Year of Inception:** 2017  \n",
        "- **Project URL:** [colab.research.google.com](https://colab.research.google.com/)\n",
        "\n",
        "### scikit-learn\n",
        "- **Creator:** David Cournapeau, et al.  \n",
        "- **Description:** Scikit-learn is a machine learning library for Python that features various classification, regression, and clustering algorithms, along with tools for model selection and evaluation.  \n",
        "- **Estimated Year of Inception:** 2007  \n",
        "- **Project URL:** [scikit-learn.org](https://scikit-learn.org/)\n",
        "\n",
        "## Chapter 2\n",
        "### Fairlearn\n",
        "- **Creator:** Microsoft  \n",
        "- **Description:** Fairlearn is an open-source Python library that helps in assessing and improving the fairness of machine learning models.  \n",
        "- **Estimated Year of Inception:** 2020  \n",
        "- **Project URL:** [fairlearn.org](https://fairlearn.org/)\n",
        "\n",
        "### Hugging Face\n",
        "- **Creator:** Hugging Face, Inc.  \n",
        "- **Description:** Hugging Face is a company known for its transformer models in NLP and provides an extensive library to easily implement and deploy machine learning models.  \n",
        "- **Estimated Year of Inception:** 2016  \n",
        "- **Project URL:** [huggingface.co](https://huggingface.co/)\n",
        "\n",
        "### Milvus\n",
        "- **Creator:** Zilliz  \n",
        "- **Description:** Milvus is an open-source vector database designed for managing embedding data, offering high-performance searching and analytics capabilities.  \n",
        "- **Estimated Year of Inception:** 2020  \n",
        "- **Project URL:** [milvus.io](https://milvus.io/)\n",
        "\n",
        "### FAISS\n",
        "- **Creator:** Facebook AI Research  \n",
        "- **Description:** FAISS is a library for efficient similarity search and clustering of dense vectors, providing algorithms that optimize searches for big datasets.  \n",
        "- **Estimated Year of Inception:** 2017  \n",
        "- **Project URL:** [faiss.ai](https://faiss.ai/)\n",
        "\n",
        "### Weaviate\n",
        "- **Creator:** SeMI Technologies  \n",
        "- **Description:** Weaviate is an open-source vector search engine that allows developers to build semantic search applications powered by machine learning.  \n",
        "- **Estimated Year of Inception:** 2019  \n",
        "- **Project URL:** [weaviate.io](https://weaviate.io/)\n",
        "\n",
        "### ChromaDB\n",
        "- **Creator:** Chroma Team  \n",
        "- **Description:** ChromaDB is an open-source embedding database that provides features for high-dimensional data and machine learning-based applications.  \n",
        "- **Estimated Year of Inception:** 2022  \n",
        "- **Project URL:** [chroma.ai](https://chroma.ai/)\n",
        "\n",
        "### Stable Diffusion\n",
        "- **Creator:** Stability AI  \n",
        "- **Description:** Stable Diffusion is a deep learning model designed for generating detailed images based on text prompts, known for its open-source nature and high quality.  \n",
        "- **Estimated Year of Inception:** 2022  \n",
        "- **Project URL:** [stability.ai](https://stability.ai/)\n",
        "\n",
        "### Gradient\n",
        "- **Creator:** Paperspace  \n",
        "- **Description:** Gradient is a platform that simplifies the process of building and deploying machine learning models, providing a suite of tools for developers and data scientists.  \n",
        "- **Estimated Year of Inception:** 2020  \n",
        "- **Project URL:** [paperspace.com/gradient](https://www.paperspace.com/gradient)\n",
        "\n",
        "### Giant-Machine\n",
        "- **Creator:** Giant Team  \n",
        "- **Description:** Giant-Machine is an advanced toolkit for building and deploying robust AI models with ease.  \n",
        "- **Estimated Year of Inception:** 2023  \n",
        "- **Project URL:** [giantmachine.ai](https://giantmachine.ai/)\n",
        "\n",
        "## Chapter 6\n",
        "### HumanLayer\n",
        "- **Creator:** Human Layer  \n",
        "- **Description:** HumanLayer is an open-source library that allows developers to build applications that leverage human input alongside AI systems.  \n",
        "- **Estimated Year of Inception:** 2022  \n",
        "- **Project URL:** [humanlayer.com](https://www.humanlayer.com/)\n",
        "\n",
        "### Gandalf\n",
        "- **Creator:** Gandalf Team  \n",
        "- **Description:** Gandalf is an open-source resource for building intuitive search applications, powered by AI.  \n",
        "- **Estimated Year of Inception:** 2021  \n",
        "- **Project URL:** [gandalf.dev](https://gandalf.dev/)\n",
        "\n",
        "## Chapter 11\n",
        "### TensorFlow\n",
        "- **Creator:** Google Brain Team  \n",
        "- **Description:** TensorFlow is an end-to-end open-source platform for machine learning, offering a comprehensive ecosystem for building ML applications.  \n",
        "- **Estimated Year of Inception:** 2015  \n",
        "- **Project URL:** [tensorflow.org](https://www.tensorflow.org/)\n",
        "\n",
        "### OpenAI Gym\n",
        "- **Creator:** OpenAI  \n",
        "- **Description:** OpenAI Gym is a toolkit for developing and comparing reinforcement learning (RL) algorithms, providing a standard API for environments.  \n",
        "- **Estimated Year of Inception:** 2016  \n",
        "- **Project URL:** [gym.openai.com](https://gym.openai.com/)\n",
        "\n",
        "### IBM Watson\n",
        "- **Creator:** IBM  \n",
        "- **Description:** IBM Watson is a suite of AI services and applications designed to help enterprises leverage advanced data analytics and cognitive services.  \n",
        "- **Estimated Year of Inception:** 2011  \n",
        "- **Project URL:** [ibm.com/watson](https://www.ibm.com/watson/)\n",
        "\n",
        "### IBM watsonx\n",
        "- **Creator:** IBM  \n",
        "- **Description:** IBM watsonx is IBM‚Äôs next-generation data, AI, and integration platform designed to empower organizations in their AI journey.  \n",
        "- **Estimated Year of Inception:** 2023  \n",
        "- **Project URL:** [ibm.com/watsonx](https://www.ibm.com/watsonx/)\n",
        "\n",
        "### Mistral\n",
        "- **Creator:** Mistral  \n",
        "- **Description:** Mistral is an open-source LLM that specializes in generating high-quality text based on prompts, with a focus on efficiency and accessibility.  \n",
        "- **Estimated Year of Inception:** 2023  \n",
        "- **Project URL:** [mistral.ai](https://mistral.ai/)\n",
        "\n",
        "### CrewAI\n",
        "- **Creator:** CrewAI  \n",
        "- **Description:** CrewAI is a collaborative AI platform focused on improving teams and organizations through machine learning tools and data analytics.  \n",
        "- **Estimated Year of Inception:** 2023  \n",
        "- **Project URL:** [crewai.com](https://crewai.com/)\n",
        "\n",
        "### LangChain\n",
        "- **Creator:** Harrison Chase, et al.  \n",
        "- **Description:** LangChain is a framework for developing applications powered by language models, emphasizing modular components for integrations, chains, and agents.  \n",
        "- **Estimated Year of Inception:** 2022  \n",
        "- **Project URL:** [langchain.readthedocs.io](https://langchain.readthedocs.io/)\n",
        "Connected to Python 3 Google Compute Engine backend\n"
      ],
      "metadata": {
        "id": "E3UQWIctu_gR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 12-2: Generate Open Source AI Architecture View\n",
        "\n",
        "This listing parses an Excel-based glossary of open-source AI projects and filters the top entries per category. It then groups and styles them into an interactive HTML reference architecture, color-coded by domain. The output offers a visual snapshot of the ecosystem, ideal for education, planning, or documentation.\n"
      ],
      "metadata": {
        "id": "kuFbvyuYf6_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "# === CATEGORY AND SUBCATEGORY DEFINITIONS ===\n",
        "category = {\n",
        "    1: {\"label\": \"Tools & Ecosystem\", \"color\": \"#546E7A\"},\n",
        "    2: {\"label\": \"Data Layer\", \"color\": \"#FFEB3B\"},\n",
        "    3: {\"label\": \"Model Development\", \"color\": \"#2196F3\"},\n",
        "    4: {\"label\": \"Agents & Operations\", \"color\": \"#ECEFF1\"},\n",
        "    5: {\"label\": \"Platform Services\", \"color\": \"#9E9E9E\"},\n",
        "    6: {\"label\": \"Governance & Oversight\", \"color\": \"#F44336\"}\n",
        "}\n",
        "\n",
        "subcategory = {\n",
        "    100: {\"label\": \"Developer Environments\", \"limit\": 2},\n",
        "    110: {\"label\": \"Model Hubs / Repos\", \"limit\": 2},\n",
        "    200: {\"label\": \"Data Basics\", \"limit\": 2},\n",
        "    210: {\"label\": \"Data Augmentation\", \"limit\": 2},\n",
        "    220: {\"label\": \"Data Synth\", \"limit\": 2},\n",
        "    300: {\"label\": \"Deep Learning Frameworks\", \"limit\": 3},\n",
        "    310: {\"label\": \"Classical ML\", \"limit\": 3},\n",
        "    320: {\"label\": \"Models\", \"limit\": 3},\n",
        "    400: {\"label\": \"Agent Frameworks\", \"limit\": 2},\n",
        "    410: {\"label\": \"Model Serve\", \"limit\": 2},\n",
        "    420: {\"label\": \"Flow Control\", \"limit\": 2},\n",
        "    500: {\"label\": \"Vector Stores\", \"limit\": 2},\n",
        "    510: {\"label\": \"Experiment Tracking\", \"limit\": 2},\n",
        "    520: {\"label\": \"Benchmarks\", \"limit\": 2},\n",
        "    600: {\"label\": \"Security & Guardrails\", \"limit\": 3},\n",
        "    610: {\"label\": \"Licensing & Compliance\", \"limit\": 3},\n",
        "    620: {\"label\": \"Ethics & Responsibility\", \"limit\": 3}\n",
        "}\n",
        "\n",
        "# === Constants ===\n",
        "BASE_URL = \"https://opensourceai-book.github.io/code/datasets/\"\n",
        "FILE_NAME = \"open_source_ai_glossary.xlsx\"\n",
        "EXCEL_URL = BASE_URL + FILE_NAME\n",
        "\n",
        "# === Load Excel file directly into memory ===\n",
        "response = requests.get(EXCEL_URL)\n",
        "excel_data = BytesIO(response.content)\n",
        "\n",
        "# === Load workbook from in-memory bytes ===\n",
        "wb = load_workbook(excel_data, data_only=False)\n",
        "ws = wb.active\n",
        "\n",
        "# === Parse headers and setup ===\n",
        "headers = [cell.value for cell in next(ws.iter_rows(min_row=1, max_row=1))]\n",
        "name_col_idx = headers.index(\"Name\")\n",
        "headers.insert(name_col_idx + 1, \"URL\")\n",
        "\n",
        "data = []\n",
        "for row in ws.iter_rows(min_row=2):\n",
        "    cell = row[name_col_idx]\n",
        "    value = str(cell.value)\n",
        "    # Handle Excel HYPERLINK formulas\n",
        "    hyperlink_match = re.match(r'=HYPERLINK\\(\"([^\"]+)\",\\s*\"([^\"]+)\"\\)', value)\n",
        "    if hyperlink_match:\n",
        "        url = hyperlink_match.group(1)\n",
        "        display = hyperlink_match.group(2)\n",
        "    else:\n",
        "        url = cell.hyperlink.target if cell.hyperlink else None\n",
        "        display = cell.value\n",
        "\n",
        "    row_values = [c.value for c in row]\n",
        "    row_values.insert(name_col_idx + 1, url)\n",
        "    row_values[name_col_idx] = display\n",
        "    data.append(row_values)\n",
        "\n",
        "df = pd.DataFrame(data, columns=headers)\n",
        "df = df.dropna(subset=[\"Name\"])\n",
        "\n",
        "# Normalize and clean\n",
        "df[\"Category\"] = pd.to_numeric(df[\"Category\"], errors=\"coerce\")\n",
        "df[\"Name_normalized\"] = df[\"Name\"].str.strip().str.lower()\n",
        "df = df.drop_duplicates(subset=[\"Name_normalized\", \"Chapter\"])\n",
        "\n",
        "# Count mentions across all chapters\n",
        "mention_counts = df[\"Name_normalized\"].value_counts().to_dict()\n",
        "df[\"Mention_Count\"] = df[\"Name_normalized\"].map(mention_counts)\n",
        "\n",
        "# Add category/subcategory labels\n",
        "df[\"Category_Label\"] = df[\"Category\"].apply(\n",
        "    lambda x: category.get(int(x // 100), {}).get(\"label\") if pd.notnull(x) else None\n",
        ")\n",
        "df[\"Category_Color\"] = df[\"Category\"].apply(\n",
        "    lambda x: category.get(int(x // 100), {}).get(\"color\") if pd.notnull(x) else None\n",
        ")\n",
        "df[\"Subcategory_Label\"] = df[\"Category\"].map(lambda x: subcategory.get(x, {}).get(\"label\"))\n",
        "df[\"Subcategory_Limit\"] = df[\"Category\"].map(lambda x: subcategory.get(x, {}).get(\"limit\"))\n",
        "\n",
        "# === PREVIEW BEFORE FILTERING ===\n",
        "display_cols = [\"Name\", \"URL\", \"Category\", \"Category_Label\", \"Category_Color\", \"Subcategory_Label\", \"Mention_Count\"]\n",
        "print(\"üü° Full dataset sample before filtering:\\n\")\n",
        "print(df[display_cols].head(100).to_string(index=False))\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# === FILTER TOP PROJECTS PER SUBCATEGORY (dedup by name within subcat) ===\n",
        "df_sorted = df.sort_values(by=[\"Category\", \"Mention_Count\"], ascending=[True, False])\n",
        "\n",
        "filtered_dfs = []\n",
        "for subcat_code, meta in subcategory.items():\n",
        "    limit = meta[\"limit\"]\n",
        "    sub_df = df_sorted[df_sorted[\"Category\"] == subcat_code]\n",
        "    sub_df = sub_df.drop_duplicates(subset=[\"Name\"])  # only keep one row per Name\n",
        "    sub_df = sub_df.head(limit)\n",
        "    filtered_dfs.append(sub_df)\n",
        "\n",
        "final_df = pd.concat(filtered_dfs, ignore_index=True)\n",
        "\n",
        "# === BEFORE FINAL DEDUP (raw filtered)\n",
        "print(\"üü¢ Filtered `final_df` before optional deduping by Name:\\n\")\n",
        "print(final_df[display_cols].head(50).to_string(index=False))\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# === OPTIONAL: DEDUP FINAL OUTPUT (ONLY KEEP ONE PER NAME)\n",
        "final_df = final_df.drop_duplicates(subset=[\"Name\"])\n",
        "print(\"‚úÖ `final_df` after deduping by Name:\\n\")\n",
        "print(final_df[display_cols].head(50).to_string(index=False))\n"
      ],
      "metadata": {
        "id": "YU_qbe7lgBsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 2: Generate HTML for Reference Architecture"
      ],
      "metadata": {
        "id": "Q9u-SyzmA_iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Generate HTML Reference Architecture from final_df ===\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# Group data by top-level category and subcategory\n",
        "grouped = defaultdict(lambda: defaultdict(list))\n",
        "for _, row in final_df.iterrows():\n",
        "    cat_label = row[\"Category_Label\"]\n",
        "    cat_color = row[\"Category_Color\"]\n",
        "    subcat_label = row[\"Subcategory_Label\"]\n",
        "    name = row[\"Name\"]\n",
        "    url = row[\"URL\"] or \"#\"  # fallback if URL is missing\n",
        "    grouped[(cat_label, cat_color)][subcat_label].append((name, url))\n",
        "\n",
        "# Start building HTML content\n",
        "html = [\n",
        "    \"<!DOCTYPE html>\",\n",
        "    \"<html lang='en'>\",\n",
        "    \"<head><meta charset='UTF-8'><title>Open Source AI Reference Architecture</title><style>\",\n",
        "    \"body { font-family: Arial, sans-serif; background: #fdfdfd; margin: 0; padding: .05em; }\",\n",
        "    \".layer-wrapper { display: flex; align-items: flex-start; gap: .5em; margin-bottom: .5em; }\",\n",
        "    \".layer-label { width: 140px; font-weight: bold; font-size: 1.1em; text-align: center; padding-top: 1em; }\",\n",
        "    \".layer-box { flex: 1; border-radius: 8px; padding: .75em; border: 2px solid #333; }\",\n",
        "    \".category-row { display: flex; flex-wrap: wrap; justify-content: center; gap: 2em; }\",\n",
        "    \".category { background: white; border-radius: 12px; padding: .25em; width: 240px; box-shadow: 0 3px 8px rgba(0,0,0,0.15); display: flex; flex-direction: column; align-items: center; }\",\n",
        "    \".category-title { font-weight: bold; font-size: 1em; margin-bottom: 0.5em; text-align: center; }\",\n",
        "    \".project-link { display: block; margin: 0.25em 0; text-align: center; font-size: .8em; text-decoration: none; color: #0056b3; }\",\n",
        "    \".project-link:hover { text-decoration: underline; }\",\n",
        "    \"</style></head><body>\"\n",
        "]\n",
        "\n",
        "# Populate HTML content\n",
        "for (layer_label, color), subcats in grouped.items():\n",
        "    html.append('<div class=\"layer-wrapper\">')\n",
        "    html.append(f'<div class=\"layer-label\">{layer_label}</div>')\n",
        "    html.append(f'<div class=\"layer-box\" style=\"background-color: {color}\">')\n",
        "    html.append('<div class=\"category-row\">')\n",
        "    for subcat, projects in subcats.items():\n",
        "        html.append('<div class=\"category\">')\n",
        "        html.append(f'<div class=\"category-title\">{subcat}</div>')\n",
        "        for name, url in projects:\n",
        "            html.append(f'<a class=\"project-link\" href=\"{url}\" target=\"_blank\">{name}</a>')\n",
        "        html.append('</div>')\n",
        "    html.append('</div></div></div>')\n",
        "\n",
        "html.extend([\"</body>\", \"</html>\"])\n",
        "\n",
        "# Write HTML file\n",
        "with open(\"open_source_ai_reference_architecture.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(html))\n",
        "\n",
        "print(\"‚úÖ Saved: open_source_ai_reference_architecture.html\")\n"
      ],
      "metadata": {
        "id": "rnPi0tCsevVK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}