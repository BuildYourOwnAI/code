{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkseHwQe__hy"
      },
      "source": [
        "## Chapter 12: Just Build It\n",
        "\n",
        "This notebook powers the final chapter by automating the extraction, curation, and visualization of open-source AI projects mentioned throughout the book. Using CrewAI agents, it builds a structured glossary and an interactive reference architecture. It supports reproducibility, highlights licensing and contribution pathways, and shows how automation can document and sustain the open-source ecosystem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgY70RnP_wKq"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohHK7cVJ_8p1"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U --quiet 'crewai[tools]' aisuite databricks-sdk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2sVXQo9nyfz"
      },
      "outputs": [],
      "source": [
        "%pip install markdown2 python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qHxiLO5_Qc4"
      },
      "outputs": [],
      "source": [
        "# Constants and API Key Configuration\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# === Load API keys securely from Google Colab Secrets ===\n",
        "def load_api_keys():\n",
        "    keys = {\n",
        "        \"HF_TOKEN\": userdata.get(\"HF_TOKEN\"),\n",
        "        \"SERPER_API_KEY\": userdata.get(\"SERPER_API_KEY\"),\n",
        "        \"OPENAI_API_KEY\": userdata.get(\"OPENAI_API_KEY\"),\n",
        "    }\n",
        "    for key, value in keys.items():\n",
        "        if not value:\n",
        "            raise ValueError(f\"‚ùå Missing {key}. Please set this API key in Colab secrets.\")\n",
        "        os.environ[key] = value\n",
        "    print(\"‚úÖ All API keys loaded and configured successfully.\")\n",
        "\n",
        "# Execute API key loading upon running this cell\n",
        "load_api_keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHmzF12hAzLH"
      },
      "source": [
        "### Listing 12-1: Agents that Extract and Merge AI Glossary Entries\n",
        "\n",
        "This listing uses CrewAI agents to scan chapter files, extract open-source project mentions, and generate structured JSON files per chapter. A second script consolidates these entries, merges duplicates, and exports a clean CSV with hyperlinks. Together, they automate building a glossary from long-form content using agentic workflows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqmW7xpAz4Qu"
      },
      "outputs": [],
      "source": [
        "# === Imports ===\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from crewai_tools import FileReadTool, DirectoryReadTool, FileWriterTool\n",
        "\n",
        "# === Constants ===\n",
        "DEFAULT_MODEL = \"gpt-4o-mini\"\n",
        "CHAPTER_DIR = \"/content/chapters\"\n",
        "GLOSSARY_MD_PATH = \"open_source_glossary.md\"\n",
        "\n",
        "# === Tools ===\n",
        "file_tool = FileReadTool()\n",
        "dir_tool = DirectoryReadTool(directory=CHAPTER_DIR)\n",
        "file_writer_tool = FileWriterTool()\n",
        "\n",
        "# === Agent Definitions ===\n",
        "\n",
        "directory_enumerator = Agent(\n",
        "    role=\"Directory Enumerator\",\n",
        "    goal=\"List all chapter files inside the provided directory.\",\n",
        "    backstory=\"You are responsible for returning a complete list of text files representing chapters.\",\n",
        "    tools=[dir_tool],\n",
        "    llm=DEFAULT_MODEL,\n",
        ")\n",
        "\n",
        "chapter_reader = Agent(\n",
        "    role=\"Chapter Scanner\",\n",
        "    goal=\"Scan each chapter file (don't skip any) and extract structured open-source project information.\",\n",
        "    backstory=\"You read chapter files from a book on open-source AI and extract complete project data, saving per-chapter output.\",\n",
        "    tools=[file_tool, file_writer_tool],\n",
        "    llm=DEFAULT_MODEL,\n",
        ")\n",
        "\n",
        "glossary_writer = Agent(\n",
        "    role=\"Glossary Assembler\",\n",
        "    goal=\"Generate a clean Markdown glossary of all open-source projects mentioned in the book.\",\n",
        "    backstory=\"You transform structured project data into a readable glossary for publication.\",\n",
        "    tools=[file_tool, dir_tool],\n",
        "    llm=DEFAULT_MODEL,\n",
        ")\n",
        "\n",
        "# === Task Definitions ===\n",
        "\n",
        "list_chapter_files_task = Task(\n",
        "    description=(\n",
        "        \"List all chapter files in the given directory. These are text-based files, \"\n",
        "        \"each representing a chapter from a book on open-source AI. \"\n",
        "        \"Return a list of file paths (or names) to be passed to the Chapter Scanner.\"\n",
        "    ),\n",
        "    expected_output=\"List of file paths or filenames for all chapter files.\",\n",
        "    agent=directory_enumerator,\n",
        ")\n",
        "\n",
        "extract_projects_task = Task(\n",
        "    description=(\n",
        "        \"You will be given a list of file paths. \\n\"\n",
        "        \"You must process every file, do not skip any!\\n\"\n",
        "        \"Each file is a chapter from a book on open-source AI.\\n\\n\"\n",
        "        \"For each file:\\n\"\n",
        "        \"- Read the file using the file reading tool.\\n\"\n",
        "        \"- Extract the chapter number and title if present (format: 'Chapter X: Title').\\n\"\n",
        "        \"- Identify all open-source projects, frameworks and tools mentioned.\\n\"\n",
        "        \"- For each project, extract or infer the following:\\n\"\n",
        "        \"  - Project name\\n\"\n",
        "        \"  - Creator (person or organization)\\n\"\n",
        "        \"  - Description (1 sentence)\\n\"\n",
        "        \"  - Year (of inception)\\n\"\n",
        "        \"  - URL (homepage or repository)\\n\"\n",
        "        #\"- If you are not certain write'N/A'.\\n\\n\"\n",
        "        \"After analyzing each file, write the output as a JSON file using the file writing tool.\\n\"\n",
        "        \"Use the same name as the input file, but change the extension to '.json'.\\n\"\n",
        "        \"Write the .json file to the same directory as the original chapter file.\"\n",
        "    ),\n",
        "    expected_output=(\n",
        "        \"One JSON file saved per chapter, in the same folder as the chapter file. \"\n",
        "        \"Each file contains a structured list of project dictionaries.\"\n",
        "    ),\n",
        "    agent=chapter_reader,\n",
        "    context=[list_chapter_files_task],\n",
        ")\n",
        "\n",
        "generate_glossary_task = Task(\n",
        "    description=(\n",
        "        \"Read all .json files in the same directory as the chapter files. \"\n",
        "        \"These files contain structured lists of open-source projects extracted per chapter.\\n\"\n",
        "        \"- Merge all entries into one unified list.\\n\"\n",
        "        \"Write a clean Markdown-formatted glossary. Each project should include:\\n\"\n",
        "        \"- Project name\\n\"\n",
        "        \"- Creator\\n\"\n",
        "        \"- Description\\n\"\n",
        "        \"- Estimated year of inception\\n\"\n",
        "        \"- List of chapters it appears in\\n\"\n",
        "        \"- Project URL\\n\"\n",
        "        \"Ensure consistent formatting and readability.\"\n",
        "    ),\n",
        "    expected_output=\"Markdown glossary combining all chapter-level extractions.\",\n",
        "    agent=glossary_writer,\n",
        "    context=[extract_projects_task],\n",
        "    output_file=GLOSSARY_MD_PATH\n",
        ")\n",
        "\n",
        "# === Crew Definition ===\n",
        "\n",
        "glossary_crew = Crew(\n",
        "    agents=[directory_enumerator, chapter_reader, glossary_writer],\n",
        "    tasks=[list_chapter_files_task, extract_projects_task, generate_glossary_task],\n",
        "    process=Process.sequential,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# === Run Program ===\n",
        "\n",
        "def run_open_source_glossary():\n",
        "    print(f\"\\nüìö Starting glossary generation from: {CHAPTER_DIR}\")\n",
        "    start = time.time()\n",
        "    glossary_crew.kickoff()\n",
        "    end = time.time()\n",
        "    print(\"\\n‚úÖ Open-source glossary complete.\")\n",
        "    print(f\"‚è±Ô∏è Duration: {end - start:.2f} seconds.\")\n",
        "    print(f\"üìÑ Glossary written to: {GLOSSARY_MD_PATH}\")\n",
        "    print(f\"üìÅ Per-chapter output saved in: {CHAPTER_DIR} (as .json files)\")\n",
        "\n",
        "# === Entry Point ===\n",
        "if __name__ == \"__main__\":\n",
        "    run_open_source_glossary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8LevySHu2c2"
      },
      "source": [
        "#### Part 2: Save Chapter JSONs to a CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddkL0o5O18YE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "\n",
        "CHAPTER_DIR = \"/content/chapters\"\n",
        "OUTPUT_CSV = \"open_source_glossary.csv\"\n",
        "\n",
        "def normalize_project_name(name):\n",
        "    return name.strip().lower()\n",
        "\n",
        "def merge_project_entries(entries):\n",
        "    merged = {}\n",
        "    for entry in entries:\n",
        "        name_key = normalize_project_name(entry.get(\"name\", \"\"))\n",
        "        if not name_key:\n",
        "            continue\n",
        "\n",
        "        if name_key not in merged:\n",
        "            merged[name_key] = entry\n",
        "        else:\n",
        "            existing = merged[name_key]\n",
        "            # Merge chapter lists\n",
        "            existing_chapters = set(existing.get(\"chapter_list\", []))\n",
        "            new_chapters = set(entry.get(\"chapter_list\", []))\n",
        "            existing[\"chapter_list\"] = sorted(existing_chapters.union(new_chapters))\n",
        "    return list(merged.values())\n",
        "\n",
        "def standardize_entry(raw, chapter_label):\n",
        "    # Try both naming styles\n",
        "    name = raw.get(\"project_name\") or raw.get(\"name\", \"N/A\")\n",
        "    creator = raw.get(\"creator\", \"N/A\")\n",
        "    description = raw.get(\"description\", \"N/A\")\n",
        "    year = raw.get(\"year\") or raw.get(\"year_inception\", \"N/A\")\n",
        "    url = raw.get(\"url\", \"N/A\")\n",
        "\n",
        "    return {\n",
        "        \"name\": name,\n",
        "        \"creator\": creator,\n",
        "        \"description\": description,\n",
        "        \"inception_year\": year,\n",
        "        \"project_url\": url,\n",
        "        \"chapter_list\": [chapter_label]\n",
        "    }\n",
        "\n",
        "def load_all_projects_from_json(directory):\n",
        "    all_entries = []\n",
        "    json_files = [f for f in os.listdir(directory) if f.endswith(\".json\")]\n",
        "\n",
        "    if not json_files:\n",
        "        print(f\"‚ö†Ô∏è No .json files found in: {directory}\")\n",
        "        return all_entries\n",
        "\n",
        "    for filename in json_files:\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        try:\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "                chapter_number = data.get(\"chapter\") or data.get(\"chapter_number\")\n",
        "                chapter_title = data.get(\"title\", \"Unknown Title\")\n",
        "                chapter_label = f\"Chapter {chapter_number}: {chapter_title}\"\n",
        "\n",
        "                for raw_proj in data.get(\"projects\", []):\n",
        "                    entry = standardize_entry(raw_proj, chapter_label)\n",
        "                    all_entries.append(entry)\n",
        "\n",
        "                print(f\"‚úÖ Loaded {len(data.get('projects', []))} projects from {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to read {filename}: {e}\")\n",
        "    return all_entries\n",
        "\n",
        "def write_csv(projects, output_path):\n",
        "    fieldnames = [\"name\", \"creator\", \"description\", \"inception_year\", \"chapter_list\", \"HYPERLINK\"]\n",
        "    with open(output_path, \"w\", newline='', encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for proj in projects:\n",
        "            url = proj.get(\"project_url\", \"\")\n",
        "            name = proj.get(\"name\", \"\")\n",
        "            hyperlink = f'=HYPERLINK(\"{url}\", \"{name}\")' if url and name else \"\"\n",
        "\n",
        "            writer.writerow({\n",
        "                \"name\": name,\n",
        "                \"creator\": proj.get(\"creator\", \"\"),\n",
        "                \"description\": proj.get(\"description\", \"\"),\n",
        "                \"inception_year\": proj.get(\"inception_year\", \"\"),\n",
        "                \"chapter_list\": \", \".join(proj.get(\"chapter_list\", [])),\n",
        "                \"HYPERLINK\": hyperlink\n",
        "            })\n",
        "    print(f\"üìÑ CSV with hyperlinks saved to: {output_path}\")\n",
        "\n",
        "def run_merge_and_export():\n",
        "    print(f\"\\nüì• Reading JSON files from: {CHAPTER_DIR}\")\n",
        "    all_entries = load_all_projects_from_json(CHAPTER_DIR)\n",
        "    print(f\"üì¶ Found {len(all_entries)} total project entries\")\n",
        "\n",
        "    merged_projects = merge_project_entries(all_entries)\n",
        "    print(f\"üîÅ Merged to {len(merged_projects)} unique project entries\")\n",
        "\n",
        "    write_csv(merged_projects, OUTPUT_CSV)\n",
        "\n",
        "# === Run It ===\n",
        "if __name__ == \"__main__\":\n",
        "    run_merge_and_export()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Dl8dVnsqXld"
      },
      "source": [
        "### Sample Glossary Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3UQWIctu_gR"
      },
      "source": [
        "# Glossary of Open-Source Projects\n",
        "\n",
        "## Chapter 1\n",
        "### Python\n",
        "- **Creator:** Guido van Rossum  \n",
        "- **Description:** Python is a high-level, interpreted programming language known for its readability and versatility, widely used for web development, data analysis, artificial intelligence, and more.  \n",
        "- **Estimated Year of Inception:** 1991  \n",
        "- **Project URL:** [python.org](https://www.python.org/)\n",
        "\n",
        "### NumPy\n",
        "- **Creator:** Travis Olliphant, et al.  \n",
        "- **Description:** NumPy is a fundamental package for scientific computing in Python that provides support for large multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.  \n",
        "- **Estimated Year of Inception:** 2006  \n",
        "- **Project URL:** [numpy.org](https://numpy.org/)\n",
        "\n",
        "### Pandas\n",
        "- **Creator:** Wes McKinney  \n",
        "- **Description:** Pandas is an open-source data analysis and manipulation library for Python, providing data structures and functions needed to work with structured data effectively.  \n",
        "- **Estimated Year of Inception:** 2008  \n",
        "- **Project URL:** [pandas.pydata.org](https://pandas.pydata.org/)\n",
        "\n",
        "### PyTorch\n",
        "- **Creator:** Facebook AI Research  \n",
        "- **Description:** PyTorch is an open-source machine learning library based on the Torch library, used for applications such as natural language processing and deep learning.  \n",
        "- **Estimated Year of Inception:** 2016  \n",
        "- **Project URL:** [pytorch.org](https://pytorch.org/)\n",
        "\n",
        "### Matplotlib\n",
        "- **Creator:** John D. Hunter  \n",
        "- **Description:** Matplotlib is a plotting library for Python and its numerical mathematics extension NumPy, allowing for the creation of static, animated, and interactive visualizations.  \n",
        "- **Estimated Year of Inception:** 2003  \n",
        "- **Project URL:** [matplotlib.org](https://matplotlib.org/)\n",
        "\n",
        "### Jupyter Notebooks\n",
        "- **Creator:** Project Jupyter  \n",
        "- **Description:** Jupyter Notebooks is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text.  \n",
        "- **Estimated Year of Inception:** 2014  \n",
        "- **Project URL:** [jupyter.org](https://jupyter.org/)\n",
        "\n",
        "### Google Colab\n",
        "- **Creator:** Google  \n",
        "- **Description:** Google Colaboratory, or Colab, is a free cloud service for Python that allows you to write and execute code in a web-based Jupyter environment, with easy access to GPUs.  \n",
        "- **Estimated Year of Inception:** 2017  \n",
        "- **Project URL:** [colab.research.google.com](https://colab.research.google.com/)\n",
        "\n",
        "### scikit-learn\n",
        "- **Creator:** David Cournapeau, et al.  \n",
        "- **Description:** Scikit-learn is a machine learning library for Python that features various classification, regression, and clustering algorithms, along with tools for model selection and evaluation.  \n",
        "- **Estimated Year of Inception:** 2007  \n",
        "- **Project URL:** [scikit-learn.org](https://scikit-learn.org/)\n",
        "\n",
        "## Chapter 2\n",
        "### Fairlearn\n",
        "- **Creator:** Microsoft  \n",
        "- **Description:** Fairlearn is an open-source Python library that helps in assessing and improving the fairness of machine learning models.  \n",
        "- **Estimated Year of Inception:** 2020  \n",
        "- **Project URL:** [fairlearn.org](https://fairlearn.org/)\n",
        "\n",
        "### Hugging Face\n",
        "- **Creator:** Hugging Face, Inc.  \n",
        "- **Description:** Hugging Face is a company known for its transformer models in NLP and provides an extensive library to easily implement and deploy machine learning models.  \n",
        "- **Estimated Year of Inception:** 2016  \n",
        "- **Project URL:** [huggingface.co](https://huggingface.co/)\n",
        "\n",
        "### Milvus\n",
        "- **Creator:** Zilliz  \n",
        "- **Description:** Milvus is an open-source vector database designed for managing embedding data, offering high-performance searching and analytics capabilities.  \n",
        "- **Estimated Year of Inception:** 2020  \n",
        "- **Project URL:** [milvus.io](https://milvus.io/)\n",
        "\n",
        "### FAISS\n",
        "- **Creator:** Facebook AI Research  \n",
        "- **Description:** FAISS is a library for efficient similarity search and clustering of dense vectors, providing algorithms that optimize searches for big datasets.  \n",
        "- **Estimated Year of Inception:** 2017  \n",
        "- **Project URL:** [faiss.ai](https://faiss.ai/)\n",
        "\n",
        "### Weaviate\n",
        "- **Creator:** SeMI Technologies  \n",
        "- **Description:** Weaviate is an open-source vector search engine that allows developers to build semantic search applications powered by machine learning.  \n",
        "- **Estimated Year of Inception:** 2019  \n",
        "- **Project URL:** [weaviate.io](https://weaviate.io/)\n",
        "\n",
        "### ChromaDB\n",
        "- **Creator:** Chroma Team  \n",
        "- **Description:** ChromaDB is an open-source embedding database that provides features for high-dimensional data and machine learning-based applications.  \n",
        "- **Estimated Year of Inception:** 2022  \n",
        "- **Project URL:** [chroma.ai](https://chroma.ai/)\n",
        "\n",
        "### Stable Diffusion\n",
        "- **Creator:** Stability AI  \n",
        "- **Description:** Stable Diffusion is a deep learning model designed for generating detailed images based on text prompts, known for its open-source nature and high quality.  \n",
        "- **Estimated Year of Inception:** 2022  \n",
        "- **Project URL:** [stability.ai](https://stability.ai/)\n",
        "\n",
        "### Gradient\n",
        "- **Creator:** Paperspace  \n",
        "- **Description:** Gradient is a platform that simplifies the process of building and deploying machine learning models, providing a suite of tools for developers and data scientists.  \n",
        "- **Estimated Year of Inception:** 2020  \n",
        "- **Project URL:** [paperspace.com/gradient](https://www.paperspace.com/gradient)\n",
        "\n",
        "### Giant-Machine\n",
        "- **Creator:** Giant Team  \n",
        "- **Description:** Giant-Machine is an advanced toolkit for building and deploying robust AI models with ease.  \n",
        "- **Estimated Year of Inception:** 2023  \n",
        "- **Project URL:** [giantmachine.ai](https://giantmachine.ai/)\n",
        "\n",
        "## Chapter 6\n",
        "### HumanLayer\n",
        "- **Creator:** Human Layer  \n",
        "- **Description:** HumanLayer is an open-source library that allows developers to build applications that leverage human input alongside AI systems.  \n",
        "- **Estimated Year of Inception:** 2022  \n",
        "- **Project URL:** [humanlayer.com](https://www.humanlayer.com/)\n",
        "\n",
        "### Gandalf\n",
        "- **Creator:** Gandalf Team  \n",
        "- **Description:** Gandalf is an open-source resource for building intuitive search applications, powered by AI.  \n",
        "- **Estimated Year of Inception:** 2021  \n",
        "- **Project URL:** [gandalf.dev](https://gandalf.dev/)\n",
        "\n",
        "## Chapter 11\n",
        "### TensorFlow\n",
        "- **Creator:** Google Brain Team  \n",
        "- **Description:** TensorFlow is an end-to-end open-source platform for machine learning, offering a comprehensive ecosystem for building ML applications.  \n",
        "- **Estimated Year of Inception:** 2015  \n",
        "- **Project URL:** [tensorflow.org](https://www.tensorflow.org/)\n",
        "\n",
        "### OpenAI Gym\n",
        "- **Creator:** OpenAI  \n",
        "- **Description:** OpenAI Gym is a toolkit for developing and comparing reinforcement learning (RL) algorithms, providing a standard API for environments.  \n",
        "- **Estimated Year of Inception:** 2016  \n",
        "- **Project URL:** [gym.openai.com](https://gym.openai.com/)\n",
        "\n",
        "### IBM Watson\n",
        "- **Creator:** IBM  \n",
        "- **Description:** IBM Watson is a suite of AI services and applications designed to help enterprises leverage advanced data analytics and cognitive services.  \n",
        "- **Estimated Year of Inception:** 2011  \n",
        "- **Project URL:** [ibm.com/watson](https://www.ibm.com/watson/)\n",
        "\n",
        "### IBM watsonx\n",
        "- **Creator:** IBM  \n",
        "- **Description:** IBM watsonx is IBM‚Äôs next-generation data, AI, and integration platform designed to empower organizations in their AI journey.  \n",
        "- **Estimated Year of Inception:** 2023  \n",
        "- **Project URL:** [ibm.com/watsonx](https://www.ibm.com/watsonx/)\n",
        "\n",
        "### Mistral\n",
        "- **Creator:** Mistral  \n",
        "- **Description:** Mistral is an open-source LLM that specializes in generating high-quality text based on prompts, with a focus on efficiency and accessibility.  \n",
        "- **Estimated Year of Inception:** 2023  \n",
        "- **Project URL:** [mistral.ai](https://mistral.ai/)\n",
        "\n",
        "### CrewAI\n",
        "- **Creator:** CrewAI  \n",
        "- **Description:** CrewAI is a collaborative AI platform focused on improving teams and organizations through machine learning tools and data analytics.  \n",
        "- **Estimated Year of Inception:** 2023  \n",
        "- **Project URL:** [crewai.com](https://crewai.com/)\n",
        "\n",
        "### LangChain\n",
        "- **Creator:** Harrison Chase, et al.  \n",
        "- **Description:** LangChain is a framework for developing applications powered by language models, emphasizing modular components for integrations, chains, and agents.  \n",
        "- **Estimated Year of Inception:** 2022  \n",
        "- **Project URL:** [langchain.readthedocs.io](https://langchain.readthedocs.io/)\n",
        "Connected to Python 3 Google Compute Engine backend\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skajX8nPbaut"
      },
      "source": [
        "### Listing 12-2: Gernerate HTML Version of Open Source AI Glossary for Appendix Use\n",
        "This listing loads an Excel-based glossary hosted on GitHub, extracting both visible names and embedded hyperlinks from the Name column. It normalizes entries, formats chapter indicators (e.g., \"F\" for Foreword, \"I\" for Introduction), and aggregates multiple references by project. The output is a clean, alphabetically sorted DataFrame showing one row per open-source project, suitable for use in appendices, indexes, or printable reference sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yY1OJ0Pbey7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, requests, re\n",
        "from io import BytesIO\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "# === Load Excel from GitHub ===\n",
        "url = \"https://buildyourownai.github.io/code/datasets/byoai_glossary.xlsx\"\n",
        "wb = load_workbook(BytesIO(requests.get(url).content), data_only=False)\n",
        "ws = wb.active\n",
        "\n",
        "# === Extract headers and rows, including HYPERLINK support ===\n",
        "headers = [cell.value for cell in next(ws.iter_rows(min_row=1, max_row=1))]\n",
        "name_col = headers.index(\"Name\")\n",
        "headers.insert(name_col + 1, \"URL\")\n",
        "\n",
        "data = []\n",
        "for row in ws.iter_rows(min_row=2):\n",
        "    cell = row[name_col]\n",
        "    val = str(cell.value) if cell.value else \"\"\n",
        "    url = cell.hyperlink.target if cell.hyperlink else None\n",
        "    m = re.match(r'=HYPERLINK\\(\"([^\"]+)\",\\s*\"([^\"]+)\"\\)', val)\n",
        "    if m: url, val = m.group(1), m.group(2)\n",
        "    row_vals = [c.value for c in row]\n",
        "    row_vals.insert(name_col + 1, url)\n",
        "    row_vals[name_col] = val\n",
        "    data.append(row_vals)\n",
        "\n",
        "# === Build DataFrame and clean ===\n",
        "df = pd.DataFrame(data, columns=headers)\n",
        "df = df.dropna(subset=[\"Name\"])\n",
        "df = df[df[\"Name\"].astype(str).str.strip() != \"\"]\n",
        "df[\"Name_normalized\"] = df[\"Name\"].str.strip().str.lower()\n",
        "\n",
        "# === Format chapters (F = Foreword, I = Intro) ===\n",
        "df[\"Chapter\"] = df[\"Chapter\"].apply(lambda x: \"F\" if x == -1 else \"I\" if x == 0 else str(int(x)) if pd.notnull(x) else \"\")\n",
        "\n",
        "# === Aggregate by project ===\n",
        "agg_df = df.groupby(\"Name_normalized\").agg({\n",
        "    \"Name\": \"first\",\n",
        "    \"URL\": \"first\",\n",
        "    \"Creator\": \"first\",\n",
        "    \"Year\": \"first\",\n",
        "    \"Chapter\": lambda x: \", \".join(sorted(set(filter(None, x)))),\n",
        "    \"Description\": \"first\"\n",
        "}).reset_index(drop=True).rename(columns={\n",
        "    \"Name\": \"Name\", \"URL\": \"URL\", \"Creator\": \"Creators\",\n",
        "    \"Year\": \"Year\", \"Chapter\": \"Chapters\", \"Description\": \"Description\"\n",
        "}).sort_values(by=\"Name\")\n",
        "\n",
        "# === Preview top entries ===\n",
        "print(\"‚úÖ Aggregated Glossary Sample (A‚ÄìZ):\\n\")\n",
        "print(agg_df[[\"Name\", \"URL\", \"Creators\", \"Year\", \"Chapters\", \"Description\"]].head(25).to_string(index=False))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm1bgEwlfFgT"
      },
      "source": [
        "#### Part 2: Using the data structure created in the cell above, format and save the HTML Glossary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aujiszSHfKQA"
      },
      "outputs": [],
      "source": [
        "output_file = \"byoai_glossary.html\"\n",
        "\n",
        "# === Begin HTML document ===\n",
        "html_rows = [\"\"\"<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "<meta charset=\"UTF-8\">\n",
        "<title>Glossary: Build Your Own AI by Cuomo & De Jes&#250;s</title>\n",
        "<style>\n",
        "body { font-family: \"Helvetica Neue\", sans-serif; background: #fff; margin: 2em; color: #222; }\n",
        "h2 { color: #1565C0; font-size: 1.5em; border-bottom: 3px solid #1565C0; padding-bottom: 0.3em; }\n",
        "table { width: 100%; border-collapse: collapse; margin-top: 1.5em; font-size: 0.95em;\n",
        "        box-shadow: 0 1px 3px rgba(0,0,0,0.1); border: 1px solid #ccc; border-radius: 4px; }\n",
        "thead { background: #FFEB3B; color: #111; }\n",
        "th { padding: 0.7em; text-align: left; font-weight: 600; border-bottom: 2px solid #ccc; }\n",
        "td { padding: 0.65em 0.9em; vertical-align: top; border-bottom: 1px solid #ddd; }\n",
        "tbody tr:nth-child(odd) { background: #F7F9FA; }\n",
        "tbody tr:nth-child(even) { background: #fff; }\n",
        "tbody tr:hover { background: #E3F2FD; }\n",
        "td:first-child { font-weight: 500; background: #F0F4FF; border-left: 3px solid #1565C0; }\n",
        "td:last-child { border-left: 3px solid #F44336; background: #FFF8F8; }\n",
        "a { color: #1565C0; text-decoration: none; }\n",
        "a:hover { text-decoration: underline; }\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "<h2>Glossary: Open Source AI <i>by Cuomo & De Jes&#250;s</i></h2>\n",
        "<table>\n",
        "<thead><tr>\n",
        "  <th>Project Name</th><th>Creator(s)</th><th>Year</th><th>Chapters</th><th>Description</th>\n",
        "</tr></thead>\n",
        "<tbody>\n",
        "\"\"\"]\n",
        "\n",
        "# === Add table rows ===\n",
        "for _, row in agg_df.iterrows():\n",
        "    name = row[\"Name\"]\n",
        "    url = row[\"URL\"]\n",
        "    link = f'<a href=\"{url}\" target=\"_blank\">{name}</a>' if url else name\n",
        "    html_rows.append(f\"\"\"<tr>\n",
        "<td>{link}</td>\n",
        "<td>{row['Creators'] or ''}</td>\n",
        "<td>{row['Year'] if pd.notnull(row['Year']) else ''}</td>\n",
        "<td>{row['Chapters']}</td>\n",
        "<td>{row['Description']}</td>\n",
        "</tr>\"\"\")\n",
        "\n",
        "# === Close HTML ===\n",
        "html_rows.append(\"</tbody></table></body></html>\")\n",
        "\n",
        "# === Write to file ===\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(html_rows))\n",
        "\n",
        "print(f\"‚úÖ Glossary HTML saved as: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuFbvyuYf6_X"
      },
      "source": [
        "### Listing 12-3: Generate Open Source AI Architecture View\n",
        "\n",
        "This listing parses an Excel-based glossary of open-source AI projects and filters the top entries per category. It then groups and styles them into an interactive HTML reference architecture, color-coded by domain. The output offers a visual snapshot of the ecosystem, ideal for education, planning, or documentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU_qbe7lgBsk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "# === CATEGORY AND SUBCATEGORY DEFINITIONS ===\n",
        "category = {\n",
        "    1: {\"label\": \"Tools & Ecosystem\", \"color\": \"#546E7A\"},\n",
        "    2: {\"label\": \"Data Layer\", \"color\": \"#FFEB3B\"},\n",
        "    3: {\"label\": \"Model Development\", \"color\": \"#2196F3\"},\n",
        "    4: {\"label\": \"Agents & Operations\", \"color\": \"#ECEFF1\"},\n",
        "    5: {\"label\": \"Platform Services\", \"color\": \"#9E9E9E\"},\n",
        "    6: {\"label\": \"Governance & Oversight\", \"color\": \"#F44336\"}\n",
        "}\n",
        "\n",
        "subcategory = {\n",
        "    100: {\"label\": \"Developer Environments\", \"limit\": 2},\n",
        "    110: {\"label\": \"Model Hubs / Repos\", \"limit\": 2},\n",
        "    200: {\"label\": \"Data Basics\", \"limit\": 2},\n",
        "    210: {\"label\": \"Data Augmentation\", \"limit\": 2},\n",
        "    220: {\"label\": \"Data Synth\", \"limit\": 2},\n",
        "    300: {\"label\": \"Deep Learning Frameworks\", \"limit\": 3},\n",
        "    310: {\"label\": \"Classical ML\", \"limit\": 3},\n",
        "    320: {\"label\": \"Models\", \"limit\": 3},\n",
        "    400: {\"label\": \"Agent Frameworks\", \"limit\": 2},\n",
        "    410: {\"label\": \"Model Serve\", \"limit\": 2},\n",
        "    420: {\"label\": \"Flow Control\", \"limit\": 2},\n",
        "    500: {\"label\": \"Vector Stores\", \"limit\": 2},\n",
        "    510: {\"label\": \"Experiment Tracking\", \"limit\": 2},\n",
        "    520: {\"label\": \"Benchmarks\", \"limit\": 2},\n",
        "    600: {\"label\": \"Security & Guardrails\", \"limit\": 3},\n",
        "    610: {\"label\": \"Licensing & Compliance\", \"limit\": 3},\n",
        "    620: {\"label\": \"Ethics & Responsibility\", \"limit\": 3}\n",
        "}\n",
        "\n",
        "# === Constants ===\n",
        "BASE_URL = \"https://buildyourownai.github.io/code/datasets/\"\n",
        "FILE_NAME = \"byoai_glossary.xlsx\"\n",
        "EXCEL_URL = BASE_URL + FILE_NAME\n",
        "\n",
        "# === Load Excel file directly into memory ===\n",
        "response = requests.get(EXCEL_URL)\n",
        "excel_data = BytesIO(response.content)\n",
        "\n",
        "# === Load workbook from in-memory bytes ===\n",
        "wb = load_workbook(excel_data, data_only=False)\n",
        "ws = wb.active\n",
        "\n",
        "# === Parse headers and setup ===\n",
        "headers = [cell.value for cell in next(ws.iter_rows(min_row=1, max_row=1))]\n",
        "name_col_idx = headers.index(\"Name\")\n",
        "headers.insert(name_col_idx + 1, \"URL\")\n",
        "\n",
        "data = []\n",
        "for row in ws.iter_rows(min_row=2):\n",
        "    cell = row[name_col_idx]\n",
        "    value = str(cell.value)\n",
        "    # Handle Excel HYPERLINK formulas\n",
        "    hyperlink_match = re.match(r'=HYPERLINK\\(\"([^\"]+)\",\\s*\"([^\"]+)\"\\)', value)\n",
        "    if hyperlink_match:\n",
        "        url = hyperlink_match.group(1)\n",
        "        display = hyperlink_match.group(2)\n",
        "    else:\n",
        "        url = cell.hyperlink.target if cell.hyperlink else None\n",
        "        display = cell.value\n",
        "\n",
        "    row_values = [c.value for c in row]\n",
        "    row_values.insert(name_col_idx + 1, url)\n",
        "    row_values[name_col_idx] = display\n",
        "    data.append(row_values)\n",
        "\n",
        "df = pd.DataFrame(data, columns=headers)\n",
        "df = df.dropna(subset=[\"Name\"])\n",
        "\n",
        "# Normalize and clean\n",
        "df[\"Category\"] = pd.to_numeric(df[\"Category\"], errors=\"coerce\")\n",
        "df[\"Name_normalized\"] = df[\"Name\"].str.strip().str.lower()\n",
        "df = df.drop_duplicates(subset=[\"Name_normalized\", \"Chapter\"])\n",
        "\n",
        "# Count mentions across all chapters\n",
        "mention_counts = df[\"Name_normalized\"].value_counts().to_dict()\n",
        "df[\"Mention_Count\"] = df[\"Name_normalized\"].map(mention_counts)\n",
        "\n",
        "# Add category/subcategory labels\n",
        "df[\"Category_Label\"] = df[\"Category\"].apply(\n",
        "    lambda x: category.get(int(x // 100), {}).get(\"label\") if pd.notnull(x) else None\n",
        ")\n",
        "df[\"Category_Color\"] = df[\"Category\"].apply(\n",
        "    lambda x: category.get(int(x // 100), {}).get(\"color\") if pd.notnull(x) else None\n",
        ")\n",
        "df[\"Subcategory_Label\"] = df[\"Category\"].map(lambda x: subcategory.get(x, {}).get(\"label\"))\n",
        "df[\"Subcategory_Limit\"] = df[\"Category\"].map(lambda x: subcategory.get(x, {}).get(\"limit\"))\n",
        "\n",
        "# === PREVIEW BEFORE FILTERING ===\n",
        "display_cols = [\"Name\", \"URL\", \"Category\", \"Category_Label\", \"Category_Color\", \"Subcategory_Label\", \"Mention_Count\"]\n",
        "print(\"üü° Full dataset sample before filtering:\\n\")\n",
        "print(df[display_cols].head(100).to_string(index=False))\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# === FILTER TOP PROJECTS PER SUBCATEGORY (dedup by name within subcat) ===\n",
        "df_sorted = df.sort_values(by=[\"Category\", \"Mention_Count\"], ascending=[True, False])\n",
        "\n",
        "filtered_dfs = []\n",
        "for subcat_code, meta in subcategory.items():\n",
        "    limit = meta[\"limit\"]\n",
        "    sub_df = df_sorted[df_sorted[\"Category\"] == subcat_code]\n",
        "    sub_df = sub_df.drop_duplicates(subset=[\"Name\"])  # only keep one row per Name\n",
        "    sub_df = sub_df.head(limit)\n",
        "    filtered_dfs.append(sub_df)\n",
        "\n",
        "final_df = pd.concat(filtered_dfs, ignore_index=True)\n",
        "\n",
        "# === BEFORE FINAL DEDUP (raw filtered)\n",
        "print(\"üü¢ Filtered `final_df` before optional deduping by Name:\\n\")\n",
        "print(final_df[display_cols].head(50).to_string(index=False))\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# === OPTIONAL: DEDUP FINAL OUTPUT (ONLY KEEP ONE PER NAME)\n",
        "final_df = final_df.drop_duplicates(subset=[\"Name\"])\n",
        "print(\"‚úÖ `final_df` after deduping by Name:\\n\")\n",
        "print(final_df[display_cols].head(50).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9u-SyzmA_iJ"
      },
      "source": [
        "#### Part 2: Generate HTML for Reference Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnPi0tCsevVK"
      },
      "outputs": [],
      "source": [
        "# === Generate HTML Reference Architecture from final_df ===\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# Group data by top-level category and subcategory\n",
        "grouped = defaultdict(lambda: defaultdict(list))\n",
        "for _, row in final_df.iterrows():\n",
        "    cat_label = row[\"Category_Label\"]\n",
        "    cat_color = row[\"Category_Color\"]\n",
        "    subcat_label = row[\"Subcategory_Label\"]\n",
        "    name = row[\"Name\"]\n",
        "    url = row[\"URL\"] or \"#\"  # fallback if URL is missing\n",
        "    grouped[(cat_label, cat_color)][subcat_label].append((name, url))\n",
        "\n",
        "# Start building HTML content\n",
        "html = [\n",
        "    \"<!DOCTYPE html>\",\n",
        "    \"<html lang='en'>\",\n",
        "    \"<head><meta charset='UTF-8'><title>Build Your Own AI - Reference Architecture</title><style>\",\n",
        "    \"body { font-family: Arial, sans-serif; background: #fdfdfd; margin: 0; padding: .05em; }\",\n",
        "    \".layer-wrapper { display: flex; align-items: flex-start; gap: .5em; margin-bottom: .5em; }\",\n",
        "    \".layer-label { width: 140px; font-weight: bold; font-size: 1.1em; text-align: center; padding-top: 1em; }\",\n",
        "    \".layer-box { flex: 1; border-radius: 8px; padding: .75em; border: 2px solid #333; }\",\n",
        "    \".category-row { display: flex; flex-wrap: wrap; justify-content: center; gap: 2em; }\",\n",
        "    \".category { background: white; border-radius: 12px; padding: .25em; width: 240px; box-shadow: 0 3px 8px rgba(0,0,0,0.15); display: flex; flex-direction: column; align-items: center; }\",\n",
        "    \".category-title { font-weight: bold; font-size: 1em; margin-bottom: 0.5em; text-align: center; }\",\n",
        "    \".project-link { display: block; margin: 0.25em 0; text-align: center; font-size: .8em; text-decoration: none; color: #0056b3; }\",\n",
        "    \".project-link:hover { text-decoration: underline; }\",\n",
        "    \"</style></head><body>\"\n",
        "]\n",
        "\n",
        "# Populate HTML content\n",
        "for (layer_label, color), subcats in grouped.items():\n",
        "    html.append('<div class=\"layer-wrapper\">')\n",
        "    html.append(f'<div class=\"layer-label\">{layer_label}</div>')\n",
        "    html.append(f'<div class=\"layer-box\" style=\"background-color: {color}\">')\n",
        "    html.append('<div class=\"category-row\">')\n",
        "    for subcat, projects in subcats.items():\n",
        "        html.append('<div class=\"category\">')\n",
        "        html.append(f'<div class=\"category-title\">{subcat}</div>')\n",
        "        for name, url in projects:\n",
        "            html.append(f'<a class=\"project-link\" href=\"{url}\" target=\"_blank\">{name}</a>')\n",
        "        html.append('</div>')\n",
        "    html.append('</div></div></div>')\n",
        "\n",
        "html.extend([\"</body>\", \"</html>\"])\n",
        "\n",
        "# Write HTML file\n",
        "with open(\"byoai_reference_architecture.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(html))\n",
        "\n",
        "print(\"‚úÖ Saved: byoai_reference_architecture.html\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
